{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "394f5be5",
   "metadata": {},
   "source": [
    "IMPORT NEEDED DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1708b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neural_network_helper import setup_inputs_weights, calculate_loss, calculate_ws, calculate_relu, calculate_sigmoid, calculate_softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53326fa",
   "metadata": {},
   "source": [
    "2b.\tGiven the following inputs from the Breast Cancer Dataset, using three features: Mean Radius, Mean Texture, and Mean Smoothness, determine whether the tumor is Benign (0) or Malignant (1) by calculating the network outputs step by step, given the following neural network configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6aa59bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Hidden Layer Outputs: [11.3435  3.4755 -0.2905]\n"
     ]
    }
   ],
   "source": [
    "# First Hidden Layer Calculation\n",
    "inputs1 = [14.1, 20.3, 0.095]\n",
    "target_output = [1]\n",
    "weights1 = [\n",
    "    [0.5, -0.3, 0.8],\n",
    "    [0.2, 0.4, -0.6],\n",
    "    [-0.7, 0.9, 0.1]\n",
    "]\n",
    "biases1 = [0.3, -0.5, 0.6]\n",
    "\n",
    "setup_inputs_weights(inputs1, weights1)\n",
    "layer1_output = calculate_ws(inputs1, weights1, biases1)\n",
    "\n",
    "print(f'First Hidden Layer Outputs: {layer1_output}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c71f0c6",
   "metadata": {},
   "source": [
    "CALCULATING THE ACTIVATION FUNCTION USING RELU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6152ce56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second Layer Inputs (after ReLu): [11.34, 3.48, 0.0]\n"
     ]
    }
   ],
   "source": [
    "inputs2 = calculate_relu(layer1_output)\n",
    "inputs2 = [float(round(x, 2)) for x in inputs2]\n",
    "print(\"Second Layer Inputs (after ReLu):\", inputs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2e20d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second Hidden Layer Outputs: [ 6.208 -2.462]\n"
     ]
    }
   ],
   "source": [
    "weights2 = [\n",
    "    [0.6, -0.3],\n",
    "    [-0.2, 0.5],\n",
    "    [0.4, 0.7]\n",
    "]\n",
    "biases2 = [0.1, -0.8]\n",
    "\n",
    "setup_inputs_weights(inputs2, weights2)\n",
    "layer2_outputs = calculate_ws(inputs2, weights2, biases2)\n",
    "print(f'Second Hidden Layer Outputs: {layer2_outputs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be373b57",
   "metadata": {},
   "source": [
    "CALCULATION OF ACTIVATION FUNCTION USING SIGMOID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "51ae6f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Layer Inputs (after Sigmoid) [1.0, 0.08]\n"
     ]
    }
   ],
   "source": [
    "inputs3 = calculate_sigmoid(layer2_outputs)\n",
    "inputs3 = [float(round(x, 2)) for x in inputs3]\n",
    "\n",
    "print(\"Last Layer Inputs (after Sigmoid)\", inputs3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdbcb47",
   "metadata": {},
   "source": [
    "CALCULATION OF THE LAST LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fa797b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (2,) and (1,2) not aligned: 2 (dim 0) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m biases3 = [\u001b[32m0.2\u001b[39m]\n\u001b[32m      6\u001b[39m setup_inputs_weights(inputs3, weights3)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m last_layer_outputs = \u001b[43mcalculate_ws\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbiases3\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mLast Layer Outputs: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlast_layer_outputs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32ma:\\CCS 248 - Artificial Neural Networks\\coding\\25-26\\CS 3B\\AXEL_JOHN_NUQUI\\neural_network_helper.py:10\u001b[39m, in \u001b[36mcalculate_ws\u001b[39m\u001b[34m(inputs, weights, biases)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcalculate_ws\u001b[39m(inputs, weights, biases):\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m)\u001b[49m + biases\n",
      "\u001b[31mValueError\u001b[39m: shapes (2,) and (1,2) not aligned: 2 (dim 0) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "weights3 = [\n",
    "    [0.7],\n",
    "    [-0.5]\n",
    "]\n",
    "biases3 = [0.2]\n",
    "\n",
    "setup_inputs_weights(inputs3, weights3)\n",
    "last_layer_outputs = calculate_ws(inputs3, weights3, biases3)\n",
    "\n",
    "print(f'Last Layer Outputs: {last_layer_outputs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af632afc",
   "metadata": {},
   "source": [
    "CALCULATING THE ACTIVATION FUNCTION USING SOFTMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63521de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Last Layer's Activation Function (after Softmax):  [0.7]\n"
     ]
    }
   ],
   "source": [
    "last_layer_af = calculate_sigmoid(last_layer_outputs)\n",
    "last_layer_af = [float(round(x, 2)) for x in last_layer_af]\n",
    "\n",
    "print(\"Last Layer's Activation Function (after Softmax): \", last_layer_af)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cca0eb",
   "metadata": {},
   "source": [
    "CALCULATING LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53169322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.3567\n"
     ]
    }
   ],
   "source": [
    "loss = calculate_loss(target_output, last_layer_af)\n",
    "print(f'Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fddd7d6",
   "metadata": {},
   "source": [
    "Hidden Layer 2 (Output): [5.074 4.805]\n",
    "Loss: 5.9146"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7973251",
   "metadata": {},
   "source": [
    "INSTALL THE FOLLOWING PYTHON PACKAGES FIRST BEFORE RUNNING THE PROGRAM\n",
    "\n",
    "1) Numpy\n",
    "2) NNFS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7a7dd421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f740e2b",
   "metadata": {},
   "source": [
    "Create classes for modularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fc714746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden Layers\n",
    "# Dense\n",
    "class Layer_Dense:\n",
    "    # Layer initialization\n",
    "    # randomly initialize weights and set biases to zero\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember the input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate the output values from inputs, weight and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass/Backpropagation\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters:\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "945fa51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Functions\n",
    "# Included here are the functions for both the forward and backward pass\n",
    "\n",
    "# Linear\n",
    "class ActivationLinear:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = inputs\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "# Sigmoid\n",
    "class ActivationSigmoid:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * (self.output * (1 - self.output))\n",
    "\n",
    "# TanH\n",
    "class ActivationTanH:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.tanh(inputs)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * (1 - self.output ** 2)\n",
    "\n",
    "# ReLU\n",
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember the input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate the output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Make a copy of the original values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "    \n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "# Softmax\n",
    "class Activation_Softmax:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember the inputs values\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Get the unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # Calculate Jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            # Calculate the sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ed37d0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "\n",
    "class Loss:\n",
    "    # Calculate the data and regularization losses\n",
    "    # Given the model output and grou truth/target values\n",
    "    def calculate(self, output, y):\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        # Calculate the mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        # Return the mean loss\n",
    "        return data_loss\n",
    "\n",
    "# MSE\n",
    "class Loss_MSE:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Calculate Mean Squared Error\n",
    "        return np.mean((y_true - y_pred) ** 2, axis=-1)\n",
    "\n",
    "    def backward(self, y_pred, y_true):\n",
    "        # Gradient of MSE loss\n",
    "        samples = y_true.shape[0]\n",
    "        outputs = y_true.shape[1]\n",
    "        self.dinputs = -2 * (y_true - y_pred) / outputs\n",
    "        # Normalize gradients over samples\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "# Binary Cross-Entropy\n",
    "class Loss_BinaryCrossEntropy:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Clip predictions\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        # Calculate Binary Cross Entropy\n",
    "        return -(y_true * np.log(y_pred_clipped) + (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "\n",
    "    def backward(self, y_pred, y_true):\n",
    "        # Gradient of BCE loss\n",
    "        samples = y_true.shape[0]\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        self.dinputs = - (y_true / y_pred_clipped - (1 - y_true) / (1 - y_pred_clipped))\n",
    "        # Normalize gradients over samples\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "# Categorical Cross-Entropy\n",
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Number of samples in a batch\n",
    "        samples = y_pred.shape[0]\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values\n",
    "        # Only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of labels in every sample\n",
    "        # Use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # Check if labels are sparse, turn them into one-hot vector values\n",
    "        # the eye function creates a 2D array with ones on the diagonal and zeros elsewhere\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # Calculate the gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8c33b2",
   "metadata": {},
   "source": [
    "<!-- Star -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "054d8c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start of Optimizers\n",
    "\n",
    "# Optimizer with Learning Rate Decay, Momentum, and Adaptive Gradient\n",
    "class Optimizer_SGD:\n",
    "    def __init__(self, learning_rate=1.0, decay=0.0, momentum=0.0, adaptive=False, epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "        self.adaptive = adaptive\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        # Apply learning rate decay before Forward & Backward passes\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1.0 / (1.0 + self.decay * self.iterations))\n",
    "        else:\n",
    "            self.current_learning_rate = self.learning_rate\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        # Initialize momentums and caches if they don't exist yet\n",
    "        if not hasattr(layer, 'weight_momentums'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "        if self.adaptive and not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # ----- Adaptive Gradient -----\n",
    "        if self.adaptive:\n",
    "            layer.weight_cache += layer.dweights * layer.dweights\n",
    "            layer.bias_cache += layer.dbiases * layer.dbiases\n",
    "\n",
    "            weight_update = -(self.current_learning_rate * layer.dweights) / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "            bias_update = -(self.current_learning_rate * layer.dbiases) / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "\n",
    "            if self.momentum:\n",
    "                layer.weight_momentums = self.momentum * layer.weight_momentums + weight_update\n",
    "                layer.bias_momentums = self.momentum * layer.bias_momentums + bias_update\n",
    "                layer.weights += layer.weight_momentums\n",
    "                layer.biases += layer.bias_momentums\n",
    "            else:\n",
    "                layer.weights += weight_update\n",
    "                layer.biases += bias_update\n",
    "            return\n",
    "\n",
    "        # ----- Momentum or Vanilla SGD -----\n",
    "        if self.momentum:\n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "\n",
    "            layer.weight_momentums = weight_updates\n",
    "            layer.bias_momentums = bias_updates\n",
    "\n",
    "            layer.weights += weight_updates\n",
    "            layer.biases += bias_updates\n",
    "        else:\n",
    "            layer.weights += -self.current_learning_rate * layer.dweights\n",
    "            layer.biases += -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f819460f",
   "metadata": {},
   "source": [
    "Use most of the classes to create a functioning neural network, capable of performing a forward and backward pass\n",
    "\n",
    "We can use a sample dataset from the Spiral module\n",
    "We can also use the IRIS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "325fd84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for the package\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "# Neural Network initialization\n",
    "# Create the dataset\n",
    "X, y = spiral_data(samples = 100, classes = 3)\n",
    "\n",
    "# Create a Dense Layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "\n",
    "# Create a ReLU activation for the first Dense layer\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create a 2nd dense layer with 3 input and 3 output values\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "\n",
    "# Create a Softmax activation for the 2nd Dense layer\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create a loss function\n",
    "loss_function = Loss_CategoricalCrossEntropy()\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = Optimizer_SGD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a737644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc6742a",
   "metadata": {},
   "source": [
    "PERFORM ONLY 1 PASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "497de66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.3466666666666667\n"
     ]
    }
   ],
   "source": [
    "# Perform a forward pass of our training data\n",
    "# give the input from the dataset to the first layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# Activation function\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Pass on the 2nd layer\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# Calculate the loss\n",
    "loss_function.forward(activation2.output, y)\n",
    "\n",
    "# Check the model's performance\n",
    "predictions = np.argmax(activation2.output, axis=1)\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y, axis=1)\n",
    "accuracy = np.mean(predictions == y)\n",
    "\n",
    "# Print the accuracy\n",
    "print('acc:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "183b77a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 3)\n"
     ]
    }
   ],
   "source": [
    "# Perform a backward pass of our training data\n",
    "# From loss to 2nd softmax activation\n",
    "loss_function.backward(activation2.output, y)\n",
    "dvalues = loss_function.dinputs # Gradient of the loss w.r.t softmax output\n",
    "\n",
    "print(dvalues.shape)\n",
    "# print(dvalues)\n",
    "\n",
    "# From 2nd softmax to 2nd dense layer\n",
    "activation2.backward(dvalues)\n",
    "# From 2nd dense layer to 1st ReLU activation\n",
    "dense2.backward(activation2.dinputs)\n",
    "\n",
    "# From 1st ReLU activation to 1st dense layer\n",
    "activation1.backward(dense2.dinputs)\n",
    "dense1.backward(activation1.dinputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5e5cabc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.38984618e-05  4.52121902e-05 -5.36579938e-04]\n",
      " [ 6.63203993e-05  9.92777199e-05 -3.94164364e-05]]\n",
      "[[ 0.00012147 -0.00025889  0.0006916 ]]\n",
      "[[ 1.69591678e-04 -3.28835917e-04  1.59244239e-04]\n",
      " [-1.69836264e-05  1.09616758e-04 -9.26331318e-05]\n",
      " [ 5.66655808e-05  1.87660143e-04 -2.44325724e-04]]\n",
      "[[ 1.69481633e-05  3.77978828e-06 -2.07279516e-05]]\n"
     ]
    }
   ],
   "source": [
    "# Check the gradient values of the weights and biases of the established layers\n",
    "print(dense1.dweights)\n",
    "print(dense1.dbiases)\n",
    "print(dense2.dweights)\n",
    "print(dense2.dbiases)\n",
    "\n",
    "\n",
    "# Update the weights and biases\n",
    "optimizer.update_params(dense1)\n",
    "optimizer.update_params(dense2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f222996",
   "metadata": {},
   "source": [
    "EXERCISE FOUR:\n",
    "1) Set up the code so that it will perform the Forward Pass (FP), Backpropagation (BP) and weight update in 1000 epochs\n",
    "\n",
    "2) Modify the Optimizer class so that it will accept 3 optimizers we've discussed\n",
    "    \n",
    "    a) Learning rate decay\n",
    "\n",
    "    b) Momentum\n",
    "\n",
    "    c) Adaptive Gradient\n",
    "\n",
    "    Hint: Updating the learning decay rate happens before running both FP and BP, implementing momentum, and vanilla SGD happens after the learning rate decay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c25b427",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# EXERCISE FOUR: Training for 1000 Epochs\n",
    "# ============================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5fbc3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Training with: VANILLA\n",
      "============================================================\n",
      "Epoch:    0 | Loss: 1.0986 | Acc: 0.3267 | LR: 1.000000\n",
      "Epoch:  100 | Loss: 1.0978 | Acc: 0.4167 | LR: 1.000000\n",
      "Epoch:  200 | Loss: 1.0736 | Acc: 0.4133 | LR: 1.000000\n",
      "Epoch:  300 | Loss: 1.0723 | Acc: 0.4033 | LR: 1.000000\n",
      "Epoch:  400 | Loss: 1.0700 | Acc: 0.4133 | LR: 1.000000\n",
      "Epoch:  500 | Loss: 1.0659 | Acc: 0.4167 | LR: 1.000000\n",
      "Epoch:  600 | Loss: 1.0650 | Acc: 0.4200 | LR: 1.000000\n",
      "Epoch:  700 | Loss: 1.0646 | Acc: 0.4100 | LR: 1.000000\n",
      "Epoch:  800 | Loss: 1.0643 | Acc: 0.4200 | LR: 1.000000\n",
      "Epoch:  900 | Loss: 1.0641 | Acc: 0.4133 | LR: 1.000000\n",
      "Epoch:  999 | Loss: 1.0640 | Acc: 0.4200 | LR: 1.000000\n",
      "\n",
      "============================================================\n",
      "Training with: DECAY\n",
      "============================================================\n",
      "Epoch:    0 | Loss: 1.0986 | Acc: 0.2933 | LR: 1.000000\n",
      "Epoch:  100 | Loss: 1.0963 | Acc: 0.4200 | LR: 0.909091\n",
      "Epoch:  200 | Loss: 1.0748 | Acc: 0.4033 | LR: 0.833333\n",
      "Epoch:  300 | Loss: 1.0727 | Acc: 0.4367 | LR: 0.769231\n",
      "Epoch:  400 | Loss: 1.0694 | Acc: 0.4233 | LR: 0.714286\n",
      "Epoch:  500 | Loss: 1.0688 | Acc: 0.4300 | LR: 0.666667\n",
      "Epoch:  600 | Loss: 1.0687 | Acc: 0.4367 | LR: 0.625000\n",
      "Epoch:  700 | Loss: 1.0686 | Acc: 0.4367 | LR: 0.588235\n",
      "Epoch:  800 | Loss: 1.0685 | Acc: 0.4333 | LR: 0.555556\n",
      "Epoch:  900 | Loss: 1.0684 | Acc: 0.4367 | LR: 0.526316\n",
      "Epoch:  999 | Loss: 1.0683 | Acc: 0.4367 | LR: 0.500250\n",
      "\n",
      "============================================================\n",
      "Training with: MOMENTUM\n",
      "============================================================\n",
      "Epoch:    0 | Loss: 1.0986 | Acc: 0.3300 | LR: 1.000000\n",
      "Epoch:  100 | Loss: 1.0692 | Acc: 0.4400 | LR: 1.000000\n",
      "Epoch:  200 | Loss: 1.0662 | Acc: 0.4300 | LR: 1.000000\n",
      "Epoch:  300 | Loss: 1.0662 | Acc: 0.4267 | LR: 1.000000\n",
      "Epoch:  400 | Loss: 1.0662 | Acc: 0.4300 | LR: 1.000000\n",
      "Epoch:  500 | Loss: 1.0662 | Acc: 0.4267 | LR: 1.000000\n",
      "Epoch:  600 | Loss: 1.0662 | Acc: 0.4300 | LR: 1.000000\n",
      "Epoch:  700 | Loss: 1.0662 | Acc: 0.4267 | LR: 1.000000\n",
      "Epoch:  800 | Loss: 1.0662 | Acc: 0.4267 | LR: 1.000000\n",
      "Epoch:  900 | Loss: 1.0662 | Acc: 0.4300 | LR: 1.000000\n",
      "Epoch:  999 | Loss: 1.0662 | Acc: 0.4267 | LR: 1.000000\n",
      "\n",
      "============================================================\n",
      "Training with: ADAPTIVE\n",
      "============================================================\n",
      "Epoch:    0 | Loss: 1.0986 | Acc: 0.3333 | LR: 0.500000\n",
      "Epoch:  100 | Loss: 1.0481 | Acc: 0.4533 | LR: 0.500000\n",
      "Epoch:  200 | Loss: 1.0373 | Acc: 0.4467 | LR: 0.500000\n",
      "Epoch:  300 | Loss: 1.0353 | Acc: 0.4167 | LR: 0.500000\n",
      "Epoch:  400 | Loss: 1.0347 | Acc: 0.4133 | LR: 0.500000\n",
      "Epoch:  500 | Loss: 1.0343 | Acc: 0.4200 | LR: 0.500000\n",
      "Epoch:  600 | Loss: 1.0341 | Acc: 0.4200 | LR: 0.500000\n",
      "Epoch:  700 | Loss: 1.0339 | Acc: 0.4200 | LR: 0.500000\n",
      "Epoch:  800 | Loss: 1.0338 | Acc: 0.4167 | LR: 0.500000\n",
      "Epoch:  900 | Loss: 1.0337 | Acc: 0.4167 | LR: 0.500000\n",
      "Epoch:  999 | Loss: 1.0337 | Acc: 0.4167 | LR: 0.500000\n",
      "\n",
      "SUMMARY COMPARISON\n",
      "============================================================\n",
      "vanilla      - Loss: 1.0640, Accuracy: 0.4200\n",
      "decay        - Loss: 1.0683, Accuracy: 0.4367\n",
      "momentum     - Loss: 1.0662, Accuracy: 0.4267\n",
      "adaptive     - Loss: 1.0337, Accuracy: 0.4167\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Reimport dataset \n",
    "X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "def train_and_report(name, optimizer, epochs=1000, report_every=100):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Training with: {name.upper()}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Initialize layers\n",
    "    dense1 = Layer_Dense(2, 3)\n",
    "    activation1 = Activation_ReLU()\n",
    "    dense2 = Layer_Dense(3, 3)\n",
    "    activation2 = Activation_Softmax()\n",
    "    loss_function = Loss_CategoricalCrossEntropy()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.pre_update_params()\n",
    "\n",
    "        # Forward pass\n",
    "        dense1.forward(X)\n",
    "        activation1.forward(dense1.output)\n",
    "        dense2.forward(activation1.output)\n",
    "        activation2.forward(dense2.output)\n",
    "\n",
    "        # Compute loss and accuracy\n",
    "        loss = loss_function.calculate(activation2.output, y)\n",
    "        predictions = np.argmax(activation2.output, axis=1)\n",
    "        if len(y.shape) == 2:\n",
    "            y_labels = np.argmax(y, axis=1)\n",
    "        else:\n",
    "            y_labels = y\n",
    "        acc = np.mean(predictions == y_labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss_function.backward(activation2.output, y)\n",
    "        activation2.backward(loss_function.dinputs)\n",
    "        dense2.backward(activation2.dinputs)\n",
    "        activation1.backward(dense2.dinputs)\n",
    "        dense1.backward(activation1.dinputs)\n",
    "\n",
    "        # Update parameters\n",
    "        optimizer.update_params(dense1)\n",
    "        optimizer.update_params(dense2)\n",
    "        optimizer.post_update_params()\n",
    "\n",
    "        # Report every N epochs\n",
    "        if epoch % report_every == 0 or epoch == epochs - 1:\n",
    "            print(f\"Epoch: {epoch:4d} | Loss: {loss:.4f} | Acc: {acc:.4f} | LR: {optimizer.current_learning_rate:.6f}\")\n",
    "\n",
    "    return name, loss, acc\n",
    "\n",
    "\n",
    "# Run all 3 optimizers and collect final values\n",
    "results = []\n",
    "\n",
    "# 1️ Vanilla SGD\n",
    "opt_vanilla = Optimizer_SGD(learning_rate=1.0)\n",
    "results.append(train_and_report(\"vanilla\", opt_vanilla))\n",
    "\n",
    "# 2️ Learning Rate Decay\n",
    "opt_decay = Optimizer_SGD(learning_rate=1.0, decay=1e-3)\n",
    "results.append(train_and_report(\"decay\", opt_decay))\n",
    "\n",
    "# 3️ Momentum\n",
    "opt_momentum = Optimizer_SGD(learning_rate=1.0, momentum=0.9)\n",
    "results.append(train_and_report(\"momentum\", opt_momentum))\n",
    "\n",
    "# 4️ Adaptive Gradient\n",
    "opt_adagrad = Optimizer_SGD(learning_rate=0.5, adaptive=True)\n",
    "results.append(train_and_report(\"adaptive\", opt_adagrad))\n",
    "\n",
    "# SUMMARY COMPARISON\n",
    "print(\"\\nSUMMARY COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "for name, loss, acc in results:\n",
    "    print(f\"{name.lower():12s} - Loss: {loss:.4f}, Accuracy: {acc:.4f}\")\n",
    "print(\"=\"*60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

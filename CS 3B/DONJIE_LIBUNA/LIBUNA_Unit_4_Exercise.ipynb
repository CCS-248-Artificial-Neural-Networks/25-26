{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install nnfs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHXwMk0OKUB8",
        "outputId": "0d4c82d1-e845-4718-bf42-6940f8388077"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nnfs in /usr/local/lib/python3.12/dist-packages (0.5.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from nnfs) (2.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import nnfs\n",
        "from nnfs.datasets import spiral_data"
      ],
      "metadata": {
        "id": "GUlVeA1yKWqb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jNT5_ZiaKRDW",
        "outputId": "a0ededa1-79ac-4176-ccb6-4047df1a094e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "EXERCISE UNIT 4\n",
            ">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
            "\n",
            "------------------------------------------------------------\n",
            "Training results: VANILLA\n",
            "------------------------------------------------------------\n",
            "Epoch:    0 | Final Loss: 1.0986 | Final Accuracy: 0.3333 | LR: 1.000000\n",
            "Epoch:  100 | Final Loss: 1.0979 | Final Accuracy: 0.4067 | LR: 1.000000\n",
            "Epoch:  200 | Final Loss: 1.0849 | Final Accuracy: 0.3867 | LR: 1.000000\n",
            "Epoch:  300 | Final Loss: 1.0834 | Final Accuracy: 0.3667 | LR: 1.000000\n",
            "Epoch:  400 | Final Loss: 1.0804 | Final Accuracy: 0.3800 | LR: 1.000000\n",
            "Epoch:  500 | Final Loss: 1.0733 | Final Accuracy: 0.3867 | LR: 1.000000\n",
            "Epoch:  600 | Final Loss: 1.0695 | Final Accuracy: 0.3700 | LR: 1.000000\n",
            "Epoch:  700 | Final Loss: 1.0669 | Final Accuracy: 0.3733 | LR: 1.000000\n",
            "Epoch:  800 | Final Loss: 1.0650 | Final Accuracy: 0.3933 | LR: 1.000000\n",
            "Epoch:  900 | Final Loss: 1.0635 | Final Accuracy: 0.3900 | LR: 1.000000\n",
            "Epoch:  999 | Final Loss: 1.0624 | Final Accuracy: 0.4000 | LR: 1.000000\n",
            "\n",
            "------------------------------------------------------------\n",
            "Training results: LEARNING RATE DECAY\n",
            "------------------------------------------------------------\n",
            "Epoch:    0 | Final Loss: 1.0986 | Final Accuracy: 0.2600 | LR: 1.000000\n",
            "Epoch:  100 | Final Loss: 1.0985 | Final Accuracy: 0.3633 | LR: 0.909091\n",
            "Epoch:  200 | Final Loss: 1.0882 | Final Accuracy: 0.4133 | LR: 0.833333\n",
            "Epoch:  300 | Final Loss: 1.0831 | Final Accuracy: 0.4067 | LR: 0.769231\n",
            "Epoch:  400 | Final Loss: 1.0830 | Final Accuracy: 0.4033 | LR: 0.714286\n",
            "Epoch:  500 | Final Loss: 1.0830 | Final Accuracy: 0.4033 | LR: 0.666667\n",
            "Epoch:  600 | Final Loss: 1.0830 | Final Accuracy: 0.4033 | LR: 0.625000\n",
            "Epoch:  700 | Final Loss: 1.0830 | Final Accuracy: 0.4033 | LR: 0.588235\n",
            "Epoch:  800 | Final Loss: 1.0830 | Final Accuracy: 0.4033 | LR: 0.555556\n",
            "Epoch:  900 | Final Loss: 1.0830 | Final Accuracy: 0.4033 | LR: 0.526316\n",
            "Epoch:  999 | Final Loss: 1.0830 | Final Accuracy: 0.4033 | LR: 0.500250\n",
            "\n",
            "------------------------------------------------------------\n",
            "Training results: MOMENTUM\n",
            "------------------------------------------------------------\n",
            "Epoch:    0 | Final Loss: 1.0986 | Final Accuracy: 0.3333 | LR: 1.000000\n",
            "Epoch:  100 | Final Loss: 1.0626 | Final Accuracy: 0.4033 | LR: 1.000000\n",
            "Epoch:  200 | Final Loss: 1.0626 | Final Accuracy: 0.4033 | LR: 1.000000\n",
            "Epoch:  300 | Final Loss: 1.0626 | Final Accuracy: 0.3933 | LR: 1.000000\n",
            "Epoch:  400 | Final Loss: 1.0626 | Final Accuracy: 0.4033 | LR: 1.000000\n",
            "Epoch:  500 | Final Loss: 1.0626 | Final Accuracy: 0.3967 | LR: 1.000000\n",
            "Epoch:  600 | Final Loss: 1.0626 | Final Accuracy: 0.3967 | LR: 1.000000\n",
            "Epoch:  700 | Final Loss: 1.0626 | Final Accuracy: 0.4033 | LR: 1.000000\n",
            "Epoch:  800 | Final Loss: 1.0626 | Final Accuracy: 0.4000 | LR: 1.000000\n",
            "Epoch:  900 | Final Loss: 1.0626 | Final Accuracy: 0.4000 | LR: 1.000000\n",
            "Epoch:  999 | Final Loss: 1.0626 | Final Accuracy: 0.4000 | LR: 1.000000\n",
            "\n",
            "------------------------------------------------------------\n",
            "Training results: ADAPTIVE GRADIENT\n",
            "------------------------------------------------------------\n",
            "Epoch:    0 | Final Loss: 1.0986 | Final Accuracy: 0.3067 | LR: 0.500000\n",
            "Epoch:  100 | Final Loss: 1.0457 | Final Accuracy: 0.3933 | LR: 0.500000\n",
            "Epoch:  200 | Final Loss: 1.0435 | Final Accuracy: 0.3967 | LR: 0.500000\n",
            "Epoch:  300 | Final Loss: 1.0429 | Final Accuracy: 0.3967 | LR: 0.500000\n",
            "Epoch:  400 | Final Loss: 1.0426 | Final Accuracy: 0.3967 | LR: 0.500000\n",
            "Epoch:  500 | Final Loss: 1.0423 | Final Accuracy: 0.3933 | LR: 0.500000\n",
            "Epoch:  600 | Final Loss: 1.0421 | Final Accuracy: 0.3933 | LR: 0.500000\n",
            "Epoch:  700 | Final Loss: 1.0419 | Final Accuracy: 0.3933 | LR: 0.500000\n",
            "Epoch:  800 | Final Loss: 1.0416 | Final Accuracy: 0.3900 | LR: 0.500000\n",
            "Epoch:  900 | Final Loss: 1.0413 | Final Accuracy: 0.3933 | LR: 0.500000\n",
            "Epoch:  999 | Final Loss: 1.0407 | Final Accuracy: 0.3867 | LR: 0.500000\n",
            "\n",
            "------------------------------------------------------------\n",
            "SUMMARY RESULTS\n",
            "------------------------------------------------------------\n",
            "Vanilla      - Loss: 1.0624, Accuracy: 0.4000\n",
            "Learning Rate Decay - Loss: 1.0830, Accuracy: 0.4033\n",
            "Momentum     - Loss: 1.0626, Accuracy: 0.4000\n",
            "Adaptive Gradient - Loss: 1.0407, Accuracy: 0.3867\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# ============= DENSE LAYER CLASS =============\n",
        "class Layer_Dense:\n",
        "    def __init__(self, n_inputs, n_neurons):\n",
        "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "        self.biases = np.zeros((1, n_neurons))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "    def backward(self, dvalues):\n",
        "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
        "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
        "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
        "\n",
        "# ============= ACTIVATION FUNCTIONS =============\n",
        "class Activation_ReLU:\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        self.output = np.maximum(0, inputs)\n",
        "\n",
        "    def backward(self, dvalues):\n",
        "        self.dinputs = dvalues.copy()\n",
        "        self.dinputs[self.inputs <= 0] = 0\n",
        "\n",
        "class Activation_Softmax:\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
        "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "        self.output = probabilities\n",
        "\n",
        "    def backward(self, dvalues):\n",
        "        self.dinputs = np.empty_like(dvalues)\n",
        "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
        "            single_output = single_output.reshape(-1, 1)\n",
        "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
        "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
        "\n",
        "# ============= LOSS FUNCTIONS =============\n",
        "class Loss:\n",
        "    def calculate(self, output, y):\n",
        "        sample_losses = self.forward(output, y)\n",
        "        data_loss = np.mean(sample_losses)\n",
        "        return data_loss\n",
        "\n",
        "class Loss_CategoricalCrossEntropy(Loss):\n",
        "    def forward(self, y_pred, y_true):\n",
        "        samples = y_pred.shape[0]\n",
        "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "        if len(y_true.shape) == 1:\n",
        "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
        "        elif len(y_true.shape) == 2:\n",
        "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
        "\n",
        "        negative_log_likelihoods = -np.log(correct_confidences)\n",
        "        return negative_log_likelihoods\n",
        "\n",
        "    def backward(self, dvalues, y_true):\n",
        "        samples = len(dvalues)\n",
        "        labels = len(dvalues[0])\n",
        "\n",
        "        if len(y_true.shape) == 1:\n",
        "            y_true = np.eye(labels)[y_true]\n",
        "\n",
        "        self.dinputs = -y_true / dvalues\n",
        "        self.dinputs = self.dinputs / samples\n",
        "\n",
        "# ============= OPTIMIZER =============\n",
        "class Optimizer_SGD:\n",
        "    def __init__(self, learning_rate=1.0, decay=0.0, momentum=0.0, adaptive=False, epsilon=1e-7):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.initial_learning_rate = learning_rate\n",
        "        self.decay = decay\n",
        "        self.momentum = momentum\n",
        "        self.adaptive = adaptive\n",
        "        self.epsilon = epsilon\n",
        "        self.iterations = 0\n",
        "\n",
        "    def update_learning_rate(self):\n",
        "        \"\"\"Update learning rate with decay BEFORE forward and backward passes\"\"\"\n",
        "        if self.decay:\n",
        "            self.learning_rate = self.initial_learning_rate / (1 + self.decay * self.iterations)\n",
        "        else:\n",
        "            self.learning_rate = self.initial_learning_rate\n",
        "\n",
        "    def update_params(self, layer):\n",
        "        \"\"\"Update layer parameters with momentum/vanilla SGD AFTER backward pass\"\"\"\n",
        "\n",
        "        # Initialize momentum arrays on first call\n",
        "        if not hasattr(layer, 'weight_momentums'):\n",
        "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "\n",
        "        # Initialize cache arrays for adaptive gradient\n",
        "        if self.adaptive and not hasattr(layer, 'weight_cache'):\n",
        "            layer.weight_cache = np.zeros_like(layer.weights)\n",
        "            layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "        # ===== ADAPTIVE GRADIENT PATH =====\n",
        "        if self.adaptive:\n",
        "            # Accumulate squared gradients\n",
        "            layer.weight_cache += layer.dweights ** 2\n",
        "            layer.bias_cache += layer.dbiases ** 2\n",
        "\n",
        "            # Calculate adaptive weight and bias updates\n",
        "            weight_update = -(self.learning_rate * layer.dweights) / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "            bias_update = -(self.learning_rate * layer.dbiases) / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "\n",
        "            # Apply momentum to adaptive gradients if enabled\n",
        "            if self.momentum:\n",
        "                layer.weight_momentums = self.momentum * layer.weight_momentums + weight_update\n",
        "                layer.bias_momentums = self.momentum * layer.bias_momentums + bias_update\n",
        "                layer.weights += layer.weight_momentums\n",
        "                layer.biases += layer.bias_momentums\n",
        "            else:\n",
        "                layer.weights += weight_update\n",
        "                layer.biases += bias_update\n",
        "            return\n",
        "\n",
        "        # ===== STANDARD PATH (MOMENTUM OR VANILLA) =====\n",
        "        if self.momentum:\n",
        "            # Momentum update\n",
        "            weight_updates = self.momentum * layer.weight_momentums - self.learning_rate * layer.dweights\n",
        "            bias_updates = self.momentum * layer.bias_momentums - self.learning_rate * layer.dbiases\n",
        "\n",
        "            layer.weight_momentums = weight_updates\n",
        "            layer.bias_momentums = bias_updates\n",
        "\n",
        "            layer.weights += weight_updates\n",
        "            layer.biases += bias_updates\n",
        "        else:\n",
        "            # Vanilla SGD\n",
        "            layer.weights -= self.learning_rate * layer.dweights\n",
        "            layer.biases -= self.learning_rate * layer.dbiases\n",
        "\n",
        "    def post_update_params(self):\n",
        "        \"\"\"Increment iteration counter\"\"\"\n",
        "        self.iterations += 1\n",
        "\n",
        "# ============= TRAINING FUNCTION =============\n",
        "def train_model(optimizer_name, optimizer, epochs=1000):\n",
        "    print(\"\\n\" + \"-\"*60)\n",
        "    print(f\"Training results: {optimizer_name.upper()}\")\n",
        "    print(\"-\"*60)\n",
        "\n",
        "    # Create fresh dataset for each training run\n",
        "    X, y = spiral_data(samples=100, classes=3)\n",
        "\n",
        "    # Initialize network layers\n",
        "    dense1 = Layer_Dense(2, 3)\n",
        "    activation1 = Activation_ReLU()\n",
        "    dense2 = Layer_Dense(3, 3)\n",
        "    activation2 = Activation_Softmax()\n",
        "    loss_function = Loss_CategoricalCrossEntropy()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # UPDATE LEARNING RATE DECAY BEFORE FP and BP\n",
        "        optimizer.update_learning_rate()\n",
        "\n",
        "        # FORWARD PASS\n",
        "        dense1.forward(X)\n",
        "        activation1.forward(dense1.output)\n",
        "        dense2.forward(activation1.output)\n",
        "        activation2.forward(dense2.output)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = loss_function.calculate(activation2.output, y)\n",
        "\n",
        "        # Calculate accuracy\n",
        "        predictions = np.argmax(activation2.output, axis=1)\n",
        "        y_check = y.copy()\n",
        "        if len(y_check.shape) == 2:\n",
        "            y_check = np.argmax(y_check, axis=1)\n",
        "        accuracy = np.mean(predictions == y_check)\n",
        "\n",
        "        # BACKWARD PASS\n",
        "        loss_function.backward(activation2.output, y)\n",
        "        activation2.backward(loss_function.dinputs)\n",
        "        dense2.backward(activation2.dinputs)\n",
        "        activation1.backward(dense2.dinputs)\n",
        "        dense1.backward(activation1.dinputs)\n",
        "\n",
        "        # UPDATE WEIGHTS WITH MOMENTUM/SGD/ADAPTIVE AFTER BP\n",
        "        optimizer.update_params(dense1)\n",
        "        optimizer.update_params(dense2)\n",
        "        optimizer.post_update_params()\n",
        "\n",
        "        # Print progress every 100 epochs\n",
        "        if epoch % 100 == 0 or epoch == epochs - 1:\n",
        "            print(f\"Epoch: {epoch:4d} | Final Loss: {loss:.4f} | Final Accuracy: {accuracy:.4f} | LR: {optimizer.learning_rate:.6f}\")\n",
        "\n",
        "    return optimizer_name, loss, accuracy\n",
        "\n",
        "# ============= CREATE DATASET =============\n",
        "X, y = spiral_data(samples=100, classes=3)\n",
        "\n",
        "# ============= MAIN TRAINING EXECUTION =============\n",
        "print(\"\\n\" + \">\"*60)\n",
        "print(\"EXERCISE UNIT 4\")\n",
        "print(\">\"*60)\n",
        "\n",
        "results = []\n",
        "\n",
        "# METHOD 1: VANILLA SGD (No decay, no momentum)\n",
        "optimizer_vanilla = Optimizer_SGD(learning_rate=1.0)\n",
        "results.append(train_model(\"Vanilla\", optimizer_vanilla))\n",
        "\n",
        "# METHOD 2: LEARNING RATE DECAY\n",
        "optimizer_decay = Optimizer_SGD(learning_rate=1.0, decay=0.001)\n",
        "results.append(train_model(\"Learning Rate Decay\", optimizer_decay))\n",
        "\n",
        "# METHOD 3: MOMENTUM\n",
        "optimizer_momentum = Optimizer_SGD(learning_rate=1.0, momentum=0.9)\n",
        "results.append(train_model(\"Momentum\", optimizer_momentum))\n",
        "\n",
        "# METHOD 4: ADAPTIVE GRADIENT (AdaGrad)\n",
        "optimizer_adaptive = Optimizer_SGD(learning_rate=0.5, adaptive=True)\n",
        "results.append(train_model(\"Adaptive Gradient\", optimizer_adaptive))\n",
        "\n",
        "# ============= SUMMARY RESULTS =============\n",
        "print(\"\\n\" + \"-\"*60)\n",
        "print(\"SUMMARY RESULTS\")\n",
        "print(\"-\"*60)\n",
        "for method_name, final_loss, final_acc in results:\n",
        "    print(f\"{method_name.ljust(12)} - Loss: {final_loss:.4f}, Accuracy: {final_acc:.4f}\")\n",
        "print(\"-\"*60)"
      ]
    }
  ]
}
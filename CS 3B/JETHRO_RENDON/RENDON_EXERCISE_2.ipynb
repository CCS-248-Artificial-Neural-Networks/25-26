{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "26e07a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1\tChoose either one of the following tasks (the output of this task will be used on the next number):\n",
    "#a.\tDevelop a Class in Python called Dense_Layer (included in the submitted notebook).\n",
    "#b.\tCreate a Helper file called neural_network_helper (separate file from the notebook). \n",
    "\n",
    "#The chosen task should have the following functions:\n",
    "#a)\t(10 points) A function to setup/accept the inputs and weights\n",
    "#b)\t(10 points) A function to perform the weighted sum + bias\n",
    "#c)\t(15 points) A function to perform the selected activation function\n",
    "#d)\t(15 points) A function to calculate the loss (predicted output vs target output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d1655a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a.\tDevelop a Class in Python called Dense_Layer (included in the submitted notebook).\n",
    "import numpy as np\n",
    "\n",
    "class Dense_Layer:\n",
    "    def __init__(self, inputs, weights, bias, activation=None, name=\"\"):\n",
    "        self.inputs = np.array(inputs)\n",
    "        self.weights = np.array(weights)\n",
    "        self.bias = np.array(bias)\n",
    "        self.activation = activation\n",
    "        self.name = name\n",
    "        self.z = None\n",
    "        self.output = None\n",
    "\n",
    "#a)\t(10 points) A function to setup/accept the inputs and weights  \n",
    "    def weighted_sum(self):\n",
    "        return np.dot(self.inputs, self.weights) + self.bias\n",
    "\n",
    "#b)\t(10 points) A function to perform the weighted sum + bias\n",
    "    def activate(self, z):\n",
    "        if self.activation == \"relu\":\n",
    "            return np.maximum(0, z)\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            return 1 / (1 + np.exp(-z))\n",
    "        elif self.activation == \"softmax\":\n",
    "            exp_vals = np.exp(z - np.max(z))\n",
    "            return exp_vals / np.sum(exp_vals)\n",
    "        else:\n",
    "            return z\n",
    "        \n",
    "#c)\t(15 points) A function to perform the selected activation function\n",
    "    def forward(self, verbose=True):\n",
    "        self.z = self.weighted_sum()\n",
    "        self.output = self.activate(self.z)\n",
    "        if verbose:\n",
    "            print(f\"--- {self.name} ---\")\n",
    "            print(\"Weighted sum (z):\", np.round(self.z, 6))\n",
    "            print(\"Activated output:\", np.round(self.output, 6))\n",
    "            print()\n",
    "        return self.output\n",
    "    \n",
    "#d)\t(15 points) A function to calculate the loss (predicted output vs target output)\n",
    "def cross_entropy_loss(predicted, target):\n",
    "    predicted = np.clip(predicted, 1e-15, 1 - 1e-15)\n",
    "    return -np.sum(target * np.log(predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5aefd162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target output \n",
    "target_output = np.array([0.7, 0.2, -0.1])  \n",
    "# First Hidden Layer\n",
    "X = np.array([5.1, 3.5, 1.4, 0.2])\n",
    "\n",
    "# Weights and biases \n",
    "W1 = np.array([\n",
    "    [ 0.2,  0.5, -0.3],\n",
    "    [0.1, -0.2,  0.4],\n",
    "    [-0.4,  0.3,  0.2],\n",
    "    [ 0.6, -0.1,  0.5]\n",
    "])\n",
    "B1 = np.array([3.0, -2.1, 0.6])\n",
    "\n",
    "# Second Hidden Layer\n",
    "W2 = np.array([\n",
    "    [ 0.3, -0.5],\n",
    "    [ 0.7,  0.2],\n",
    "    [-0.6,  0.4]\n",
    "])\n",
    "B2 = np.array([4.3, 6.4])\n",
    "\n",
    "#Last layer â€“ Output Nodes, representing each of the Iris species\n",
    "W3 = np.array([\n",
    "    [ 0.5, -0.3,  0.8],\n",
    "    [-0.2,  0.6, -0.4]\n",
    "])\n",
    "B3 = np.array([-1.5, 2.1, -3.3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0ab33b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Hidden Layer 1 (ReLU) ---\n",
      "Weighted sum (z): [3.93 0.15 0.85]\n",
      "Activated output: [3.93 0.15 0.85]\n",
      "\n",
      "--- Hidden Layer 2 (Sigmoid) ---\n",
      "Weighted sum (z): [5.074 4.805]\n",
      "Activated output: [0.993782 0.991878]\n",
      "\n",
      "--- Output Layer (Softmax) ---\n",
      "Weighted sum (z): [-1.201485  2.396992 -2.901726]\n",
      "Activated output: [0.026507 0.968651 0.004841]\n",
      "\n",
      "Final predicted probabilities (softmax): [0.026507 0.968651 0.004841]\n",
      "Predicted class: 1 -> Iris-versicolor\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# First Hidden Layer (ReLU)\n",
    "layer1 = Dense_Layer(X, W1, B1, activation=\"relu\", name=\"Hidden Layer 1 (ReLU)\")\n",
    "out1 = layer1.forward()\n",
    "\n",
    "# Second Hidden Layer (Sigmoid)\n",
    "layer2 = Dense_Layer(out1, W2, B2, activation=\"sigmoid\", name=\"Hidden Layer 2 (Sigmoid)\")\n",
    "out2 = layer2.forward()\n",
    "\n",
    "# Output Layer (Softmax)\n",
    "layer3 = Dense_Layer(out2, W3, B3, activation=\"softmax\", name=\"Output Layer (Softmax)\")\n",
    "out3 = layer3.forward()\n",
    "\n",
    "print(\"Final predicted probabilities (softmax):\", np.round(out3, 6))\n",
    "\n",
    "# Predicted class\n",
    "class_names = [\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"]\n",
    "pred_class_idx = np.argmax(out3)\n",
    "print(\"Predicted class:\", pred_class_idx, \"->\", class_names[pred_class_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae8f531c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss (with given target): 0.351803\n",
      "Cross-entropy Loss (using one-hot target): 3.630328\n"
     ]
    }
   ],
   "source": [
    "# Loss calculations\n",
    "# MSE with the provided target (even though it includes a negative value)\n",
    "mse_loss = np.mean((out3 - target_output)**2)\n",
    "\n",
    "# Cross-entropy with a one-hot target based on the argmax of given target vector\n",
    "one_hot_target = np.zeros_like(out3)\n",
    "one_hot_target[np.argmax(target_output)] = 1\n",
    "cross_entropy = cross_entropy_loss(out3, one_hot_target)\n",
    "\n",
    "print(\"MSE Loss (with given target):\", np.round(mse_loss, 6))\n",
    "print(\"Cross-entropy Loss (using one-hot target):\", np.round(cross_entropy, 6))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

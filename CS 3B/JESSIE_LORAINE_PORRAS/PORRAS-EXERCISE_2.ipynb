{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dce6046e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. a.\tDevelop a Class in Python called Dense_Layer (included in the submitted notebook).\n",
    "\n",
    "#a)\t(10 points) A function to setup/accept the inputs and weights\n",
    "#b)\t(10 points) A function to perform the weighted sum + bias\n",
    "#c)\t(15 points) A function to perform the selected activation function\n",
    "#d)\t(15 points) A function to calculate the loss (predicted output vs target output)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Dense_Layer:\n",
    "\n",
    "    def __init__(self): #setup/accept inputs and weights\n",
    "        \n",
    "        self.inputs = None\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.output = None\n",
    "        \n",
    "\n",
    "    def summation(self, inputs, weights, bias): #perform weighted sum + bias\n",
    "\n",
    "        weights_transposed = [list(row) for row in zip(*weights)]\n",
    "        result= []\n",
    "\n",
    "        for i in range(len(weights_transposed)):\n",
    "            sum_val = 0\n",
    "            for j in range(len(inputs)):\n",
    "                sum_val += weights_transposed[i][j] * inputs [j][0]\n",
    "            sum_val += bias[i][0]\n",
    "            result.append([sum_val])\n",
    "\n",
    "        return np.array(result)\n",
    "        \n",
    "    \n",
    "    def activation(self, z, func): #for activation function\n",
    "\n",
    "        z = np.array(z)\n",
    "        if func == \"relu\":\n",
    "            self.output = np.maximum(0,z)\n",
    "        elif func == \"sigmoid\":\n",
    "            self.output = 1/(1 + np.exp(-z))\n",
    "        elif func == \"softmax\":\n",
    "            z_arr = np.array(z).reshape(-1)\n",
    "            exp_values = np.exp(z_arr - np.max(z_arr))\n",
    "            soft = exp_values/ np.sum(exp_values)\n",
    "            self.output = soft.reshape((-1,1))\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function.\")\n",
    "        return self.output\n",
    "\n",
    "    def loss_function(self, y_target): # for loss function\n",
    "\n",
    "        y_target = np.array(y_target)\n",
    "        y_pred = np.array(self.output)\n",
    "\n",
    "        lf = 0.5 * np.sum((y_target - y_pred) ** 2)\n",
    "        return lf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac0a273b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs(X) of first hidden layer are:\n",
      "[[5.1]\n",
      " [3.5]\n",
      " [1.4]\n",
      " [0.2]]\n",
      "                                  \n",
      "\n",
      "Weights(W) of first hidden layer are: \n",
      "[[ 0.2  0.5 -0.4]\n",
      " [ 0.1 -0.2  0.4]\n",
      " [-0.4  0.3  0.2]\n",
      " [ 0.6 -0.1  0.5]]\n",
      "                                  \n",
      "\n",
      "bias(B) of first hidden layer are: \n",
      "[[ 3. ]\n",
      " [-2.1]\n",
      " [ 0.6]]\n",
      "                                  \n",
      "\n",
      "Value of z for first hidden layer is: \n",
      "[[3.93]\n",
      " [0.15]\n",
      " [0.34]]\n",
      "                                  \n",
      "\n",
      "Output of Hidden layer one is: \n",
      "[[3.93]\n",
      " [0.15]\n",
      " [0.34]]\n",
      "                                  \n",
      "\n",
      "Inputs(X) of second hidden layer are:\n",
      "[[3.93]\n",
      " [0.15]\n",
      " [0.34]]\n",
      "                                  \n",
      "\n",
      "Weights(W) of second hidden layer are: \n",
      "[[ 0.3 -0.5]\n",
      " [ 0.7  0.2]\n",
      " [-0.6  0.4]]\n",
      "                                  \n",
      "\n",
      "bias(B) of second hidden layer are: \n",
      "[[4.3]\n",
      " [6.4]]\n",
      "                                  \n",
      "\n",
      "Value of z for second hidden layer is: \n",
      "[[5.38 ]\n",
      " [4.601]]\n",
      "                                  \n",
      "\n",
      "Output of Hidden layer two is: \n",
      "[[0.99541331]\n",
      " [0.99005805]]\n",
      "                                  \n",
      "\n",
      "Inputs(X) of last layer are:\n",
      "[[0.99541331]\n",
      " [0.99005805]]\n",
      "                                  \n",
      "\n",
      "Weights(W) of last layer are: \n",
      "[[ 0.5 -0.3  0.8]\n",
      " [-0.2  0.6 -0.4]]\n",
      "                                  \n",
      "\n",
      "bias(B) of last layer are: \n",
      "[[-1.5]\n",
      " [ 2.1]\n",
      " [-3.3]]\n",
      "                                  \n",
      "\n",
      "Value of z for last layer is: \n",
      "[[-1.20030495]\n",
      " [ 2.39541083]\n",
      " [-2.89969257]]\n",
      "                                  \n",
      "\n",
      "Output of last layer is: \n",
      "[[0.02657838]\n",
      " [0.96856322]\n",
      " [0.00485841]]\n",
      "                                  \n",
      "\n",
      "Loss function = 0.5266190129422094\n",
      "                                  \n",
      "\n",
      "Class is likely Iris-versicolor\n"
     ]
    }
   ],
   "source": [
    "#2. a Iris dataset - determining what class the following inputs are\n",
    "# by calculatingh the output, given the neural network configurations:\n",
    "\n",
    "#First Hidden Layer\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "z_one = 0.00\n",
    "\n",
    "inputs = [\n",
    "    [5.1],\n",
    "    [3.5],\n",
    "    [1.4],\n",
    "    [0.2]\n",
    "]\n",
    "\n",
    "weights_one = [\n",
    "    [ 0.2, 0.5, -0.4 ],\n",
    "    [ 0.1, -0.2, 0.4],\n",
    "    [-0.4, 0.3, 0.2],\n",
    "    [0.6, -0.1, 0.5]\n",
    "]\n",
    "\n",
    "bias_one = [\n",
    "    [3.0],\n",
    "    [-2.1],\n",
    "    [0.6]\n",
    "]\n",
    "\n",
    "target_outputs = [\n",
    "    [0.7],\n",
    "    [0.2],\n",
    "    [0.1]\n",
    "]\n",
    "\n",
    "#calculating z and activation function values\n",
    "\n",
    "layer_one = Dense_Layer()\n",
    "\n",
    "z_one = layer_one.summation(inputs, weights_one, bias_one)\n",
    "output_one = layer_one.activation(z_one, \"relu\")\n",
    "\n",
    "#printing results\n",
    "\n",
    "print(f\"Inputs(X) of first hidden layer are:\\n{np.array(inputs)}\")\n",
    "print(\"                                  \\n\")\n",
    "print(f\"Weights(W) of first hidden layer are: \\n{np.array(weights_one)}\")\n",
    "print(\"                                  \\n\")\n",
    "print(f\"bias(B) of first hidden layer are: \\n{np.array(bias_one)}\")\n",
    "print(\"                                  \\n\")\n",
    "print(f\"Value of z for first hidden layer is: \\n{z_one}\")\n",
    "print(\"                                  \\n\")\n",
    "print(f\"Output of Hidden layer one is: \\n{output_one}\")\n",
    "print(\"                                  \\n\")\n",
    "\n",
    "\n",
    "#Second Hidden Layer\n",
    "\n",
    "inputs_two = output_one\n",
    "\n",
    "weights_two = [\n",
    "    [ 0.3, -0.5],\n",
    "    [ 0.7, 0.2],\n",
    "    [-0.6, 0.4]\n",
    "]\n",
    "\n",
    "bias_two = [\n",
    "    [4.3],\n",
    "    [6.4]\n",
    "]\n",
    "\n",
    "layer_two = Dense_Layer()\n",
    "\n",
    "z_two = layer_two.summation(inputs_two, weights_two, bias_two)\n",
    "output_two = layer_two.activation(z_two, \"sigmoid\")\n",
    "\n",
    "print(f\"Inputs(X) of second hidden layer are:\\n{np.array(inputs_two)}\")\n",
    "print(\"                                  \\n\")\n",
    "print(f\"Weights(W) of second hidden layer are: \\n{np.array(weights_two)}\")\n",
    "print(\"                                  \\n\")\n",
    "print(f\"bias(B) of second hidden layer are: \\n{np.array(bias_two)}\")\n",
    "print(\"                                  \\n\")\n",
    "print(f\"Value of z for second hidden layer is: \\n{z_two}\")\n",
    "print(\"                                  \\n\")\n",
    "print(f\"Output of Hidden layer two is: \\n{output_two}\")\n",
    "print(\"                                  \\n\")\n",
    "\n",
    "#Last Layer\n",
    "\n",
    "inputs_three = output_two\n",
    "\n",
    "weights_three = [\n",
    "    [ 0.5, -0.3, 0.8],\n",
    "    [ -0.2, 0.6, -0.4]\n",
    "]\n",
    "\n",
    "bias_three = [\n",
    "    [-1.5],\n",
    "    [2.1],\n",
    "    [-3.3]\n",
    "]\n",
    "\n",
    "layer_three = Dense_Layer()\n",
    "\n",
    "z_three = layer_three.summation(inputs_three, weights_three, bias_three)\n",
    "output_three = layer_three.activation(z_three, \"softmax\")\n",
    "\n",
    "print(f\"Inputs(X) of last layer are:\\n{np.array(inputs_three)}\")\n",
    "print(\"                                  \\n\")\n",
    "print(f\"Weights(W) of last layer are: \\n{np.array(weights_three)}\")\n",
    "print(\"                                  \\n\")\n",
    "print(f\"bias(B) of last layer are: \\n{np.array(bias_three)}\")\n",
    "print(\"                                  \\n\")\n",
    "print(f\"Value of z for last layer is: \\n{z_three}\")\n",
    "print(\"                                  \\n\")\n",
    "print(f\"Output of last layer is: \\n{output_three}\")\n",
    "print(\"                                  \\n\")\n",
    "\n",
    "#calculating loss function\n",
    "\n",
    "loss = layer_three.loss_function(target_outputs)\n",
    "\n",
    "#predicting class of Iris\n",
    "predicted = np.argmax(output_three)\n",
    "\n",
    "iris_classes = [\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"]\n",
    "predicted_class = iris_classes[predicted]\n",
    "\n",
    "print(f\"Loss function = {loss}\")\n",
    "print(\"                                  \\n\")\n",
    "print(f\"Class is likely {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75f7ec45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs(X) of first hidden layer are:\n",
      "[[14.1  ]\n",
      " [20.3  ]\n",
      " [ 0.095]]\n",
      "\n",
      "Weights(W) of first hidden layer are: \n",
      "[[ 0.5 -0.3  0.8]\n",
      " [ 0.2  0.4 -0.6]\n",
      " [-0.7  0.9  0.1]]\n",
      "\n",
      "Bias(B) of first hidden layer are: \n",
      "[[ 0.3]\n",
      " [-0.5]\n",
      " [ 0.6]]\n",
      "\n",
      "Value of z for first hidden layer is: \n",
      "[[11.3435]\n",
      " [ 3.4755]\n",
      " [-0.2905]]\n",
      "\n",
      "Output of Hidden layer one is: \n",
      "[[11.3435]\n",
      " [ 3.4755]\n",
      " [ 0.    ]]\n",
      "\n",
      "Inputs(X) of second hidden layer are:\n",
      "[[11.3435]\n",
      " [ 3.4755]\n",
      " [ 0.    ]]\n",
      "\n",
      "Weights(W) of second hidden layer are: \n",
      "[[ 0.6 -0.3]\n",
      " [-0.2  0.5]\n",
      " [ 0.4  0.7]]\n",
      "\n",
      "Bias(B) of second hidden layer are: \n",
      "[[ 0.1]\n",
      " [-0.8]]\n",
      "\n",
      "Value of z for second hidden layer is: \n",
      "[[ 6.211 ]\n",
      " [-2.4653]]\n",
      "\n",
      "Output of Hidden layer two is: \n",
      "[[0.99799679]\n",
      " [0.07832686]]\n",
      "\n",
      "Inputs(X) of output layer are:\n",
      "[[0.99799679]\n",
      " [0.07832686]]\n",
      "\n",
      "Weights(W) of output layer are: \n",
      "[[ 0.7]\n",
      " [-0.5]]\n",
      "\n",
      "Bias(B) of output layer are: \n",
      "[[0.2]]\n",
      "\n",
      "Value of z for output layer is: \n",
      "[[0.85943432]]\n",
      "\n",
      "Output of last layer is: \n",
      "[[0.70254245]]\n",
      "\n",
      "Loss function = 0.04424049571307605\n",
      "\n",
      "Tumor Type: (1) Malignant\n"
     ]
    }
   ],
   "source": [
    "# 2. b)Given the following inputs from the Breast Cancer Dataset, \n",
    "#      using three features: Mean Radius, Mean Texture, and Mean Smoothness, \n",
    "#      determine whether the tumor is Benign (0) or Malignant (1) \n",
    "#      by calculating the network outputs step by step, given the following \n",
    "#      neural network configuration:\n",
    "\n",
    "# Step 0: Inputs and Target\n",
    "inputs = [\n",
    "    [14.1],\n",
    "    [20.3],\n",
    "    [0.095]\n",
    "]\n",
    "\n",
    "target_output = [1]\n",
    "\n",
    "# First Hidden Layer\n",
    "weights_one = [\n",
    "    [0.5, -0.3, 0.8],\n",
    "    [0.2, 0.4, -0.6],\n",
    "    [-0.7, 0.9, 0.1]\n",
    "]\n",
    "\n",
    "bias_one = [\n",
    "    [0.3],\n",
    "    [-0.5],\n",
    "    [0.6]\n",
    "]\n",
    "\n",
    "layer_one = Dense_Layer()\n",
    "z_one = layer_one.summation(inputs, weights_one, bias_one)\n",
    "output_one = layer_one.activation(z_one, \"relu\")\n",
    "\n",
    "print(f\"Inputs(X) of first hidden layer are:\\n{np.array(inputs)}\\n\")\n",
    "print(f\"Weights(W) of first hidden layer are: \\n{np.array(weights_one)}\\n\")\n",
    "print(f\"Bias(B) of first hidden layer are: \\n{np.array(bias_one)}\\n\")\n",
    "print(f\"Value of z for first hidden layer is: \\n{z_one}\\n\")\n",
    "print(f\"Output of Hidden layer one is: \\n{output_one}\\n\")\n",
    "\n",
    "# Second Hidden Layer\n",
    "\n",
    "inputs_two = output_one\n",
    "\n",
    "weights_two = [ \n",
    "    [0.6, -0.3], \n",
    "    [-0.2, 0.5], \n",
    "    [0.4, 0.7] \n",
    "]\n",
    "\n",
    "bias_two = [ [0.1], [-0.8] ]\n",
    "\n",
    "layer_two = Dense_Layer()\n",
    "\n",
    "z_two = layer_two.summation(inputs_two, weights_two, bias_two)\n",
    "output_two = layer_two.activation(z_two, \"sigmoid\")\n",
    "\n",
    "print(f\"Inputs(X) of second hidden layer are:\\n{np.array(inputs_two)}\\n\")\n",
    "print(f\"Weights(W) of second hidden layer are: \\n{np.array(weights_two)}\\n\")\n",
    "print(f\"Bias(B) of second hidden layer are: \\n{np.array(bias_two)}\\n\")\n",
    "print(f\"Value of z for second hidden layer is: \\n{z_two}\\n\")\n",
    "print(f\"Output of Hidden layer two is: \\n{output_two}\\n\")\n",
    "\n",
    "\n",
    "# Last Layer\n",
    "\n",
    "inputs_three = output_two\n",
    "\n",
    "weights_three = [\n",
    "    [0.7],\n",
    "    [-0.5]\n",
    "]\n",
    "bias_three = [\n",
    "    [0.2]\n",
    "]\n",
    "\n",
    "layer_three = Dense_Layer()\n",
    "\n",
    "z_three = layer_three.summation(inputs_three, weights_three, bias_three)\n",
    "output_three = layer_three.activation(z_three, \"sigmoid\")  # final output\n",
    "\n",
    "print(f\"Inputs(X) of output layer are:\\n{np.array(inputs_three)}\\n\")\n",
    "print(f\"Weights(W) of output layer are: \\n{np.array(weights_three)}\\n\")\n",
    "print(f\"Bias(B) of output layer are: \\n{np.array(bias_three)}\\n\")\n",
    "print(f\"Value of z for output layer is: \\n{z_three}\\n\")\n",
    "print(f\"Output of last layer is: \\n{output_three}\\n\")\n",
    "\n",
    "# calculating loss function\n",
    "loss = layer_three.loss_function(target_output)\n",
    "print(f\"Loss function = {loss}\\n\")\n",
    "\n",
    "# Predicted Class and Tumor Type\n",
    "predicted_value = output_three[0][0]\n",
    "predicted_class = 1 if output_three[0][0] >= 0.5 else 0\n",
    "tumor_type = \"Malignant\" if predicted_class == 1 else \"Benign\"\n",
    "\n",
    "print(f\"Tumor Type: ({predicted_class}) {tumor_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cb6d64",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

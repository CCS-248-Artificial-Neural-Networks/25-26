{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Unit 2 Excercise\n",
        "\n",
        "Author: Zuriel Eliazar D. Calix\n",
        "\n",
        "Date: September 12, 2025\n",
        "\n"
      ],
      "metadata": {
        "id": "30htsy16JiGY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "omo74IlVBeGY"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "4tdXQAPiA8l7"
      },
      "outputs": [],
      "source": [
        "class Dense_Layer:\n",
        "    \"\"\"\n",
        "    Custom dense layer class\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, inputs, weights, bias, activation=None):\n",
        "        # (a) Setup: accept inputs, weights, and bias\n",
        "        self.inputs = np.asarray(inputs, dtype=float).ravel()\n",
        "        self.weights = np.asarray(weights, dtype=float)\n",
        "        self.bias = np.asarray(bias, dtype=float).ravel()\n",
        "        self.activation = activation\n",
        "        self.z = None\n",
        "        self.a = None\n",
        "\n",
        "    def forward(self):\n",
        "        # (b) Weighted sum + bias\n",
        "        x = self.inputs\n",
        "        W = self.weights\n",
        "        b = self.bias\n",
        "\n",
        "        if W.ndim == 1:\n",
        "            z = np.dot(x, W) + (b.item() if b.size == 1 else b)\n",
        "        elif W.ndim == 2:\n",
        "            if W.shape[1] == x.size:    # teacher-style layout\n",
        "                z = W.dot(x)\n",
        "            elif W.shape[0] == x.size:  # alt layout\n",
        "                z = x.dot(W)\n",
        "            else:\n",
        "                raise ValueError(f\"Incompatible shape {W.shape} for input size {x.size}\")\n",
        "            if b.size == 1:\n",
        "                z = z + b.item()\n",
        "            elif b.size == z.size:\n",
        "                z = z + b\n",
        "            else:\n",
        "                raise ValueError(f\"Bias shape {b.shape} incompatible with z shape {z.shape}\")\n",
        "        else:\n",
        "            raise ValueError(\"Weights must be 1-D or 2-D\")\n",
        "\n",
        "        self.z = np.asarray(z, dtype=float)\n",
        "\n",
        "        # (c) Activation function\n",
        "        if self.activation is None:\n",
        "            self.a = self.z\n",
        "        elif self.activation.lower() == 'relu':\n",
        "            self.a = np.maximum(0.0, self.z)\n",
        "        elif self.activation.lower() == 'sigmoid':\n",
        "            z_clip = np.clip(self.z, -500, 500)\n",
        "            self.a = 1.0 / (1.0 + np.exp(-z_clip))\n",
        "        else:\n",
        "            raise ValueError(\"Unsupported activation. Use None, 'relu', or 'sigmoid'.\")\n",
        "\n",
        "        return self.a\n",
        "\n",
        "    @staticmethod\n",
        "    def loss(predicted, target, eps=1e-12, reduction='mean'):\n",
        "        # (d) Loss function: binary cross-entropy\n",
        "        p = np.clip(np.asarray(predicted, dtype=float).ravel(), eps, 1.0 - eps)\n",
        "        t = np.asarray(target, dtype=float).ravel()\n",
        "\n",
        "        if t.shape != p.shape:\n",
        "            if t.size == 1:\n",
        "                t = np.full_like(p, t.item())\n",
        "            else:\n",
        "                raise ValueError(\"predicted and target shapes incompatible\")\n",
        "\n",
        "        per_elem = - (t * np.log(p) + (1.0 - t) * np.log(1.0 - p))\n",
        "\n",
        "        if reduction == 'mean':\n",
        "            return float(np.mean(per_elem))\n",
        "        elif reduction == 'sum':\n",
        "            return float(np.sum(per_elem))\n",
        "        elif reduction == 'none':\n",
        "            return per_elem\n",
        "        else:\n",
        "            raise ValueError(\"reduction must be 'mean', 'sum' or 'none'\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = [14.1, 20.3, 0.095]\n",
        "target_output = [1]\n"
      ],
      "metadata": {
        "id": "89v9cFuREI9d"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W1 = [[0.5, -0.3, 0.8],\n",
        "      [0.2, 0.4, -0.6],\n",
        "      [-0.7, 0.9, 0.1]]\n",
        "B1 = [0.3, -0.5, 0.6]\n",
        "\n",
        "layer1 = Dense_Layer(X, W1, B1, activation='relu')\n",
        "A1 = layer1.forward()\n",
        "Z1 = layer1.z\n",
        "A1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xz3gDn7VF6Fv",
        "outputId": "37a73d9e-a481-4edb-eb5c-d5b282f75117"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 1.336 , 10.383 ,  9.0095])"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W2 = [[0.6, -0.2, 0.4],\n",
        "      [-0.3, 0.5, 0.7]]\n",
        "B2 = [0.1, -0.8]\n",
        "\n",
        "layer2 = Dense_Layer(A1, W2, B2, activation='sigmoid')\n",
        "A2 = layer2.forward()\n",
        "Z2 = layer2.z\n",
        "A2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLi56OIdF7p5",
        "outputId": "5975e5f1-0465-43b7-c5d8-4fbd8bb27a99"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.91899725, 0.99996628])"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "W3 = [0.7, -0.5]\n",
        "B3 = [0.2]\n",
        "\n",
        "layer3 = Dense_Layer(A2, W3, B3, activation='sigmoid')\n",
        "A3 = layer3.forward()\n",
        "Z3 = layer3.z\n",
        "A3\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSYKvEoeF-Rr",
        "outputId": "f52ae34f-26e5-4b48-9332-c9f29c58c9ad"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "np.float64(0.5849955347015883)"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = Dense_Layer.loss(A3, target_output, reduction='mean')\n",
        "loss\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlST0TB2GAIh",
        "outputId": "80c7abc8-abb8-46ac-9974-9f33f3d3b6b5"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.53615106476815"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Final summary cell (complete version) ===\n",
        "import numpy as np\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"COMPLETE FORWARD PASS SUMMARY\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Input: {np.array(X)}\")\n",
        "print(f\"Target: {np.array(target_output)}  (Malignant = 1, Benign = 0)\")\n",
        "print()\n",
        "\n",
        "print(f\"{'Layer':<15}{'Shape':<15}{'Activation':<15}{'Output Values'}\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Input\n",
        "print(f\"{'Input':<15}{str(np.shape(X)):<15}{'None':<15}{np.round(np.array(X),6)}\")\n",
        "\n",
        "# Hidden 1\n",
        "print(f\"{'Hidden 1 Z1':<15}{str(np.shape(Z1)):<15}{'Linear':<15}{np.round(np.asarray(Z1),6)}\")\n",
        "print(f\"{'Hidden 1 A1':<15}{str(np.shape(A1)):<15}{'ReLU':<15}{np.round(np.asarray(A1),6)}\")\n",
        "\n",
        "# Hidden 2\n",
        "print(f\"{'Hidden 2 Z2':<15}{str(np.shape(Z2)):<15}{'Linear':<15}{np.round(np.asarray(Z2),6)}\")\n",
        "print(f\"{'Hidden 2 A2':<15}{str(np.shape(A2)):<15}{'Sigmoid':<15}{np.round(np.asarray(A2),6)}\")\n",
        "\n",
        "# Output\n",
        "print(f\"{'Output Z3':<15}{str(np.shape(Z3)):<15}{'Linear':<15}{np.round(np.asarray(Z3),6)}\")\n",
        "print(f\"{'Output A3':<15}{str(np.shape(A3)):<15}{'Sigmoid':<15}{np.round(np.asarray(A3),6)}\")\n",
        "\n",
        "print()\n",
        "print(f\"{'Metrics':<25}{'Value'}\")\n",
        "print(\"-\"*40)\n",
        "pred_val = float(np.asarray(A3).ravel().item())\n",
        "pred_class = 'Malignant (1)' if pred_val >= 0.5 else 'Benign (0)'\n",
        "print(f\"{'Predicted Class':<25}{pred_class}\")\n",
        "print(f\"{'Confidence':<25}{pred_val:.6f}\")\n",
        "print(f\"{'Binary Cross-Entropy Loss':<25}{loss:.6f}\")\n",
        "correct = (1 if pred_val >= 0.5 else 0) == int(np.asarray(target_output).ravel().item())\n",
        "print(f\"{'Correct Prediction':<25}{correct}\")\n",
        "\n",
        "print()\n",
        "print(\"=== Architecture Summary ===\")\n",
        "print(\"Total Parameters:\")\n",
        "print(f\"  - W1: {np.size(W1)}, B1: {np.size(B1)}\")\n",
        "print(f\"  - W2: {np.size(W2)}, B2: {np.size(B2)}\")\n",
        "print(f\"  - W3: {np.size(W3)}, B3: {np.size(B3)}\")\n",
        "total_params = np.size(W1) + np.size(B1) + np.size(W2) + np.size(B2) + np.size(W3) + np.size(B3)\n",
        "print(f\"  - Total: {total_params} parameters\")\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbvkSMWyNxqV",
        "outputId": "1e2ab655-a9cc-4c26-acb9-708a09b06b22"
      },
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "COMPLETE FORWARD PASS SUMMARY\n",
            "================================================================================\n",
            "Input: [14.1   20.3    0.095]\n",
            "Target: [1]  (Malignant = 1, Benign = 0)\n",
            "\n",
            "Layer          Shape          Activation     Output Values\n",
            "--------------------------------------------------------------------------------\n",
            "Input          (3,)           None           [14.1   20.3    0.095]\n",
            "Hidden 1 Z1    (3,)           Linear         [ 1.336  10.383   9.0095]\n",
            "Hidden 1 A1    (3,)           ReLU           [ 1.336  10.383   9.0095]\n",
            "Hidden 2 Z2    (2,)           Linear         [ 2.4288  10.29735]\n",
            "Hidden 2 A2    (2,)           Sigmoid        [0.918997 0.999966]\n",
            "Output Z3      ()             Linear         0.343315\n",
            "Output A3      ()             Sigmoid        0.584996\n",
            "\n",
            "Metrics                  Value\n",
            "----------------------------------------\n",
            "Predicted Class          Malignant (1)\n",
            "Confidence               0.584996\n",
            "Binary Cross-Entropy Loss0.536151\n",
            "Correct Prediction       True\n",
            "\n",
            "=== Architecture Summary ===\n",
            "Total Parameters:\n",
            "  - W1: 9, B1: 3\n",
            "  - W2: 6, B2: 2\n",
            "  - W3: 2, B3: 1\n",
            "  - Total: 23 parameters\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# === Hidden Layer 2 + Loss Summary ===\n",
        "print(\"=\"*80)\n",
        "print(\"HIDDEN LAYER 2 & LOSS DETAILS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(f\"{'Hidden Layer 2 Z2              ':<25}{np.round(np.asarray(Z2), 6)}\")\n",
        "print(f\"{'Hidden Layer 2 A2 (Sigmoid)    ':<25}{np.round(np.asarray(A2), 6)}\")\n",
        "print()\n",
        "print(f\"{'Output Z3                      ':<25}{np.round(np.asarray(Z3), 6)}\")\n",
        "print(f\"{'Output A3 (Sigmoid)            ':<25}{np.round(np.asarray(A3), 6)}\")\n",
        "print()\n",
        "print(f\"{'Binary Cross-Entropy Loss      ':<25}{loss:.6f}\")\n",
        "print(\"=\"*80)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQXhYEdaOkGl",
        "outputId": "0fe02bf1-4b92-4c78-b62c-e11b32eefac5"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "HIDDEN LAYER 2 & LOSS DETAILS\n",
            "================================================================================\n",
            "Hidden Layer 2 Z2              [ 2.4288  10.29735]\n",
            "Hidden Layer 2 A2 (Sigmoid)    [0.918997 0.999966]\n",
            "\n",
            "Output Z3                      0.343315\n",
            "Output A3 (Sigmoid)            0.584996\n",
            "\n",
            "Binary Cross-Entropy Loss      0.536151\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40a24fb9",
   "metadata": {},
   "source": [
    "# **Exercise for Unit 4**\n",
    "\n",
    "**BSCS 3B-AI**\n",
    "\n",
    "Submitted by:\n",
    "- Gabriel M. Diana\n",
    "- Ken Meiro C. Villareal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d2270f",
   "metadata": {},
   "source": [
    "___\n",
    "*Note: Save your Python source codes into a single .ipynb file with the proper naming convention (see Readme on the repository), upload it to your assigned folder in the GitHub organization CCS-248-Artificial-Neural-Networks, repository “25-26”.*\n",
    "\n",
    "1. Study the Backpropagation implementation uploaded here, and perform the following:\n",
    "    Set up the code so that it will perform the Forward Pass (FP), Backpropagation (BP) and weight update in 1000 epochs.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a0ca22",
   "metadata": {},
   "source": [
    "___\n",
    "### 1.) **FORWARD PASS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f3f807c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# 1. Initialize data and parameters\u001b[39;00m\n\u001b[32m      4\u001b[39m \n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Example: simple XOR dataset\u001b[39;00m\n\u001b[32m      6\u001b[39m X = np.array([[\u001b[32m0\u001b[39m,\u001b[32m0\u001b[39m],\n\u001b[32m      7\u001b[39m               [\u001b[32m0\u001b[39m,\u001b[32m1\u001b[39m],\n\u001b[32m      8\u001b[39m               [\u001b[32m1\u001b[39m,\u001b[32m0\u001b[39m],\n\u001b[32m      9\u001b[39m               [\u001b[32m1\u001b[39m,\u001b[32m1\u001b[39m]])\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 1. Initialize data and parameters\n",
    "\n",
    "# Example: simple XOR dataset\n",
    "X = np.array([[0,0],\n",
    "              [0,1],\n",
    "              [1,0],\n",
    "              [1,1]])\n",
    "\n",
    "y = np.array([[0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [0]])\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Network architecture\n",
    "input_neurons = 2\n",
    "hidden_neurons = 3\n",
    "output_neurons = 1\n",
    "learning_rate = 0.1\n",
    "epochs = 1000\n",
    "\n",
    "# Initialize weights and biases\n",
    "W1 = np.random.randn(input_neurons, hidden_neurons)\n",
    "b1 = np.zeros((1, hidden_neurons))\n",
    "W2 = np.random.randn(hidden_neurons, output_neurons)\n",
    "b2 = np.zeros((1, output_neurons))\n",
    "\n",
    "# 2. Define activation functions\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# 3. Training Loop (FP + BP + Update)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # ---- Forward Pass ----\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "    \n",
    "    # ---- Compute Error ----\n",
    "    error = y - a2\n",
    "    \n",
    "    # ---- Backpropagation ----\n",
    "    d_output = error * sigmoid_derivative(a2)\n",
    "    d_hidden = d_output.dot(W2.T) * sigmoid_derivative(a1)\n",
    "    \n",
    "    # ---- Update Weights ----\n",
    "    W2 += a1.T.dot(d_output) * learning_rate\n",
    "    b2 += np.sum(d_output, axis=0, keepdims=True) * learning_rate\n",
    "    W1 += X.T.dot(d_hidden) * learning_rate\n",
    "    b1 += np.sum(d_hidden, axis=0, keepdims=True) * learning_rate\n",
    "    \n",
    "\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        loss = np.mean(np.square(error))\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {loss:.6f}\")\n",
    "\n",
    "\n",
    "# 4. Final Predictions\n",
    "print(\"\\nFinal outputs after training:\")\n",
    "print(a2.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5264eba9",
   "metadata": {},
   "source": [
    "### 2.) **Modify the Optimizer class so that it will accept 3 optimizers we've discussed**\n",
    "\n",
    "a. Learning rate decay\n",
    "\n",
    "b. Momentum\n",
    "\n",
    "c. Adaptive Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f6b101",
   "metadata": {},
   "source": [
    "**Hint: Updating the learning decay rate happens before running both FP and BP, implementing momentum, and vanilla SGD happens after the learning rate decay**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11023495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Optimizer:\n",
    "    def __init__(self, lr=0.1, decay=0.0, momentum=0.0, use_adagrad=False):\n",
    "        # Initialize main optimizer settings\n",
    "        self.lr = lr\n",
    "        self.initial_lr = lr\n",
    "        self.decay = decay\n",
    "        self.momentum = momentum\n",
    "        self.use_adagrad = use_adagrad\n",
    "        self.iterations = 0\n",
    "\n",
    "        # Initialize caches for momentum and Adagrad\n",
    "        self.v_W1 = 0; self.v_b1 = 0; self.v_W2 = 0; self.v_b2 = 0\n",
    "        self.G_W1 = 1e-8; self.G_b1 = 1e-8; self.G_W2 = 1e-8; self.G_b2 = 1e-8\n",
    "\n",
    "    def update_learning_rate(self):\n",
    "        # Apply learning rate decay before forward and backward pass\n",
    "        if self.decay > 0:\n",
    "            self.lr = self.initial_lr / (1 + self.decay * self.iterations)\n",
    "\n",
    "    def update_weights(self, W1, b1, W2, b2, dW1, db1, dW2, db2):\n",
    "        # Apply momentum\n",
    "        self.v_W1 = self.momentum * self.v_W1 + (1 - self.momentum) * dW1\n",
    "        self.v_b1 = self.momentum * self.v_b1 + (1 - self.momentum) * db1\n",
    "        self.v_W2 = self.momentum * self.v_W2 + (1 - self.momentum) * dW2\n",
    "        self.v_b2 = self.momentum * self.v_b2 + (1 - self.momentum) * db2\n",
    "\n",
    "        if self.use_adagrad:\n",
    "            # Update Adagrad caches and apply adaptive scaling\n",
    "            self.G_W1 += dW1 ** 2; self.G_b1 += db1 ** 2\n",
    "            self.G_W2 += dW2 ** 2; self.G_b2 += db2 ** 2\n",
    "            W1 += self.lr * self.v_W1 / (np.sqrt(self.G_W1) + 1e-8)\n",
    "            b1 += self.lr * self.v_b1 / (np.sqrt(self.G_b1) + 1e-8)\n",
    "            W2 += self.lr * self.v_W2 / (np.sqrt(self.G_W2) + 1e-8)\n",
    "            b2 += self.lr * self.v_b2 / (np.sqrt(self.G_b2) + 1e-8)\n",
    "        else:\n",
    "            # Standard SGD + Momentum update\n",
    "            W1 += self.lr * self.v_W1; b1 += self.lr * self.v_b1\n",
    "            W2 += self.lr * self.v_W2; b2 += self.lr * self.v_b2\n",
    "\n",
    "        self.iterations += 1\n",
    "        return W1, b1, W2, b2\n",
    "\n",
    "\n",
    "# Example training loop\n",
    "optimizer = Optimizer(lr=0.1, decay=0.001, momentum=0.9, use_adagrad=False)\n",
    "\n",
    "for epoch in range(1000):\n",
    "    optimizer.update_learning_rate()  # Decay step\n",
    "\n",
    "    # Forward pass\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    # Backpropagation\n",
    "    error = y - a2\n",
    "    d_output = error * sigmoid_derivative(a2)\n",
    "    d_hidden = d_output.dot(W2.T) * sigmoid_derivative(a1)\n",
    "    \n",
    "    dW2 = a1.T.dot(d_output); db2 = np.sum(d_output, axis=0, keepdims=True)\n",
    "    dW1 = X.T.dot(d_hidden); db1 = np.sum(d_hidden, axis=0, keepdims=True)\n",
    "\n",
    "    # Update weights using optimizer\n",
    "    W1, b1, W2, b2 = optimizer.update_weights(W1, b1, W2, b2, dW1, db1, dW2, db2)\n",
    "\n",
    "    # Display loss every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        loss = np.mean(np.square(error))\n",
    "        print(f\"Epoch {epoch+1}/1000 | LR: {optimizer.lr:.4f} | Loss: {loss:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f574d9",
   "metadata": {},
   "source": [
    "### 3. ) **Display the accuracy once every 100 epochs have elapsed, to see if the accuracy is increasing. Paste a screenshot here of your console that shows the accuracy.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2217bf1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize data and parameters\n",
    "X = np.array([[0,0],\n",
    "              [0,1],\n",
    "              [1,0],\n",
    "              [1,1]])\n",
    "\n",
    "y = np.array([[0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [0]])\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Initialize weights and biases\n",
    "W1 = np.random.randn(2, 3)\n",
    "b1 = np.zeros((1, 3))\n",
    "W2 = np.random.randn(3, 1)\n",
    "b2 = np.zeros((1, 1))\n",
    "\n",
    "# Define activation functions\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return x * (1 - x)\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = Optimizer(lr=0.1, decay=0.001, momentum=0.9, use_adagrad=False)\n",
    "\n",
    "# Training loop with accuracy display\n",
    "for epoch in range(1000):\n",
    "    optimizer.update_learning_rate()\n",
    "\n",
    "    # Forward pass\n",
    "    z1 = np.dot(X, W1) + b1\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = np.dot(a1, W2) + b2\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    # Backpropagation\n",
    "    error = y - a2\n",
    "    d_output = error * sigmoid_derivative(a2)\n",
    "    d_hidden = d_output.dot(W2.T) * sigmoid_derivative(a1)\n",
    "    \n",
    "    dW2 = a1.T.dot(d_output)\n",
    "    db2 = np.sum(d_output, axis=0, keepdims=True)\n",
    "    dW1 = X.T.dot(d_hidden)\n",
    "    db1 = np.sum(d_hidden, axis=0, keepdims=True)\n",
    "\n",
    "    # Update weights\n",
    "    W1, b1, W2, b2 = optimizer.update_weights(W1, b1, W2, b2, dW1, db1, dW2, db2)\n",
    "\n",
    "    # Display loss and accuracy every 100 epochs\n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        loss = np.mean(np.square(error))\n",
    "        predictions = (a2 > 0.5).astype(int)\n",
    "        accuracy = np.mean(predictions == y) * 100\n",
    "        print(f\"Epoch {epoch+1}/1000 | LR: {optimizer.lr:.4f} | Loss: {loss:.6f} | Accuracy: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec73e74b",
   "metadata": {},
   "source": [
    "### 4. ) **Compare the difference of two optimizers you’ve implemented in terms of:**\n",
    "**a) how many epoch did it take to stabilize the loss, and**\n",
    "**b) the accuracy of the model.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10b1af5",
   "metadata": {},
   "source": [
    "When comparing the two optimizers, the **Momentum optimizer** trained the model faster and more smoothly than Adagrad. The loss started to level off around **300 to 400 epochs**, and the model achieved about **98–100% accuracy** by the 500th epoch. Momentum helped the network move past small hurdles by keeping its direction and speed steady during training, which led to more stable updates and quicker learning.\n",
    "\n",
    "In contrast, the **Adagrad optimizer** took longer to settle down, with the loss remaining uneven until around **600 to 700 epochs**. Its accuracy reached about **95–98%**, a little lower than Momentum. Although Adagrad adapted its learning rate well at the beginning, its progress slowed over time because the accumulated adjustments caused the learning rate to decrease, making training slower.\n",
    "\n",
    "Overall, Momentum delivered more steady improvements and achieved higher accuracy in fewer steps, while Adagrad showed good flexibility early on but took longer to train effectively over the long run."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

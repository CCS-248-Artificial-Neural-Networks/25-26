{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bef551f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#We chose to create a Python Class Dense Layer for Number 1, indicating some necessary functions (setup and accepting inputs and weights,\n",
    "#perform the weighted sum + bias, perform selected activation function, and function to calculate loss (we used Mean Squared Error))\n",
    "class Dense_Layer:\n",
    "    def __init__(self, weights, bias, activation=\"relu\"):\n",
    "        self.weights = np.array(weights)\n",
    "        self.bias = np.array(bias)\n",
    "        self.activation_name = activation\n",
    "        self.output = None\n",
    "        self.z = None\n",
    "\n",
    "    # Weighted sum + bias\n",
    "    def forward_pass(self, inputs):\n",
    "        self.inputs = np.array(inputs)\n",
    "        self.z = np.dot(self.inputs, self.weights) + self.bias\n",
    "        return self.z\n",
    "\n",
    "    # Activation functions\n",
    "    def activation(self):\n",
    "        if self.activation_name == \"relu\":\n",
    "            self.output = np.maximum(0, self.z)\n",
    "        elif self.activation_name == \"sigmoid\":\n",
    "            self.output = 1 / (1 + np.exp(-self.z))\n",
    "        elif self.activation_name == \"softmax\":\n",
    "            exp_vals = np.exp(self.z - np.max(self.z))  # stability trick\n",
    "            self.output = exp_vals / np.sum(exp_vals)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function.\")\n",
    "        return self.output\n",
    "\n",
    "    # Loss function (MSE for hidden layers / demonstration)\n",
    "    def calculate_loss(self, target):\n",
    "        target = np.array(target)\n",
    "        self.loss = np.mean((self.output - target) ** 2)\n",
    "        return self.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b78ae566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 output (ReLU): [3.93 0.15 0.85]\n",
      "Layer 2 output (Sigmoid): [0.99378157 0.99187781]\n",
      "Final Output (Softmax probabilities): [0.0265075  0.96865119 0.00484132]\n",
      "Loss (MSE): 0.3511573252826841\n",
      "Predicted Class: Iris-versicolor\n"
     ]
    }
   ],
   "source": [
    "#For this Exercise, I handled Number 2 Letter A. My partner, Kurt Allen R. Alorro, separately handled Letter B.\n",
    "\n",
    "# Input features (Sepal length, Sepal width, Petal length, Petal width)\n",
    "X = [5.1, 3.5, 1.4, 0.2]\n",
    "target_output = [0.7, 0.2, 0.1]   # one-hot-ish vector\n",
    "\n",
    "# First hidden layer\n",
    "W1 = [[0.2, 0.5, -0.3],\n",
    "      [0.1, -0.2, 0.4],\n",
    "      [-0.4, 0.3, 0.2],\n",
    "      [0.6, -0.1, 0.5]]\n",
    "B1 = [3.0, -2.1, 0.6]\n",
    "\n",
    "# Second hidden layer\n",
    "W2 = [[0.3, -0.5],\n",
    "      [0.7, 0.2],\n",
    "      [-0.6, 0.4]]\n",
    "B2 = [4.3, 6.4]\n",
    "\n",
    "# Output layer (3 classes: setosa, versicolor, virginica)\n",
    "W3 = [[0.5, -0.3, 0.8],\n",
    "      [-0.2, 0.6, -0.4]]\n",
    "B3 = [-1.5, 2.1, -3.3]\n",
    "\n",
    "\n",
    "# First Hidden Layer (ReLU)\n",
    "layer1 = Dense_Layer(W1, B1, activation=\"relu\")\n",
    "z1 = layer1.forward_pass(X)\n",
    "a1 = layer1.activation()\n",
    "print(\"Layer 1 output (ReLU):\", a1)\n",
    "\n",
    "# Second Hidden Layer (Sigmoid)\n",
    "layer2 = Dense_Layer(W2, B2, activation=\"sigmoid\")\n",
    "z2 = layer2.forward_pass(a1)\n",
    "a2 = layer2.activation()\n",
    "print(\"Layer 2 output (Sigmoid):\", a2)\n",
    "\n",
    "# Output Layer (Softmax)\n",
    "layer3 = Dense_Layer(W3, B3, activation=\"softmax\")\n",
    "z3 = layer3.forward_pass(a2)\n",
    "output = layer3.activation()\n",
    "print(\"Final Output (Softmax probabilities):\", output)\n",
    "\n",
    "# Loss Calculation (MSE vs target_output)\n",
    "loss = layer3.calculate_loss(target_output)\n",
    "print(\"Loss (MSE):\", loss)\n",
    "\n",
    "# Predicted class\n",
    "classes = [\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"]\n",
    "predicted_class = classes[np.argmax(output)]\n",
    "print(\"Predicted Class:\", predicted_class)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

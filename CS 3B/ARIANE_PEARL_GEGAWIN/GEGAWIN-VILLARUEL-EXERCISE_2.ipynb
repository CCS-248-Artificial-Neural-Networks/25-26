{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ea8a7d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ddd575",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense_Layer:\n",
    "    def __init__(self, weights, biases, activation=\"relu\"):\n",
    "        self.weights = np.array(weights, dtype=float)\n",
    "        self.biases = np.array(biases, dtype=float)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = np.array(inputs, dtype=float)\n",
    "        z = self.weights.dot(x) + self.biases\n",
    "        a = self._activate(z)\n",
    "        return z, a\n",
    "\n",
    "    def _activate(self, z):\n",
    "        if self.activation == \"relu\":\n",
    "            return np.maximum(0, z)\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            return 1.0 / (1.0 + np.exp(-z))\n",
    "        elif self.activation == \"softmax\":\n",
    "            exp_vals = np.exp(z - np.max(z))\n",
    "            return exp_vals / np.sum(exp_vals)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation: \" + str(self.activation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833bed68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def add_layer(self, weights, biases, activation=\"relu\"):\n",
    "        self.layers.append(Dense_Layer(weights, biases, activation))\n",
    "\n",
    "    def forward_pass(self, inputs, verbose=False, round_digits=4):\n",
    "        x = np.array(inputs, dtype=float)\n",
    "        intermediates = []\n",
    "        for i, layer in enumerate(self.layers, start=1):\n",
    "            z, a = layer.forward(x)\n",
    "            intermediates.append({\"z\": z, \"a\": a, \"activation\": layer.activation})\n",
    "            if verbose:\n",
    "                print(f\"  Layer {i} (activation={layer.activation})\")\n",
    "                print(\"    Z =\", np.round(z, round_digits))\n",
    "                print(\"    A =\", np.round(a, round_digits))\n",
    "            x = a\n",
    "        return intermediates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4573f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "def categorical_cross_entropy(y_pred, y_true):\n",
    "    eps = 1e-15\n",
    "    y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "    return -np.sum(np.array(y_true) * np.log(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5aa061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_pred, y_true):\n",
    "    eps = 1e-15\n",
    "    y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "    return - (y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d310b4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # -----------------------\n",
    "    # ARIANE: Problem A (Iris dataset)\n",
    "    # -----------------------\n",
    "    print(\"<Problem A>\")\n",
    "    print(\"-\" * 40)\n",
    "    X = [5.1, 3.5, 1.4, 0.2]\n",
    "    target = [0.7, 0.2, 0.1]\n",
    "\n",
    "    model = NeuralNetwork()\n",
    "    # First hidden layer\n",
    "    model.add_layer(\n",
    "        weights=[[0.2, 0.5, -0.3, 0.1],\n",
    "                 [-0.2, 0.4, -0.4, 0.3],\n",
    "                 [0.2, 0.6, -0.1, 0.5]],\n",
    "        biases=[3.0, -2.1, 0.6],\n",
    "        activation=\"relu\"\n",
    "    )\n",
    "    # Second hidden layer\n",
    "    model.add_layer(\n",
    "        weights=[[0.3, -0.5, 0.7],\n",
    "                 [0.2, -0.6, 0.4]],\n",
    "        biases=[4.3, 6.4],\n",
    "        activation=\"sigmoid\"\n",
    "    )\n",
    "    # Output layer\n",
    "    model.add_layer(\n",
    "        weights=[[0.5, -0.3],\n",
    "                 [0.8, -0.2],\n",
    "                 [0.6, -0.4]],\n",
    "        biases=[-1.5, 2.1, -3.3],\n",
    "        activation=\"softmax\"\n",
    "    )\n",
    "\n",
    "    results = model.forward_pass(X, verbose=True, round_digits=4)\n",
    "    print(\"\\nHidden Layer 2 (Output):\", np.round(results[1][\"a\"], 4))\n",
    "    final_output = results[-1][\"a\"]\n",
    "    print(\"Final Output (Softmax):\", np.round(final_output, 6))\n",
    "\n",
    "    classes = [\"Iris-setosa\", \"Iris-versicolor\", \"Iris-virginica\"]\n",
    "    print(\"Predicted Class:\", classes[int(np.argmax(final_output))])\n",
    "    loss = categorical_cross_entropy(final_output, target)\n",
    "    print(\"Loss:\", round(loss, 6))\n",
    "    print(\"-\" * 40, \"\\n\")\n",
    "\n",
    "    # -----------------------\n",
    "    # LEANN: Problem B (Breast Cancer dataset)\n",
    "    # -----------------------\n",
    "    print(\"<Problem B>\")\n",
    "    print(\"-\" * 40)\n",
    "    Xb = [14.1, 20.3, 0.095]\n",
    "    target_b = 1.0  # malignant\n",
    "\n",
    "    model_b = NeuralNetwork()\n",
    "    # First hidden layer\n",
    "    model_b.add_layer(\n",
    "        weights=[[0.5, -0.3, 0.8],\n",
    "                 [0.2, 0.4, -0.6],\n",
    "                 [-0.7, 0.9, 0.1]],\n",
    "        biases=[0.3, -0.5, 0.6],\n",
    "        activation=\"relu\"\n",
    "    )\n",
    "    # Second hidden layer\n",
    "    model_b.add_layer(\n",
    "        weights=[[0.6, -0.2, 0.4],\n",
    "                 [-0.3, 0.5, 0.7]],\n",
    "        biases=[0.1, -0.8],\n",
    "        activation=\"sigmoid\"\n",
    "    )\n",
    "    # Output layer\n",
    "    model_b.add_layer(\n",
    "        weights=[[0.7, -0.5]],\n",
    "        biases=[0.2],\n",
    "        activation=\"sigmoid\"\n",
    "    )\n",
    "\n",
    "    results_b = model_b.forward_pass(Xb, verbose=True, round_digits=4)\n",
    "    print(\"\\nHidden Layer 2 (Output):\", np.round(results_b[1][\"a\"], 4))\n",
    "    final_output_b = results_b[-1][\"a\"].item()\n",
    "    print(\"Final Output (Sigmoid):\", round(final_output_b, 6))\n",
    "\n",
    "    pred_label = 1 if final_output_b >= 0.5 else 0\n",
    "    print(\"Predicted Class:\", \"Malignant (1)\" if pred_label == 1 else \"Benign (0)\")\n",
    "    loss_b = binary_cross_entropy(final_output_b, target_b)\n",
    "    print(\"Loss:\", round(loss_b, 6))\n",
    "    print(\"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

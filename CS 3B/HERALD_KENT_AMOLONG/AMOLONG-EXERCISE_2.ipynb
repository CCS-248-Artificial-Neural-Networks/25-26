{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9f0dc06",
   "metadata": {},
   "source": [
    "# Unit 2 Exercise\n",
    "## Dense Layer Class with Forward Pass\n",
    "\n",
    "**Author:** Herald Kent Amolong  \n",
    "**Date:** September 11, 2025\n",
    "\n",
    "This notebook implements a `Dense_Layer` class for neural network computation with the Iris dataset. The implementation includes:\n",
    "- Dense layer with configurable activation functions\n",
    "- Forward pass through multiple layers\n",
    "- Loss calculation using cross-entropy\n",
    "- Step-by-step demonstration with Iris dataset sample\n",
    "\n",
    "### Neural Network Architecture\n",
    "- **Input Layer**: 4 features (sepal length, sepal width, petal length, petal width)\n",
    "- **Hidden Layer 1**: ReLU activation\n",
    "- **Hidden Layer 2**: Sigmoid activation  \n",
    "- **Output Layer**: Softmax activation (3 classes: Setosa, Versicolor, Virginica)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d56bf9ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import numpy as np\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "244f222f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense_Layer class defined successfully!\n"
     ]
    }
   ],
   "source": [
    "class Dense_Layer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.inputs = None\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.z = None  # weighted sum (before activation)\n",
    "        self.output = None  # after activation\n",
    "    \n",
    "    def set_inputs_and_weights(self, inputs, weights, bias):\n",
    "        # Store input values, weights, and bias\n",
    "        self.inputs = np.array(inputs)\n",
    "        self.weights = np.array(weights)\n",
    "        self.bias = np.array(bias)\n",
    "        \n",
    "        print(f\"Input shape: {self.inputs.shape}\")\n",
    "        print(f\"Weights shape: {self.weights.shape}\")\n",
    "        print(f\"Bias shape: {self.bias.shape}\")\n",
    "    \n",
    "    def weighted_sum(self):\n",
    "        # Compute z = np.dot(inputs, weights) + bias\n",
    "        if self.inputs is None or self.weights is None or self.bias is None:\n",
    "            raise ValueError(\"Inputs, weights, and bias must be set first!\")\n",
    "        \n",
    "        self.z = np.dot(self.inputs, self.weights) + self.bias\n",
    "        print(f\"Weighted sum (z): {self.z}\")\n",
    "        return self.z\n",
    "    \n",
    "    def activation(self, function=\"relu\"):\n",
    "        # Apply the chosen activation function\n",
    "        if self.z is None:\n",
    "            raise ValueError(\"Must compute weighted sum first!\")\n",
    "        \n",
    "        if function == \"relu\":\n",
    "            self.output = np.maximum(0, self.z)\n",
    "            print(f\"ReLU activation applied: {self.output}\")\n",
    "            \n",
    "        elif function == \"sigmoid\":\n",
    "            self.output = 1 / (1 + np.exp(-np.clip(self.z, -500, 500)))  # clip to prevent overflow\n",
    "            print(f\"Sigmoid activation applied: {self.output}\")\n",
    "            \n",
    "        elif function == \"softmax\":\n",
    "            # Subtract max for numerical stability\n",
    "            z_shifted = self.z - np.max(self.z)\n",
    "            exp_values = np.exp(z_shifted)\n",
    "            self.output = exp_values / np.sum(exp_values)\n",
    "            print(f\"Softmax activation applied: {self.output}\")\n",
    "            print(f\"Sum of probabilities: {np.sum(self.output):.6f}\")\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(\"Supported functions: 'relu', 'sigmoid', 'softmax'\")\n",
    "        \n",
    "        return self.output\n",
    "    \n",
    "    def calculate_loss(self, predicted, target):\n",
    "        # Compute cross-entropy loss\n",
    "        predicted = np.array(predicted)\n",
    "        target = np.array(target)\n",
    "        \n",
    "        # Add small epsilon to prevent log(0)\n",
    "        epsilon = 1e-15\n",
    "        predicted = np.clip(predicted, epsilon, 1 - epsilon)\n",
    "        \n",
    "        # Cross-entropy loss\n",
    "        loss = -np.sum(target * np.log(predicted))\n",
    "        \n",
    "        print(f\"Predicted: {predicted}\")\n",
    "        print(f\"Target: {target}\")\n",
    "        print(f\"Cross-entropy loss: {loss:.6f}\")\n",
    "        \n",
    "        return loss\n",
    "\n",
    "print(\"Dense_Layer class defined successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "25710372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Neural Network Architecture ===\n",
      "Input Layer: 4 neurons\n",
      "Hidden Layer 1: 3 neurons (ReLU activation)\n",
      "Hidden Layer 2: 2 neurons (Sigmoid activation)\n",
      "Output Layer: 3 neurons (Softmax activation)\n",
      "\n",
      "=== Weight and Bias Shapes ===\n",
      "W1 shape: (4, 3), B1 shape: (3,)\n",
      "W2 shape: (3, 2), B2 shape: (2,)\n",
      "W3 shape: (2, 3), B3 shape: (3,)\n",
      "\n",
      "=== Weight and Bias Values===\n",
      "W1 (Input -> Hidden1):\n",
      "[[ 0.2  0.5 -0.3]\n",
      " [ 0.1 -0.2  0.4]\n",
      " [-0.4  0.3  0.2]\n",
      " [ 0.6 -0.1  0.5]]\n",
      "B1: [ 3.  -2.1  0.6]\n",
      "W2 (Hidden1 -> Hidden2):\n",
      "[[ 0.3 -0.5]\n",
      " [ 0.7  0.2]\n",
      " [-0.6  0.4]]\n",
      "B2: [4.3 6.4]\n",
      "W3 (Hidden2 -> Output):\n",
      "[[ 0.5 -0.3  0.8]\n",
      " [-0.2  0.6 -0.4]]\n",
      "B3: [-1.5  2.1 -3.3]\n",
      "\n",
      "Target output: [0.7 0.2 0.1]\n",
      "Sample input: [5.1 3.5 1.4 0.2]\n"
     ]
    }
   ],
   "source": [
    "# Define Neural Network Architecture and Initialize Weights\n",
    "\n",
    "# Network architecture\n",
    "input_size = 4      # 4 features in Iris dataset\n",
    "hidden1_size = 3    # First hidden layer\n",
    "hidden2_size = 2    # Second hidden layer\n",
    "output_size = 3     # 3 classes (setosa, versicolor, virginica)\n",
    "\n",
    "print(\"=== Neural Network Architecture ===\")\n",
    "print(f\"Input Layer: {input_size} neurons\")\n",
    "print(f\"Hidden Layer 1: {hidden1_size} neurons (ReLU activation)\")\n",
    "print(f\"Hidden Layer 2: {hidden2_size} neurons (Sigmoid activation)\")\n",
    "print(f\"Output Layer: {output_size} neurons (Softmax activation)\")\n",
    "\n",
    "# Layer 1: Input -> Hidden1 (4 -> 3)\n",
    "W1 = np.array([\n",
    "    [0.2,  0.5, -0.3],\n",
    "    [0.1, -0.2,  0.4],\n",
    "    [-0.4, 0.3,  0.2],\n",
    "    [0.6, -0.1,  0.5]\n",
    "])\n",
    "\n",
    "B1 = np.array([3.0, -2.1, 0.6])\n",
    "\n",
    "# Layer 2: Hidden1 -> Hidden2 (3 -> 2)  \n",
    "W2 = np.array([\n",
    "    [0.3, -0.5],\n",
    "    [0.7,  0.2],\n",
    "    [-0.6, 0.4]\n",
    "])\n",
    "\n",
    "B2 = np.array([4.3, 6.4])\n",
    "\n",
    "# Layer 3: Hidden2 -> Output (2 -> 3)\n",
    "W3 = np.array([\n",
    "    [0.5, -0.3,  0.8],\n",
    "    [-0.2, 0.6, -0.4]\n",
    "])\n",
    "\n",
    "B3 = np.array([-1.5, 2.1, -3.3])\n",
    "\n",
    "print(\"\\n=== Weight and Bias Shapes ===\")\n",
    "print(f\"W1 shape: {W1.shape}, B1 shape: {B1.shape}\")\n",
    "print(f\"W2 shape: {W2.shape}, B2 shape: {B2.shape}\")\n",
    "print(f\"W3 shape: {W3.shape}, B3 shape: {B3.shape}\")\n",
    "\n",
    "print(\"\\n=== Weight and Bias Values===\")\n",
    "print(f\"W1 (Input -> Hidden1):\\n{W1}\")\n",
    "print(f\"B1: {B1}\")\n",
    "print(f\"W2 (Hidden1 -> Hidden2):\\n{W2}\")\n",
    "print(f\"B2: {B2}\")\n",
    "print(f\"W3 (Hidden2 -> Output):\\n{W3}\")\n",
    "print(f\"B3: {B3}\")\n",
    "\n",
    "target_output = np.array([0.7, 0.2, 0.1])\n",
    "print(f\"\\nTarget output: {target_output}\")\n",
    "\n",
    "sample_input = np.array([5.1, 3.5, 1.4, 0.2])\n",
    "print(f\"Sample input: {sample_input}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "794117a8",
   "metadata": {},
   "source": [
    "## Forward Pass Implementation\n",
    "\n",
    "step-by-step forward pass through a neural network using the Dense_Layer class:\n",
    "\n",
    "### Step-by-Step Process:\n",
    "1. **Input Layer → Hidden Layer 1**: Apply linear transformation + ReLU activation\n",
    "2. **Hidden Layer 1 → Hidden Layer 2**: Apply linear transformation + Sigmoid activation  \n",
    "3. **Hidden Layer 2 → Output Layer**: Apply linear transformation + Softmax activation\n",
    "4. **Loss Calculation**: Compute cross-entropy loss between predicted and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f190be29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "STEP 1: INPUT LAYER → HIDDEN LAYER 1 (ReLU Activation)\n",
      "============================================================\n",
      "Input shape: (4,)\n",
      "Weights shape: (4, 3)\n",
      "Bias shape: (3,)\n",
      "\n",
      "Input values: [5.1 3.5 1.4 0.2]\n",
      "Input shape: (4,)\n",
      "Weighted sum (z): [3.93 0.15 0.85]\n",
      "ReLU activation applied: [3.93 0.15 0.85]\n",
      "\n",
      "=== Hidden Layer 1 Results ===\n",
      "Weighted sum (z1): [3.93 0.15 0.85]\n",
      "After ReLU activation: [3.93 0.15 0.85]\n",
      "Hidden Layer 1 output shape: (3,)\n",
      "Number of activated neurons: 3/3\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Forward Pass - Input Layer to Hidden Layer 1 (ReLU)\n",
    "print(\"=\" * 60)\n",
    "print(\"STEP 1: INPUT LAYER → HIDDEN LAYER 1 (ReLU Activation)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create Dense Layer 1\n",
    "layer1 = Dense_Layer()\n",
    "\n",
    "# Set inputs, weights, and bias for layer 1\n",
    "layer1.set_inputs_and_weights(sample_input, W1, B1)\n",
    "\n",
    "print(f\"\\nInput values: {sample_input}\")\n",
    "print(f\"Input shape: {sample_input.shape}\")\n",
    "\n",
    "# Compute weighted sum\n",
    "z1 = layer1.weighted_sum()\n",
    "\n",
    "# Apply ReLU activation\n",
    "hidden1_output = layer1.activation(\"relu\")\n",
    "\n",
    "print(f\"\\n=== Hidden Layer 1 Results ===\")\n",
    "print(f\"Weighted sum (z1): {z1}\")\n",
    "print(f\"After ReLU activation: {hidden1_output}\")\n",
    "print(f\"Hidden Layer 1 output shape: {hidden1_output.shape}\")\n",
    "\n",
    "# Show which neurons were activated (> 0)\n",
    "activated_neurons = np.sum(hidden1_output > 0)\n",
    "print(f\"Number of activated neurons: {activated_neurons}/{len(hidden1_output)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "ddf5931b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 2: HIDDEN LAYER 1 → HIDDEN LAYER 2 (Sigmoid Activation)\n",
      "============================================================\n",
      "Input shape: (3,)\n",
      "Weights shape: (3, 2)\n",
      "Bias shape: (2,)\n",
      "\n",
      "Input from Hidden Layer 1: [3.93 0.15 0.85]\n",
      "Input shape: (3,)\n",
      "Weighted sum (z): [5.074 4.805]\n",
      "Sigmoid activation applied: [0.99378157 0.99187781]\n",
      "\n",
      "=== Hidden Layer 2 Results ===\n",
      "Weighted sum (z2): [5.074 4.805]\n",
      "After Sigmoid activation: [0.99378157 0.99187781]\n",
      "Hidden Layer 2 output shape: (2,)\n",
      "Sigmoid output range: [0.9919, 0.9938]\n",
      "Mean activation level: 0.9928\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Forward Pass - Hidden Layer 1 to Hidden Layer 2 (Sigmoid)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 2: HIDDEN LAYER 1 → HIDDEN LAYER 2 (Sigmoid Activation)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create Dense Layer 2\n",
    "layer2 = Dense_Layer()\n",
    "\n",
    "# Set inputs (output from layer 1), weights, and bias for layer 2\n",
    "layer2.set_inputs_and_weights(hidden1_output, W2, B2)\n",
    "\n",
    "print(f\"\\nInput from Hidden Layer 1: {hidden1_output}\")\n",
    "print(f\"Input shape: {hidden1_output.shape}\")\n",
    "\n",
    "# Compute weighted sum\n",
    "z2 = layer2.weighted_sum()\n",
    "\n",
    "# Apply Sigmoid activation  \n",
    "hidden2_output = layer2.activation(\"sigmoid\")\n",
    "\n",
    "print(f\"\\n=== Hidden Layer 2 Results ===\")\n",
    "print(f\"Weighted sum (z2): {z2}\")\n",
    "print(f\"After Sigmoid activation: {hidden2_output}\")\n",
    "print(f\"Hidden Layer 2 output shape: {hidden2_output.shape}\")\n",
    "\n",
    "# Show activation range (sigmoid outputs between 0 and 1)\n",
    "print(f\"Sigmoid output range: [{np.min(hidden2_output):.4f}, {np.max(hidden2_output):.4f}]\")\n",
    "print(f\"Mean activation level: {np.mean(hidden2_output):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "be6b8780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 3: HIDDEN LAYER 2 → OUTPUT LAYER (Softmax Activation)\n",
      "============================================================\n",
      "Input shape: (2,)\n",
      "Weights shape: (2, 3)\n",
      "Bias shape: (3,)\n",
      "\n",
      "Input from Hidden Layer 2: [0.99378157 0.99187781]\n",
      "Input shape: (2,)\n",
      "Weighted sum (z): [-1.20148478  2.39699221 -2.90172587]\n",
      "Softmax activation applied: [0.0265075  0.96865119 0.00484132]\n",
      "Sum of probabilities: 1.000000\n",
      "\n",
      "=== Output Layer Results ===\n",
      "Weighted sum (z3): [-1.20148478  2.39699221 -2.90172587]\n",
      "After Softmax activation: [0.0265075  0.96865119 0.00484132]\n",
      "Final output shape: (3,)\n",
      "\n",
      "=== Class Probabilities ===\n",
      "Setosa: 0.0265 (2.65%)\n",
      "Versicolor: 0.9687 (96.87%)\n",
      "Virginica: 0.0048 (0.48%)\n",
      "\n",
      "=== Prediction Summary ===\n",
      "Predicted class: Versicolor\n",
      "Confidence: 0.9687 (96.87%)\n",
      "Actual class: Setosa\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Forward Pass - Hidden Layer 2 to Output Layer (Softmax)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 3: HIDDEN LAYER 2 → OUTPUT LAYER (Softmax Activation)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create Dense Layer 3 (Output Layer)\n",
    "layer3 = Dense_Layer()\n",
    "\n",
    "# Set inputs (output from layer 2), weights, and bias for layer 3\n",
    "layer3.set_inputs_and_weights(hidden2_output, W3, B3)\n",
    "\n",
    "print(f\"\\nInput from Hidden Layer 2: {hidden2_output}\")\n",
    "print(f\"Input shape: {hidden2_output.shape}\")\n",
    "\n",
    "# Compute weighted sum\n",
    "z3 = layer3.weighted_sum()\n",
    "\n",
    "# Apply Softmax activation\n",
    "final_output = layer3.activation(\"softmax\")\n",
    "\n",
    "print(f\"\\n=== Output Layer Results ===\")\n",
    "print(f\"Weighted sum (z3): {z3}\")\n",
    "print(f\"After Softmax activation: {final_output}\")\n",
    "print(f\"Final output shape: {final_output.shape}\")\n",
    "\n",
    "# Interpret the predictions\n",
    "class_names = ['Setosa', 'Versicolor', 'Virginica']\n",
    "print(f\"\\n=== Class Probabilities ===\")\n",
    "for i, (class_name, prob) in enumerate(zip(class_names, final_output)):\n",
    "    print(f\"{class_name}: {prob:.4f} ({prob*100:.2f}%)\")\n",
    "\n",
    "# Find predicted class\n",
    "predicted_class_idx = np.argmax(final_output)\n",
    "predicted_class = class_names[predicted_class_idx]\n",
    "confidence = final_output[predicted_class_idx]\n",
    "\n",
    "print(f\"\\n=== Prediction Summary ===\")\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "print(f\"Confidence: {confidence:.4f} ({confidence*100:.2f}%)\")\n",
    "print(f\"Actual class: Setosa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9db1c0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "STEP 4: LOSS CALCULATION (Cross-Entropy)\n",
      "============================================================\n",
      "Predicted: [0.0265075  0.96865119 0.00484132]\n",
      "Target: [0.7 0.2 0.1]\n",
      "Cross-entropy loss: 3.080656\n",
      "\n",
      "=== Loss Analysis ===\n",
      "Target (one-hot): [0.7 0.2 0.1]\n",
      "Predicted probabilities: [0.0265075  0.96865119 0.00484132]\n",
      "Cross-entropy loss: 3.080656\n",
      "\n",
      "=== Individual Loss Contributions ===\n",
      "Setosa: 2.541229\n",
      "Versicolor: 0.006370\n",
      "Virginica: 0.533057\n",
      "Total loss: 3.080656\n",
      "\n",
      "Loss interpretation: Poor prediction\n",
      "Lower loss values indicate better predictions.\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Loss Calculation - Cross-Entropy Loss\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"STEP 4: LOSS CALCULATION (Cross-Entropy)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate loss using the output layer\n",
    "loss = layer3.calculate_loss(final_output, target_output)\n",
    "\n",
    "print(f\"\\n=== Loss Analysis ===\")\n",
    "print(f\"Target (one-hot): {target_output}\")\n",
    "print(f\"Predicted probabilities: {final_output}\")\n",
    "print(f\"Cross-entropy loss: {loss:.6f}\")\n",
    "\n",
    "# Show individual loss contributions\n",
    "print(f\"\\n=== Individual Loss Contributions ===\")\n",
    "epsilon = 1e-15\n",
    "predicted_clipped = np.clip(final_output, epsilon, 1 - epsilon)\n",
    "target_output_np = np.array(target_output)\n",
    "individual_losses = -target_output_np * np.log(predicted_clipped)\n",
    "for i, (class_name, contrib) in enumerate(zip(class_names, individual_losses)):\n",
    "    print(f\"{class_name}: {contrib:.6f}\")\n",
    "\n",
    "print(f\"Total loss: {np.sum(individual_losses):.6f}\")\n",
    "\n",
    "# Loss interpretation\n",
    "if loss < 0.5:\n",
    "    interpretation = \"Very good prediction\"\n",
    "elif loss < 1.0:\n",
    "    interpretation = \"Good prediction\"\n",
    "elif loss < 2.0:\n",
    "    interpretation = \"Moderate prediction\"\n",
    "else:\n",
    "    interpretation = \"Poor prediction\"\n",
    "\n",
    "print(f\"\\nLoss interpretation: {interpretation}\")\n",
    "print(f\"Lower loss values indicate better predictions.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "09a08033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Hidden Layer 2 (Output) and Loss\n",
      "======================================================================\n",
      "Hidden Layer 2 (Output)\n",
      "Values: [0.99378157 0.99187781]\n",
      "\n",
      "Loss\n",
      "Cross-entropy loss value: 3.080656\n",
      "\n",
      "======================================================================\n",
      "COMPLETE RESULTS SUMMARY\n",
      "======================================================================\n",
      "Input (X): [5.1 3.5 1.4 0.2]\n",
      "Target output: [0.7 0.2 0.1]\n",
      "\n",
      "Weights and Biases:\n",
      "W1:\n",
      "[[ 0.2  0.5 -0.3]\n",
      " [ 0.1 -0.2  0.4]\n",
      " [-0.4  0.3  0.2]\n",
      " [ 0.6 -0.1  0.5]]\n",
      "B1: [ 3.  -2.1  0.6]\n",
      "W2:\n",
      "[[ 0.3 -0.5]\n",
      " [ 0.7  0.2]\n",
      " [-0.6  0.4]]\n",
      "B2: [4.3 6.4]\n",
      "W3:\n",
      "[[ 0.5 -0.3  0.8]\n",
      " [-0.2  0.6 -0.4]]\n",
      "B3: [-1.5  2.1 -3.3]\n",
      "\n",
      "Forward Pass Results:\n",
      "Hidden Layer 1 (after ReLU): [3.93 0.15 0.85]\n",
      "Hidden Layer 2 (after Sigmoid): [0.99378157 0.99187781]\n",
      "Output Layer (after Softmax): [0.0265075  0.96865119 0.00484132]\n",
      "Loss (Cross-entropy): 3.080656\n",
      "\n",
      "Final Prediction:\n",
      "Predicted class: Versicolor\n",
      "Confidence: 0.9687 (96.87%)\n"
     ]
    }
   ],
   "source": [
    "# Hidden Layer 2 (Output) and Loss\n",
    "print(\"=\" * 70)\n",
    "print(\"Hidden Layer 2 (Output) and Loss\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"Hidden Layer 2 (Output)\")\n",
    "print(f\"Values: {hidden2_output}\")\n",
    "\n",
    "print(f\"\\nLoss\")\n",
    "print(f\"Cross-entropy loss value: {loss:.6f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPLETE RESULTS SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"Input (X): {sample_input}\")\n",
    "print(f\"Target output: {target_output}\")\n",
    "\n",
    "print(f\"\\nWeights and Biases:\")\n",
    "print(f\"W1:\\n{W1}\")\n",
    "print(f\"B1: {B1}\")\n",
    "print(f\"W2:\\n{W2}\")\n",
    "print(f\"B2: {B2}\")\n",
    "print(f\"W3:\\n{W3}\")\n",
    "print(f\"B3: {B3}\")\n",
    "\n",
    "print(f\"\\nForward Pass Results:\")\n",
    "print(f\"Hidden Layer 1 (after ReLU): {hidden1_output}\")\n",
    "print(f\"Hidden Layer 2 (after Sigmoid): {hidden2_output}\")\n",
    "print(f\"Output Layer (after Softmax): {final_output}\")\n",
    "print(f\"Loss (Cross-entropy): {loss:.6f}\")\n",
    "\n",
    "print(f\"\\nFinal Prediction:\")\n",
    "print(f\"Predicted class: {predicted_class}\")\n",
    "print(f\"Confidence: {confidence:.4f} ({confidence*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fef0cda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPLETE FORWARD PASS SUMMARY\n",
      "================================================================================\n",
      "Input: [5.1 3.5 1.4 0.2]\n",
      "Target: [0.7 0.2 0.1] (Iris-setosa)\n",
      "\n",
      "Layer           Shape           Activation      Output Values\n",
      "--------------------------------------------------------------------------------\n",
      "Input           (4,)            None            [5.1 3.5 1.4 0.2]\n",
      "Hidden 1        (3,)            ReLU            [3.93 0.15 0.85]\n",
      "Hidden 2        (2,)            Sigmoid         [0.99378157 0.99187781]\n",
      "Output          (3,)            Softmax         [0.0265075  0.96865119 0.00484132]\n",
      "\n",
      "Metrics              Value\n",
      "----------------------------------------\n",
      "Predicted Class      Versicolor\n",
      "Confidence           0.9687\n",
      "Cross-Entropy Loss   3.080656\n",
      "Correct Prediction   False\n",
      "\n",
      "=== Architecture Summary ===\n",
      "Total Parameters:\n",
      "  - W1: 12, B1: 3\n",
      "  - W2: 6, B2: 2\n",
      "  - W3: 6, B3: 3\n",
      "  - Total: 32 parameters\n"
     ]
    }
   ],
   "source": [
    "# Complete Forward Pass Summary\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"COMPLETE FORWARD PASS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"Input: {sample_input}\")\n",
    "print(f\"Target: {target_output} (Iris-setosa)\")\n",
    "\n",
    "print(f\"\\n{'Layer':<15} {'Shape':<15} {'Activation':<15} {'Output Values'}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Input':<15} {str(sample_input.shape):<15} {'None':<15} {sample_input}\")\n",
    "print(f\"{'Hidden 1':<15} {str(hidden1_output.shape):<15} {'ReLU':<15} {hidden1_output}\")\n",
    "print(f\"{'Hidden 2':<15} {str(hidden2_output.shape):<15} {'Sigmoid':<15} {hidden2_output}\")\n",
    "print(f\"{'Output':<15} {str(final_output.shape):<15} {'Softmax':<15} {final_output}\")\n",
    "\n",
    "print(f\"\\n{'Metrics':<20} {'Value'}\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"{'Predicted Class':<20} {predicted_class}\")\n",
    "print(f\"{'Confidence':<20} {confidence:.4f}\")\n",
    "print(f\"{'Cross-Entropy Loss':<20} {loss:.6f}\")\n",
    "print(f\"{'Correct Prediction':<20} {predicted_class == 'Setosa'}\")\n",
    "\n",
    "print(f\"\\n=== Architecture Summary ===\")\n",
    "print(f\"Total Parameters:\")\n",
    "total_params = (W1.size + B1.size + W2.size + B2.size + W3.size + B3.size)\n",
    "print(f\"  - W1: {W1.size}, B1: {B1.size}\")\n",
    "print(f\"  - W2: {W2.size}, B2: {B2.size}\")\n",
    "print(f\"  - W3: {W3.size}, B3: {B3.size}\")\n",
    "print(f\"  - Total: {total_params} parameters\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

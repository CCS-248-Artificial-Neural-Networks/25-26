{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33965005",
   "metadata": {},
   "source": [
    "# Exercise 4 - Neural Network with Different Optimizers\n",
    "Herald Kent Amolong - CS 3B\n",
    "\n",
    "This exercise implements a neural network with backpropagation using 3 different optimizers and compares their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2e3484fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print libraries successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"print libraries successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e49a9fa",
   "metadata": {},
   "source": [
    "## Layer Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "537428e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer_Dense:\n",
    "    # Layer initialization\n",
    "    # randomly initialize weights and set biases to zero\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember the input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate the output values from inputs, weight and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass/Backpropagation\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters:\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cede97",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "560b1877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Functions\n",
    "# Included here are the functions for both the forward and backward pass\n",
    "\n",
    "# Linear\n",
    "class ActivationLinear:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = inputs\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "# Sigmoid\n",
    "class ActivationSigmoid:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * (self.output * (1 - self.output))\n",
    "\n",
    "# TanH\n",
    "class ActivationTanH:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.tanh(inputs)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * (1 - self.output ** 2)\n",
    "\n",
    "# ReLU\n",
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember the input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate the output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Make a copy of the original values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "    \n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "# Softmax\n",
    "class Activation_Softmax:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember the inputs values\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Get the unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # Calculate Jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            # Calculate the sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a756a7",
   "metadata": {},
   "source": [
    "## Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f4128ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "\n",
    "class Loss:\n",
    "    # Calculate the data and regularization losses\n",
    "    # Given the model output and grou truth/target values\n",
    "    def calculate(self, output, y):\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        # Calculate the mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        # Return the mean loss\n",
    "        return data_loss\n",
    "\n",
    "# MSE\n",
    "class Loss_MSE:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Calculate Mean Squared Error\n",
    "        return np.mean((y_true - y_pred) ** 2, axis=-1)\n",
    "\n",
    "    def backward(self, y_pred, y_true):\n",
    "        # Gradient of MSE loss\n",
    "        samples = y_true.shape[0]\n",
    "        outputs = y_true.shape[1]\n",
    "        self.dinputs = -2 * (y_true - y_pred) / outputs\n",
    "        # Normalize gradients over samples\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "# Binary Cross-Entropy\n",
    "class Loss_BinaryCrossEntropy:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Clip predictions\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        # Calculate Binary Cross Entropy\n",
    "        return -(y_true * np.log(y_pred_clipped) + (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "\n",
    "    def backward(self, y_pred, y_true):\n",
    "        # Gradient of BCE loss\n",
    "        samples = y_true.shape[0]\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        self.dinputs = - (y_true / y_pred_clipped - (1 - y_true) / (1 - y_pred_clipped))\n",
    "        # Normalize gradients over samples\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "# Categorical Cross-Entropy\n",
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Number of samples in a batch\n",
    "        samples = y_pred.shape[0]\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values\n",
    "        # Only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    # Backward pass - FIXED VERSION\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of labels in every sample\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # Check if labels are sparse, turn them into one-hot vector values\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # Calculate the gradient - FIXED: Clip dvalues to prevent division by zero\n",
    "        dvalues_clipped = np.clip(dvalues, 1e-7, 1 - 1e-7)\n",
    "        self.dinputs = -y_true / dvalues_clipped\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5281ef5",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "edc8fecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizers created\n"
     ]
    }
   ],
   "source": [
    "# Three different optimizers\n",
    "\n",
    "class Optimizer_SGD_Decay:\n",
    "    def __init__(self, learning_rate=1.0, decay=0.0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "    \n",
    "    def pre_update_params(self):\n",
    "        # Update learning rate before forward pass\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * (1.0 / (1.0 + self.decay * self.iterations))\n",
    "    \n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.current_learning_rate * layer.dweights\n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases\n",
    "    \n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "class Optimizer_SGD_Momentum:\n",
    "    def __init__(self, learning_rate=1.0, momentum=0.9):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.iterations = 0\n",
    "    \n",
    "    def pre_update_params(self):\n",
    "        pass\n",
    "    \n",
    "    def update_params(self, layer):\n",
    "        # Initialize momentum if first time\n",
    "        if not hasattr(layer, 'weight_momentums'):\n",
    "            layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "            layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "        \n",
    "        # Update momentum\n",
    "        layer.weight_momentums = self.momentum * layer.weight_momentums - self.learning_rate * layer.dweights\n",
    "        layer.bias_momentums = self.momentum * layer.bias_momentums - self.learning_rate * layer.dbiases\n",
    "        \n",
    "        # Apply updates\n",
    "        layer.weights += layer.weight_momentums\n",
    "        layer.biases += layer.bias_momentums\n",
    "    \n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "class Optimizer_AdaGrad:\n",
    "    def __init__(self, learning_rate=1.0, epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        self.iterations = 0\n",
    "    \n",
    "    def pre_update_params(self):\n",
    "        pass\n",
    "    \n",
    "    def update_params(self, layer):\n",
    "        # Initialize cache if first time\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "        \n",
    "        # Update cache with squared gradients\n",
    "        layer.weight_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbiases**2\n",
    "        \n",
    "        # Update parameters\n",
    "        layer.weights += -self.learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "    \n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "print(\"Optimizers created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0f5d56",
   "metadata": {},
   "source": [
    "## Create Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "25e582ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: X=(300, 2), y=(300,)\n",
      "Dataset created successfully\n"
     ]
    }
   ],
   "source": [
    "# Create spiral dataset\n",
    "def create_spiral_data(samples, classes):\n",
    "    X = np.zeros((samples*classes, 2))\n",
    "    y = np.zeros(samples*classes, dtype='uint8')\n",
    "    \n",
    "    for class_number in range(classes):\n",
    "        ix = range(samples*class_number, samples*(class_number+1))\n",
    "        r = np.linspace(0.0, 1, samples)\n",
    "        t = np.linspace(class_number*4, (class_number+1)*4, samples) + np.random.randn(samples)*0.2\n",
    "        X[ix] = np.c_[r*np.sin(t*2.5), r*np.cos(t*2.5)]\n",
    "        y[ix] = class_number\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "# Generate data\n",
    "X, y = create_spiral_data(samples=100, classes=3)\n",
    "print(f\"Data shape: X={X.shape}, y={y.shape}\")\n",
    "print(\"Dataset created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2deb4eee",
   "metadata": {},
   "source": [
    "## Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fe576aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(X, y, optimizer, epochs=1000, print_interval=100):\n",
    "    # Create network layers\n",
    "    dense1 = Layer_Dense(2, 64)\n",
    "    activation1 = Activation_ReLU()\n",
    "    dense2 = Layer_Dense(64, 32)\n",
    "    activation2 = Activation_ReLU()\n",
    "    dense3 = Layer_Dense(32, 3)\n",
    "    activation3 = Activation_Softmax()\n",
    "    loss_function = Loss_CategoricalCrossEntropy()\n",
    "    \n",
    "    # Track results\n",
    "    accuracy_log = []\n",
    "    loss_log = []\n",
    "    loss_history = []\n",
    "    epochs_to_stabilize = None\n",
    "    \n",
    "    print(f\"Training with {optimizer.__class__.__name__}\")\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Learning rate decay before forward pass\n",
    "        optimizer.pre_update_params()\n",
    "        \n",
    "        # Forward pass\n",
    "        dense1.forward(X)\n",
    "        activation1.forward(dense1.output)\n",
    "        dense2.forward(activation1.output)\n",
    "        activation2.forward(dense2.output)\n",
    "        dense3.forward(activation2.output)\n",
    "        activation3.forward(dense3.output)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = loss_function.calculate(activation3.output, y)\n",
    "        loss_history.append(loss)\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        predictions = np.argmax(activation3.output, axis=1)\n",
    "        if len(y.shape) == 2:\n",
    "            y_true = np.argmax(y, axis=1)\n",
    "        else:\n",
    "            y_true = y\n",
    "        accuracy = np.mean(predictions == y_true)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss_function.backward(activation3.output, y)\n",
    "        activation3.backward(loss_function.dinputs)\n",
    "        dense3.backward(activation3.dinputs)\n",
    "        activation2.backward(dense3.dinputs)\n",
    "        dense2.backward(activation2.dinputs)\n",
    "        activation1.backward(dense2.dinputs)\n",
    "        dense1.backward(activation1.dinputs)\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.update_params(dense1)\n",
    "        optimizer.update_params(dense2)\n",
    "        optimizer.update_params(dense3)\n",
    "        optimizer.post_update_params()\n",
    "        \n",
    "        # Print progress\n",
    "        if epoch % print_interval == 0:\n",
    "            lr = getattr(optimizer, \"current_learning_rate\", getattr(optimizer, \"learning_rate\", \"N/A\"))\n",
    "            print(f'Epoch: {epoch:4d}, Acc: {accuracy:.4f}, Loss: {loss:.4f}, LR: {lr:.6f}')\n",
    "            accuracy_log.append(accuracy)\n",
    "            loss_log.append(loss)\n",
    "        \n",
    "        # Check if loss stabilized\n",
    "        if epochs_to_stabilize is None and epoch >= 50:\n",
    "            recent_losses = loss_history[-50:]\n",
    "            if max(recent_losses) - min(recent_losses) < 0.01:\n",
    "                epochs_to_stabilize = epoch\n",
    "    \n",
    "    final_accuracy = accuracy\n",
    "    if epochs_to_stabilize is None:\n",
    "        epochs_to_stabilize = epochs\n",
    "    \n",
    "    print(f\"Final Accuracy: {final_accuracy:.4f}\")\n",
    "    print(f\"Stabilized at epoch: {epochs_to_stabilize}\")\n",
    "    \n",
    "    return accuracy_log, loss_log, epochs_to_stabilize, final_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab07747",
   "metadata": {},
   "source": [
    "## Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f1e8a080",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1: Learning Rate Decay\n",
      "Training with Optimizer_SGD_Decay\n",
      "Epoch:    0, Acc: 0.3567, Loss: 1.0986, LR: 1.000000\n",
      "Epoch:  100, Acc: 0.4233, Loss: 1.0986, LR: 0.909091\n",
      "Epoch:  100, Acc: 0.4233, Loss: 1.0986, LR: 0.909091\n",
      "Epoch:  200, Acc: 0.4267, Loss: 1.0986, LR: 0.833333\n",
      "Epoch:  200, Acc: 0.4267, Loss: 1.0986, LR: 0.833333\n",
      "Epoch:  300, Acc: 0.4300, Loss: 1.0986, LR: 0.769231\n",
      "Epoch:  300, Acc: 0.4300, Loss: 1.0986, LR: 0.769231\n",
      "Epoch:  400, Acc: 0.4233, Loss: 1.0986, LR: 0.714286\n",
      "Epoch:  400, Acc: 0.4233, Loss: 1.0986, LR: 0.714286\n",
      "Epoch:  500, Acc: 0.4233, Loss: 1.0985, LR: 0.666667\n",
      "Epoch:  500, Acc: 0.4233, Loss: 1.0985, LR: 0.666667\n",
      "Epoch:  600, Acc: 0.4233, Loss: 1.0982, LR: 0.625000\n",
      "Epoch:  600, Acc: 0.4233, Loss: 1.0982, LR: 0.625000\n",
      "Epoch:  700, Acc: 0.4267, Loss: 1.0964, LR: 0.588235\n",
      "Epoch:  700, Acc: 0.4267, Loss: 1.0964, LR: 0.588235\n",
      "Epoch:  800, Acc: 0.4100, Loss: 1.0828, LR: 0.555556\n",
      "Epoch:  800, Acc: 0.4100, Loss: 1.0828, LR: 0.555556\n",
      "Epoch:  900, Acc: 0.4167, Loss: 1.0765, LR: 0.526316\n",
      "Epoch:  900, Acc: 0.4167, Loss: 1.0765, LR: 0.526316\n",
      "Final Accuracy: 0.4100\n",
      "Stabilized at epoch: 50\n",
      "\n",
      "Final Accuracy: 0.4100\n",
      "Stabilized at epoch: 50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 1: Learning Rate Decay\n",
    "print(\"Test 1: Learning Rate Decay\")\n",
    "optimizer1 = Optimizer_SGD_Decay(learning_rate=1.0, decay=0.001)\n",
    "acc_log1, loss_log1, stabilize1, final_acc1 = train_network(X, y, optimizer1, epochs=1000)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d63f3280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 2: Momentum\n",
      "Training with Optimizer_SGD_Momentum\n",
      "Epoch:    0, Acc: 0.3967, Loss: 1.0986, LR: 0.100000\n",
      "Epoch:  100, Acc: 0.4267, Loss: 1.0986, LR: 0.100000\n",
      "Epoch:  100, Acc: 0.4267, Loss: 1.0986, LR: 0.100000\n",
      "Epoch:  200, Acc: 0.4133, Loss: 1.0986, LR: 0.100000\n",
      "Epoch:  200, Acc: 0.4133, Loss: 1.0986, LR: 0.100000\n",
      "Epoch:  300, Acc: 0.4367, Loss: 1.0985, LR: 0.100000\n",
      "Epoch:  300, Acc: 0.4367, Loss: 1.0985, LR: 0.100000\n",
      "Epoch:  400, Acc: 0.4233, Loss: 1.0983, LR: 0.100000\n",
      "Epoch:  400, Acc: 0.4233, Loss: 1.0983, LR: 0.100000\n",
      "Epoch:  500, Acc: 0.4167, Loss: 1.0954, LR: 0.100000\n",
      "Epoch:  500, Acc: 0.4167, Loss: 1.0954, LR: 0.100000\n",
      "Epoch:  600, Acc: 0.4067, Loss: 1.0765, LR: 0.100000\n",
      "Epoch:  600, Acc: 0.4067, Loss: 1.0765, LR: 0.100000\n",
      "Epoch:  700, Acc: 0.4133, Loss: 1.0759, LR: 0.100000\n",
      "Epoch:  700, Acc: 0.4133, Loss: 1.0759, LR: 0.100000\n",
      "Epoch:  800, Acc: 0.4200, Loss: 1.0750, LR: 0.100000\n",
      "Epoch:  800, Acc: 0.4200, Loss: 1.0750, LR: 0.100000\n",
      "Epoch:  900, Acc: 0.4433, Loss: 1.0697, LR: 0.100000\n",
      "Epoch:  900, Acc: 0.4433, Loss: 1.0697, LR: 0.100000\n",
      "Final Accuracy: 0.4233\n",
      "Stabilized at epoch: 50\n",
      "\n",
      "Final Accuracy: 0.4233\n",
      "Stabilized at epoch: 50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 2: Momentum\n",
    "print(\"Test 2: Momentum\")\n",
    "optimizer2 = Optimizer_SGD_Momentum(learning_rate=0.1, momentum=0.9)\n",
    "acc_log2, loss_log2, stabilize2, final_acc2 = train_network(X, y, optimizer2, epochs=1000)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "21012ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 3: AdaGrad\n",
      "Training with Optimizer_AdaGrad\n",
      "Epoch:    0, Acc: 0.2867, Loss: 1.0986, LR: 1.000000\n",
      "Epoch:  100, Acc: 0.3333, Loss: 10.7454, LR: 1.000000\n",
      "Epoch:  100, Acc: 0.3333, Loss: 10.7454, LR: 1.000000\n",
      "Epoch:  200, Acc: 0.3333, Loss: 10.7454, LR: 1.000000\n",
      "Epoch:  200, Acc: 0.3333, Loss: 10.7454, LR: 1.000000\n",
      "Epoch:  300, Acc: 0.3333, Loss: 10.7454, LR: 1.000000\n",
      "Epoch:  300, Acc: 0.3333, Loss: 10.7454, LR: 1.000000\n",
      "Epoch:  400, Acc: 0.3333, Loss: 10.7454, LR: 1.000000\n",
      "Epoch:  400, Acc: 0.3333, Loss: 10.7454, LR: 1.000000\n",
      "Epoch:  500, Acc: 0.3333, Loss: 10.7454, LR: 1.000000\n",
      "Epoch:  500, Acc: 0.3333, Loss: 10.7454, LR: 1.000000\n",
      "Epoch:  600, Acc: 0.3333, Loss: 10.7454, LR: 1.000000\n",
      "Epoch:  600, Acc: 0.3333, Loss: 10.7454, LR: 1.000000\n",
      "Epoch:  700, Acc: 0.3333, Loss: 10.7454, LR: 1.000000\n",
      "Epoch:  700, Acc: 0.3333, Loss: 10.7454, LR: 1.000000\n",
      "Epoch:  800, Acc: 0.3333, Loss: 10.7454, LR: 1.000000\n",
      "Epoch:  800, Acc: 0.3333, Loss: 10.7454, LR: 1.000000\n",
      "Epoch:  900, Acc: 0.3333, Loss: 10.7454, LR: 1.000000\n",
      "Epoch:  900, Acc: 0.3333, Loss: 10.7454, LR: 1.000000\n",
      "Final Accuracy: 0.3333\n",
      "Stabilized at epoch: 50\n",
      "\n",
      "Final Accuracy: 0.3333\n",
      "Stabilized at epoch: 50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test 3: AdaGrad\n",
    "print(\"Test 3: AdaGrad\")\n",
    "optimizer3 = Optimizer_AdaGrad(learning_rate=1.0)\n",
    "acc_log3, loss_log3, stabilize3, final_acc3 = train_network(X, y, optimizer3, epochs=1000)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d23e6c",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1788b496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy every 100 epochs:\n",
      "Epoch\t| LR Decay\t| Momentum\t| AdaGrad\n",
      "--------------------------------------------------\n",
      "   0\t| 0.3567\t| 0.3967\t| 0.2867\n",
      " 100\t| 0.4233\t| 0.4267\t| 0.3333\n",
      " 200\t| 0.4267\t| 0.4133\t| 0.3333\n",
      " 300\t| 0.4300\t| 0.4367\t| 0.3333\n",
      " 400\t| 0.4233\t| 0.4233\t| 0.3333\n",
      " 500\t| 0.4233\t| 0.4167\t| 0.3333\n",
      " 600\t| 0.4233\t| 0.4067\t| 0.3333\n",
      " 700\t| 0.4267\t| 0.4133\t| 0.3333\n",
      " 800\t| 0.4100\t| 0.4200\t| 0.3333\n",
      " 900\t| 0.4167\t| 0.4433\t| 0.3333\n"
     ]
    }
   ],
   "source": [
    "# Show accuracy progression\n",
    "print(\"Accuracy every 100 epochs:\")\n",
    "print(\"Epoch\\t| LR Decay\\t| Momentum\\t| AdaGrad\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i, epoch in enumerate(range(0, 1000, 100)):\n",
    "    print(f\"{epoch:4d}\\t| {acc_log1[i]:.4f}\\t| {acc_log2[i]:.4f}\\t| {acc_log3[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "f6097846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Results:\n",
      "LR Decay: 50 epochs to stabilize, 0.4100 accuracy\n",
      "Momentum: 50 epochs to stabilize, 0.4233 accuracy\n",
      "AdaGrad: 50 epochs to stabilize, 0.3333 accuracy\n",
      "\n",
      "Fastest convergence: LR Decay (50 epochs)\n",
      "Highest accuracy: Momentum (0.4233)\n"
     ]
    }
   ],
   "source": [
    "# Comparison summary\n",
    "print(\"\\nFinal Results:\")\n",
    "print(f\"LR Decay: {stabilize1} epochs to stabilize, {final_acc1:.4f} accuracy\")\n",
    "print(f\"Momentum: {stabilize2} epochs to stabilize, {final_acc2:.4f} accuracy\") \n",
    "print(f\"AdaGrad: {stabilize3} epochs to stabilize, {final_acc3:.4f} accuracy\")\n",
    "\n",
    "# Find best performers\n",
    "results = [\n",
    "    (\"LR Decay\", stabilize1, final_acc1),\n",
    "    (\"Momentum\", stabilize2, final_acc2),\n",
    "    (\"AdaGrad\", stabilize3, final_acc3)\n",
    "]\n",
    "\n",
    "fastest = min(results, key=lambda x: x[1])\n",
    "most_accurate = max(results, key=lambda x: x[2])\n",
    "\n",
    "print(f\"\\nFastest convergence: {fastest[0]} ({fastest[1]} epochs)\")\n",
    "print(f\"Highest accuracy: {most_accurate[0]} ({most_accurate[2]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888a71ff",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3e1b216d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing Momentum vs LR Decay:\n",
      "Momentum: 50 epochs, 0.4233 accuracy\n",
      "LR Decay: 50 epochs, 0.4100 accuracy\n",
      "\n",
      "Conclusion:\n",
      "For this spiral dataset, Momentum performed better with 0.4233 accuracy.\n",
      "It took 50 epochs to stabilize vs 50 for LR Decay.\n",
      "Momentum is clearly superior for this task.\n",
      "The momentum optimizer generally works well for this type of classification problem.\n"
     ]
    }
   ],
   "source": [
    "# Compare the two best optimizers\n",
    "best_two = sorted(results, key=lambda x: x[2], reverse=True)[:2]\n",
    "opt1_name, opt1_stab, opt1_acc = best_two[0]\n",
    "opt2_name, opt2_stab, opt2_acc = best_two[1]\n",
    "\n",
    "print(f\"Comparing {opt1_name} vs {opt2_name}:\")\n",
    "print(f\"{opt1_name}: {opt1_stab} epochs, {opt1_acc:.4f} accuracy\")\n",
    "print(f\"{opt2_name}: {opt2_stab} epochs, {opt2_acc:.4f} accuracy\")\n",
    "\n",
    "print(f\"\\nConclusion:\")\n",
    "print(f\"For this spiral dataset, {opt1_name} performed better with {opt1_acc:.4f} accuracy.\")\n",
    "print(f\"It took {opt1_stab} epochs to stabilize vs {opt2_stab} for {opt2_name}.\")\n",
    "\n",
    "if opt1_acc - opt2_acc > 0.01:\n",
    "    print(f\"{opt1_name} is clearly superior for this task.\")\n",
    "else:\n",
    "    print(f\"Both optimizers performed similarly, but {opt1_name} has a slight edge.\")\n",
    "    \n",
    "print(\"The momentum optimizer generally works well for this type of classification problem.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

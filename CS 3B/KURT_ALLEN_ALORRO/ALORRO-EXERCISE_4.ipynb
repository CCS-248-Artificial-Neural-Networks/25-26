{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7973251",
   "metadata": {},
   "source": [
    "INSTALL THE FOLLOWING PYTHON PACKAGES FIRST BEFORE RUNNING THE PROGRAM\n",
    "\n",
    "1) Numpy\n",
    "2) NNFS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7a7dd421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f740e2b",
   "metadata": {},
   "source": [
    "Create classes for modularity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc714746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden Layers\n",
    "# Dense\n",
    "class Layer_Dense:\n",
    "    # Layer initialization\n",
    "    # randomly initialize weights and set biases to zero\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember the input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate the output values from inputs, weight and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass/Backpropagation\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters:\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "945fa51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation Functions\n",
    "# Included here are the functions for both the forward and backward pass\n",
    "\n",
    "# Linear\n",
    "class ActivationLinear:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = inputs\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "# Sigmoid\n",
    "class ActivationSigmoid:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * (self.output * (1 - self.output))\n",
    "\n",
    "# TanH\n",
    "class ActivationTanH:\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.tanh(inputs)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        self.dinputs = dvalues * (1 - self.output ** 2)\n",
    "\n",
    "# ReLU\n",
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember the input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate the output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Make a copy of the original values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "    \n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "# Softmax\n",
    "class Activation_Softmax:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember the inputs values\n",
    "        self.inputs = inputs\n",
    "\n",
    "        # Get the unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "\n",
    "        self.output = probabilities\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "\n",
    "        # Enumerate outputs and gradients\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # Calculate Jacobian matrix of the output\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            # Calculate the sample-wise gradient\n",
    "            # and add it to the array of sample gradients\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ed37d0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "\n",
    "class Loss:\n",
    "    # Calculate the data and regularization losses\n",
    "    # Given the model output and grou truth/target values\n",
    "    def calculate(self, output, y):\n",
    "        # Calculate sample losses\n",
    "        sample_losses = self.forward(output, y)\n",
    "        # Calculate the mean loss\n",
    "        data_loss = np.mean(sample_losses)\n",
    "        # Return the mean loss\n",
    "        return data_loss\n",
    "\n",
    "# MSE\n",
    "class Loss_MSE:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Calculate Mean Squared Error\n",
    "        return np.mean((y_true - y_pred) ** 2, axis=-1)\n",
    "\n",
    "    def backward(self, y_pred, y_true):\n",
    "        # Gradient of MSE loss\n",
    "        samples = y_true.shape[0]\n",
    "        outputs = y_true.shape[1]\n",
    "        self.dinputs = -2 * (y_true - y_pred) / outputs\n",
    "        # Normalize gradients over samples\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "# Binary Cross-Entropy\n",
    "class Loss_BinaryCrossEntropy:\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Clip predictions\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        # Calculate Binary Cross Entropy\n",
    "        return -(y_true * np.log(y_pred_clipped) + (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "\n",
    "    def backward(self, y_pred, y_true):\n",
    "        # Gradient of BCE loss\n",
    "        samples = y_true.shape[0]\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        self.dinputs = - (y_true / y_pred_clipped - (1 - y_true) / (1 - y_pred_clipped))\n",
    "        # Normalize gradients over samples\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "# Categorical Cross-Entropy\n",
    "class Loss_CategoricalCrossEntropy(Loss):\n",
    "    # Forward pass\n",
    "    def forward(self, y_pred, y_true):\n",
    "        # Number of samples in a batch\n",
    "        samples = y_pred.shape[0]\n",
    "\n",
    "        # Clip data to prevent division by 0\n",
    "        # Clip both sides to not drag mean towards any value\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        # Probabilities for target values\n",
    "        # Only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "\n",
    "        # Losses\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        # Number of labels in every sample\n",
    "        # Use the first sample to count them\n",
    "        labels = len(dvalues[0])\n",
    "\n",
    "        # Check if labels are sparse, turn them into one-hot vector values\n",
    "        # the eye function creates a 2D array with ones on the diagonal and zeros elsewhere\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "\n",
    "        # Calculate the gradient\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8c33b2",
   "metadata": {},
   "source": [
    "<!-- Star -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "054d8c85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass Optimizer_SGD:\\n    # Initialie the optimizer - default learning rate to 1\\n    def __init__(self, learning_rate=1.0):\\n        self.learning_rate = learning_rate\\n        self.iterations = 0\\n\\n    # Update the parameters\\n    def update_params(self, layer):\\n        layer.weights += -self.learning_rate * layer.dweights\\n        layer.biases += -self.learning_rate * layer.dbiases\\n    \\n    def post_update_params(self):\\n        self.iterations += 1\\n\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start of Optimizers (commented, just in case)\n",
    "\"\"\"\n",
    "class Optimizer_SGD:\n",
    "    # Initialie the optimizer - default learning rate to 1\n",
    "    def __init__(self, learning_rate=1.0):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = 0\n",
    "\n",
    "    # Update the parameters\n",
    "    def update_params(self, layer):\n",
    "        layer.weights += -self.learning_rate * layer.dweights\n",
    "        layer.biases += -self.learning_rate * layer.dbiases\n",
    "    \n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1055f052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No.2: Start of Optimizers (UPDATED VERSION that handles 3 optimizers discussed: \n",
    "# Learning Rate Decay, Momentum, and Adaptive Gradient)\n",
    "\n",
    "class Optimizer_SGD:\n",
    "    # Initialize the optimizer - default learning rate to 1\n",
    "    # decay rate is set to 0.01, momentum to 0.0, and adaptive off because it hasn't been specified\n",
    "    # As a result, training becomes smoother and more stable, improving the loss and accuracy trends\n",
    "\n",
    "    # === Momentum (uses momentum=0.9 to accelerate learning and reduce oscillations) ===\n",
    "    #def __init__(self, learning_rate=1.0, decay=0.01, momentum=0.9, adaptive=False, epsilon=1e-7):\n",
    "\n",
    "    # === Adagrad (adaptive=True enables adaptive learning rate adjustment per parameter) ===\n",
    "    #def __init__(self, learning_rate=1.0, decay=0.01, momentum=0.0, adaptive=True, epsilon=1e-7):\n",
    "\n",
    "    # === Vanilla SGD (default) ===\n",
    "    def __init__(self, learning_rate=1.0, decay=0.01, momentum=0.0, adaptive=False, epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.momentum = momentum\n",
    "        self.adaptive = adaptive\n",
    "        self.epsilon = epsilon\n",
    "        self.iterations = 0\n",
    "\n",
    "        # Initialize dictionaries for momentum and adaptive gradient\n",
    "        self.weight_momentums = {}\n",
    "        self.bias_momentums = {}\n",
    "        self.weight_cache = {}\n",
    "        self.bias_cache = {}\n",
    "\n",
    "    # <----- LEARNING RATE DECAY ----->\n",
    "    # Update learning rate using decay before running both FP and BP\n",
    "    # Formula used: lr = lr / (1 + decay * iteration)\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate / (1 + self.decay * self.iterations)\n",
    "        else:\n",
    "            self.current_learning_rate = self.learning_rate\n",
    "\n",
    "\n",
    "    # Update the parameters\n",
    "    def update_params(self, layer):\n",
    "       \n",
    "        # <----- MOMENTUM IMPLEMENTATION ----->\n",
    "        if self.momentum:\n",
    "            # Initialize momentums if not present\n",
    "            if layer not in self.weight_momentums:\n",
    "                self.weight_momentums[layer] = 0\n",
    "                self.bias_momentums[layer] = 0\n",
    "\n",
    "            # Apply momentum formula:\n",
    "            # v = (momentum * v) - (current_lr * gradient)\n",
    "            weight_updates = (self.momentum * self.weight_momentums[layer]) - (\n",
    "                self.current_learning_rate * layer.dweights\n",
    "            )\n",
    "            bias_updates = (self.momentum * self.bias_momentums[layer]) - (\n",
    "                self.current_learning_rate * layer.dbiases\n",
    "            )\n",
    "\n",
    "            # Store momentum updates\n",
    "            self.weight_momentums[layer] = weight_updates\n",
    "            self.bias_momentums[layer] = bias_updates\n",
    "\n",
    "        else:\n",
    "            # Vanilla SGD\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "        \n",
    "        # <----- ADAPTIVE GRADIENT IMPLEMENTATION ----->       \n",
    "        if self.adaptive:\n",
    "            # Initialize cache if not present\n",
    "            if layer not in self.weight_cache:\n",
    "                self.weight_cache[layer] = 0\n",
    "                self.bias_cache[layer] = 0\n",
    "\n",
    "            # Accumulate squared gradients\n",
    "            # cache = cache + (gradient ** 2)\n",
    "            self.weight_cache[layer] += layer.dweights ** 2\n",
    "            self.bias_cache[layer] += layer.dbiases ** 2\n",
    "\n",
    "            # Update parameters using Adagrad formula:\n",
    "            # w = w - (lr / sqrt(cache + epsilon)) * gradient\n",
    "            layer.weights += -self.current_learning_rate * layer.dweights / (\n",
    "                (self.weight_cache[layer] ** 0.5) + self.epsilon\n",
    "            )\n",
    "            layer.biases += -self.current_learning_rate * layer.dbiases / (\n",
    "                (self.bias_cache[layer] ** 0.5) + self.epsilon\n",
    "            )\n",
    "        else:\n",
    "            # Apply either Vanilla SGD or Momentum update\n",
    "            layer.weights += weight_updates\n",
    "            layer.biases += bias_updates\n",
    "\n",
    "    # Increment iteration count after parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f819460f",
   "metadata": {},
   "source": [
    "Use most of the classes to create a functioning neural network, capable of performing a forward and backward pass\n",
    "\n",
    "We can use a sample dataset from the Spiral module\n",
    "We can also use the IRIS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "325fd84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for the package\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "# Neural Network initialization\n",
    "# Create the dataset\n",
    "X, y = spiral_data(samples = 100, classes = 3)\n",
    "\n",
    "# Create a Dense Layer with 2 input features and 3 output values\n",
    "dense1 = Layer_Dense(2, 3)\n",
    "\n",
    "# Create a ReLU activation for the first Dense layer\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Create a 2nd dense layer with 3 input and 3 output values\n",
    "dense2 = Layer_Dense(3, 3)\n",
    "\n",
    "# Create a Softmax activation for the 2nd Dense layer\n",
    "activation2 = Activation_Softmax()\n",
    "\n",
    "# Create a loss function\n",
    "loss_function = Loss_CategoricalCrossEntropy()\n",
    "\n",
    "# Create the optimizer\n",
    "optimizer = Optimizer_SGD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a737644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc6742a",
   "metadata": {},
   "source": [
    "PERFORM ONLY 1 PASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "497de66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.33666666666666667\n"
     ]
    }
   ],
   "source": [
    "# Perform a forward pass of our training data\n",
    "# give the input from the dataset to the first layer\n",
    "dense1.forward(X)\n",
    "\n",
    "# Activation function\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "# Pass on the 2nd layer\n",
    "dense2.forward(activation1.output)\n",
    "\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "# Calculate the loss\n",
    "loss_function.forward(activation2.output, y)\n",
    "\n",
    "# Check the model's performance\n",
    "predictions = np.argmax(activation2.output, axis=1)\n",
    "if len(y.shape) == 2:\n",
    "    y = np.argmax(y, axis=1)\n",
    "accuracy = np.mean(predictions == y)\n",
    "\n",
    "# Print the accuracy\n",
    "print('acc:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "183b77a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(300, 3)\n"
     ]
    }
   ],
   "source": [
    "# Perform a backward pass of our training data\n",
    "# From loss to 2nd softmax activation\n",
    "loss_function.backward(activation2.output, y)\n",
    "dvalues = loss_function.dinputs # Gradient of the loss w.r.t softmax output\n",
    "\n",
    "print(dvalues.shape)\n",
    "# print(dvalues)\n",
    "\n",
    "# From 2nd softmax to 2nd dense layer\n",
    "activation2.backward(dvalues)\n",
    "# From 2nd dense layer to 1st ReLU activation\n",
    "dense2.backward(activation2.dinputs)\n",
    "\n",
    "# From 1st ReLU activation to 1st dense layer\n",
    "activation1.backward(dense2.dinputs)\n",
    "dense1.backward(activation1.dinputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e5cabc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.3042468e-06 -3.9488236e-06 -9.9410361e-05]\n",
      " [-2.2006872e-05  3.0671345e-04  1.6974623e-04]]\n",
      "[[-1.8163289e-05 -5.1999168e-04  1.4667865e-05]]\n",
      "[[ 9.1446236e-05 -2.5220116e-04  1.6075492e-04]\n",
      " [-1.7278346e-04  3.9700870e-04 -2.2422522e-04]\n",
      " [ 4.4883702e-05 -1.2783038e-04  8.2946674e-05]]\n",
      "[[ 4.6686037e-06 -8.4029743e-06  3.6098063e-06]]\n"
     ]
    }
   ],
   "source": [
    "# Check the gradient values of the weights and biases of the established layers\n",
    "print(dense1.dweights)\n",
    "print(dense1.dbiases)\n",
    "print(dense2.dweights)\n",
    "print(dense2.dbiases)\n",
    "\n",
    "\n",
    "# Update the weights and biases\n",
    "optimizer.update_params(dense1)\n",
    "optimizer.update_params(dense2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f222996",
   "metadata": {},
   "source": [
    "EXERCISE FOUR:\n",
    "1) Set up the code so that it will perform the Forward Pass (FP), Backpropagation (BP) and weight update in 1000 epochs\n",
    "\n",
    "2) Modify the Optimizer class so that it will accept 3 optimizers we've discussed\n",
    "    \n",
    "    a) Learning rate decay\n",
    "\n",
    "    b) Momentum\n",
    "\n",
    "    c) Adaptive Gradient\n",
    "\n",
    "    Hint: Updating the learning decay rate happens before running both FP and BP, implementing momentum, and vanilla SGD happens after the learning rate decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93a6f15a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No.1A: TRAINING SETUP AND EXECUTION\n",
    "\n",
    "\n",
    "# reimporting packages for clarity\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "# Initialize nnfs settings\n",
    "nnfs.init()\n",
    "\n",
    "# Create the dataset (100 samples and 3 classes)\n",
    "X, y = spiral_data(samples = 100, classes = 3)\n",
    "\n",
    "# Create the model layers\n",
    "layer1 = Layer_Dense(2, 3)        # Input layer: 2 inputs going to 3 neurons\n",
    "activation1 = Activation_ReLU()   # ReLU activation\n",
    "layer2 = Layer_Dense(3, 3)        # Hidden layer: 3 neurons going to 3 outputs\n",
    "activation2 = Activation_Softmax()# used Softmax for multi-class output\n",
    "\n",
    "# Setting hyperparameters\n",
    "learning_rate = 0.01\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33533dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results displaying every 100-epoch interval:\n",
      "\n",
      "Epoch    0 | Loss: 1.0986 | Accuracy: 0.340\n",
      "Epoch  100 | Loss: 1.0368 | Accuracy: 0.427\n",
      "Epoch  200 | Loss: 1.0297 | Accuracy: 0.437\n",
      "Epoch  300 | Loss: 1.0259 | Accuracy: 0.430\n",
      "Epoch  400 | Loss: 1.0246 | Accuracy: 0.417\n",
      "Epoch  500 | Loss: 1.0237 | Accuracy: 0.417\n",
      "Epoch  600 | Loss: 1.0233 | Accuracy: 0.417\n",
      "Epoch  700 | Loss: 1.0229 | Accuracy: 0.417\n",
      "Epoch  800 | Loss: 1.0226 | Accuracy: 0.417\n",
      "Epoch  900 | Loss: 1.0222 | Accuracy: 0.417\n",
      "Epoch  999 | Loss: 1.0220 | Accuracy: 0.417\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# No.1B: TRAINING LOOP (for 1000 Epochs)\n",
    "# ============================\n",
    "\n",
    "# Result title\n",
    "print(\"Results displaying every 100-epoch interval:\\n\")\n",
    "\n",
    "# Initialize the optimizer (using only learning rate)\n",
    "optimizer = Optimizer_SGD(learning_rate=1.0)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # <---- Forward Pass ---- >\n",
    "    layer1.forward(X)\n",
    "    activation1.forward(layer1.output)\n",
    "    layer2.forward(activation1.output)\n",
    "    activation2.forward(layer2.output)\n",
    "\n",
    "    # Initialize the loss object (only once outside loop would also be fine)\n",
    "    loss_function = Loss_CategoricalCrossEntropy()\n",
    "\n",
    "    # Calculate the loss using the class method\n",
    "    loss = loss_function.calculate(activation2.output, y)\n",
    "\n",
    "    # Accuracy: how many predictions match labels\n",
    "    predictions = np.argmax(activation2.output, axis=1)\n",
    "    accuracy = np.mean(predictions == y)\n",
    "\n",
    "    # prints only 100 epochs per interval\n",
    "    if epoch % 100 == 0 or epoch == epochs - 1:\n",
    "        print(f'Epoch {epoch:4d} | Loss: {loss:.4f} | Accuracy: {accuracy:.3f}')\n",
    "\n",
    "    # < ---- Backward Pass ---- >\n",
    "    dvalues = activation2.output.copy()\n",
    "    dvalues[range(len(X)), y] -= 1\n",
    "    dvalues = dvalues / len(X)\n",
    "\n",
    "    layer2.backward(dvalues)\n",
    "    activation1.backward(layer2.dinputs)\n",
    "    layer1.backward(activation1.dinputs)\n",
    "\n",
    "    # < ---- Weight and Bias Updates ---- >\n",
    "    # In this part, we use the Optimizer class instead of manually updating the weights and biases\n",
    "    # The optimizer handles the parameter updates and includes learning rate decay adjustment\n",
    "    # This means that in the pre-update, the learning rate is recalculated if decay is enabled\n",
    "    # Pre-update: apply learning rate decay (currently set to 0.01)\n",
    "    optimizer.pre_update_params()\n",
    "\n",
    "    # Update parameters using the optimizer\n",
    "    optimizer.update_params(layer1)\n",
    "    optimizer.update_params(layer2)\n",
    "\n",
    "    # Post-update: increment iteration counter\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f86cb1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

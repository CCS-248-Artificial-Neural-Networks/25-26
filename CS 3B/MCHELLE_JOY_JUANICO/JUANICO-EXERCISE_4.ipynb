{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "732f804b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "NEURAL NETWORK TRAINING: IRIS DATASET (3-CLASS)\n",
      "============================================================\n",
      "\n",
      "[INFO] Dataset: Iris | X.shape=(150, 4), y.shape=(150,)\n",
      "       >> Train: X_train=(120, 4), y_train=(120,)\n",
      "       >> Test : X_test=(30, 4), y_test=(30,)\n",
      "\n",
      "[INFO] Building Neural Network Architecture...\n",
      "[INFO] Initialized Hidden_Layer_1\n",
      "       >> weights.shape = (4, 8), biases.shape = (1, 8)\n",
      "\n",
      "[INFO] Initialized Output_Layer\n",
      "       >> weights.shape = (8, 3), biases.shape = (1, 3)\n",
      "\n",
      "[INFO] Loss Function: CategoricalCrossentropy\n",
      "[INFO] Optimizer: SGD (lr=0.1, decay=0.001, momentum=0.9)\n",
      "[INFO] Training for 1000 epochs...\n",
      "\n",
      "============================================================\n",
      "Epoch    0 | Loss: 1.098388 | Train Accuracy: 69.17% | LR: 0.10000\n",
      "Epoch  100 | Loss: 0.059261 | Train Accuracy: 97.50% | LR: 0.09091\n",
      "Epoch  200 | Loss: 0.045921 | Train Accuracy: 98.33% | LR: 0.08333\n",
      "Epoch  300 | Loss: 0.042862 | Train Accuracy: 98.33% | LR: 0.07692\n",
      "Epoch  400 | Loss: 0.041618 | Train Accuracy: 98.33% | LR: 0.07143\n",
      "Epoch  500 | Loss: 0.041017 | Train Accuracy: 98.33% | LR: 0.06667\n",
      "Epoch  600 | Loss: 0.040698 | Train Accuracy: 98.33% | LR: 0.06250\n",
      "Epoch  700 | Loss: 0.040516 | Train Accuracy: 98.33% | LR: 0.05882\n",
      "Epoch  800 | Loss: 0.040405 | Train Accuracy: 98.33% | LR: 0.05556\n",
      "Epoch  900 | Loss: 0.040334 | Train Accuracy: 98.33% | LR: 0.05263\n",
      "Epoch  999 | Loss: 0.040287 | Train Accuracy: 98.33% | LR: 0.05003\n",
      "============================================================\n",
      "\n",
      "[INFO] Training Complete!\n",
      "\n",
      "============================================================\n",
      "TEST RESULTS\n",
      "============================================================\n",
      "Test Accuracy: 100.00%\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "OPTIMIZER COMPARISON: Momentum SGD vs Adagrad\n",
      "============================================================\n",
      "[INFO] Initialized Hidden_Layer_1\n",
      "       >> weights.shape = (4, 8), biases.shape = (1, 8)\n",
      "\n",
      "[INFO] Initialized Output_Layer\n",
      "       >> weights.shape = (8, 3), biases.shape = (1, 3)\n",
      "\n",
      "[INFO] Initialized Hidden_Layer_1\n",
      "       >> weights.shape = (4, 8), biases.shape = (1, 8)\n",
      "\n",
      "[INFO] Initialized Output_Layer\n",
      "       >> weights.shape = (8, 3), biases.shape = (1, 3)\n",
      "\n",
      "============================================================ \n",
      "\n",
      "Optimizer      | Stabilize Epoch | Final Loss  | Final Acc (%)\n",
      "-----------------------------------------------------------\n",
      "Momentum SGD   |            158 | 0.040399 |        98.33\n",
      "Adagrad        |            192 | 0.041504 |        98.33\n",
      "\n",
      "============================================================\n",
      "\n",
      "[INFO] Comparison complete.\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ============================================================\n",
    "# LAYER CLASS\n",
    "# ============================================================\n",
    "\n",
    "class Layer_Dense:\n",
    "    \"\"\"\n",
    "    Dense (fully connected) layer with random weight initialization.\n",
    "    \n",
    "    Attributes:\n",
    "        weights: weight matrix of shape (n_inputs, n_neurons)\n",
    "        biases: bias vector of shape (1, n_neurons)\n",
    "        inputs: stored inputs for backward pass\n",
    "        output: stored output for backward pass\n",
    "        dweights: gradient of loss w.r.t. weights\n",
    "        dbiases: gradient of loss w.r.t. biases\n",
    "        dinputs: gradient of loss w.r.t. inputs\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_inputs, n_neurons, name=None):\n",
    "        \"\"\"\n",
    "        Initialize layer with random weights and zero biases.\n",
    "        \n",
    "        Parameters:\n",
    "            n_inputs: number of input features\n",
    "            n_neurons: number of neurons in the layer\n",
    "            name: optional name for the layer\n",
    "        \"\"\"\n",
    "        # Initialize weights with small random values (He initialization scaled down)\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        # Initialize biases to zero\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        self.name = name or f\"Dense_Layer({n_inputs}→{n_neurons})\"\n",
    "        \n",
    "        print(f\"[INFO] Initialized {self.name}\")\n",
    "        print(f\"       >> weights.shape = {self.weights.shape}, biases.shape = {self.biases.shape}\\n\")\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Forward pass: compute output = inputs · weights + biases\n",
    "        \n",
    "        Parameters:\n",
    "            inputs: input data of shape (batch_size, n_inputs) or (n_inputs,)\n",
    "        \n",
    "        Returns:\n",
    "            output: layer output of shape (batch_size, n_neurons)\n",
    "        \"\"\"\n",
    "        # Store inputs for backward pass\n",
    "        self.inputs = inputs\n",
    "        # Compute output\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients w.r.t. weights, biases, and inputs.\n",
    "        \n",
    "        Parameters:\n",
    "            dvalues: gradient of loss w.r.t. layer output (batch_size, n_neurons)\n",
    "        \"\"\"\n",
    "        # Gradients on parameters\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on inputs\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# ACTIVATION FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "class ActivationLinear:\n",
    "    \"\"\"\n",
    "    Linear (identity) activation function.\n",
    "    f(x) = x\n",
    "    f'(x) = 1\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name=\"Linear\"):\n",
    "        self.name = name\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward pass: output = input\"\"\"\n",
    "        self.inputs = inputs\n",
    "        self.output = inputs\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        \"\"\"Backward pass: gradient passes through unchanged\"\"\"\n",
    "        self.dinputs = dvalues.copy()\n",
    "\n",
    "\n",
    "class ActivationSigmoid:\n",
    "    \"\"\"\n",
    "    Sigmoid activation function.\n",
    "    f(x) = 1 / (1 + e^(-x))\n",
    "    f'(x) = f(x) * (1 - f(x))\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name=\"Sigmoid\"):\n",
    "        self.name = name\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward pass: apply sigmoid function\"\"\"\n",
    "        self.inputs = inputs\n",
    "        self.output = 1 / (1 + np.exp(-inputs))\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        \"\"\"Backward pass: compute gradient using sigmoid derivative\"\"\"\n",
    "        # Derivative: sigmoid * (1 - sigmoid)\n",
    "        self.dinputs = dvalues * self.output * (1 - self.output)\n",
    "\n",
    "\n",
    "class ActivationTanh:\n",
    "    \"\"\"\n",
    "    Hyperbolic tangent activation function.\n",
    "    f(x) = tanh(x) = (e^x - e^(-x)) / (e^x + e^(-x))\n",
    "    f'(x) = 1 - tanh²(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name=\"Tanh\"):\n",
    "        self.name = name\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward pass: apply tanh function\"\"\"\n",
    "        self.inputs = inputs\n",
    "        self.output = np.tanh(inputs)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        \"\"\"Backward pass: compute gradient using tanh derivative\"\"\"\n",
    "        # Derivative: 1 - tanh²(x)\n",
    "        self.dinputs = dvalues * (1 - self.output ** 2)\n",
    "\n",
    "\n",
    "class ActivationReLU:\n",
    "    \"\"\"\n",
    "    Rectified Linear Unit activation function.\n",
    "    f(x) = max(0, x)\n",
    "    f'(x) = 1 if x > 0, else 0\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name=\"ReLU\"):\n",
    "        self.name = name\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward pass: apply ReLU function\"\"\"\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        \"\"\"Backward pass: compute gradient using ReLU derivative\"\"\"\n",
    "        # Derivative: 1 where input > 0, else 0\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "\n",
    "class ActivationSoftmax:\n",
    "    \"\"\"\n",
    "    Softmax activation function for multi-class classification.\n",
    "    f(x_i) = e^(x_i) / Σ(e^(x_j))\n",
    "    \n",
    "    Converts logits to probability distribution.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name=\"Softmax\"):\n",
    "        self.name = name\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Forward pass: apply softmax function\"\"\"\n",
    "        self.inputs = inputs\n",
    "        # Subtract max for numerical stability\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        # Normalize to get probabilities\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "        return self.output\n",
    "    \n",
    "    def backward(self, dvalues):\n",
    "        \"\"\"Backward pass: compute gradient using softmax derivative\"\"\"\n",
    "        # Create uninitialized array\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        \n",
    "        # Calculate gradient for each sample\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            # Flatten output array\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # Calculate Jacobian matrix\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            # Calculate sample-wise gradient\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# LOSS FUNCTIONS\n",
    "# ============================================================\n",
    "\n",
    "class LossMSE:\n",
    "    \"\"\"\n",
    "    Mean Squared Error loss function.\n",
    "    L = (1/n) * Σ(y_true - y_pred)²\n",
    "    \n",
    "    Commonly used for regression problems.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name=\"MSE\"):\n",
    "        self.name = name\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Forward pass: calculate MSE loss.\n",
    "        \n",
    "        Parameters:\n",
    "            y_pred: predicted values\n",
    "            y_true: true values\n",
    "        \n",
    "        Returns:\n",
    "            loss: mean squared error\n",
    "        \"\"\"\n",
    "        # Calculate loss\n",
    "        sample_losses = np.mean((y_true - y_pred) ** 2, axis=-1)\n",
    "        return np.mean(sample_losses)\n",
    "    \n",
    "    def backward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradient of MSE loss.\n",
    "        \n",
    "        Parameters:\n",
    "            y_pred: predicted values\n",
    "            y_true: true values\n",
    "        \"\"\"\n",
    "        # Number of samples\n",
    "        samples = len(y_pred)\n",
    "        # Number of outputs in every sample\n",
    "        outputs = len(y_pred[0])\n",
    "        \n",
    "        # Gradient: -2/n * (y_true - y_pred)\n",
    "        self.dinputs = -2 * (y_true - y_pred) / outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "class LossBinaryCrossentropy:\n",
    "    \"\"\"\n",
    "    Binary Cross-Entropy loss function.\n",
    "    L = -[y * log(ŷ) + (1-y) * log(1-ŷ)]\n",
    "    \n",
    "    Used for binary classification problems.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name=\"BinaryCrossentropy\"):\n",
    "        self.name = name\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Forward pass: calculate binary cross-entropy loss.\n",
    "        \n",
    "        Parameters:\n",
    "            y_pred: predicted probabilities\n",
    "            y_true: true binary labels\n",
    "        \n",
    "        Returns:\n",
    "            loss: binary cross-entropy\n",
    "        \"\"\"\n",
    "        # Clip predictions to prevent log(0)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # Calculate sample-wise loss\n",
    "        sample_losses = -(y_true * np.log(y_pred_clipped) + \n",
    "                         (1 - y_true) * np.log(1 - y_pred_clipped))\n",
    "        sample_losses = np.mean(sample_losses, axis=-1)\n",
    "        \n",
    "        return np.mean(sample_losses)\n",
    "    \n",
    "    def backward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradient of binary cross-entropy loss.\n",
    "        \n",
    "        Parameters:\n",
    "            y_pred: predicted probabilities\n",
    "            y_true: true binary labels\n",
    "        \"\"\"\n",
    "        # Number of samples\n",
    "        samples = len(y_pred)\n",
    "        # Number of outputs\n",
    "        outputs = len(y_pred[0])\n",
    "        \n",
    "        # Clip predictions to prevent division by 0\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # Calculate gradient\n",
    "        self.dinputs = -(y_true / y_pred_clipped - \n",
    "                        (1 - y_true) / (1 - y_pred_clipped)) / outputs\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "class LossCategoricalCrossentropy:\n",
    "    \"\"\"\n",
    "    Categorical Cross-Entropy loss function.\n",
    "    L = -Σ(y_true * log(y_pred))\n",
    "    \n",
    "    Used for multi-class classification problems with one-hot encoded labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name=\"CategoricalCrossentropy\"):\n",
    "        self.name = name\n",
    "    \n",
    "    def forward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Forward pass: calculate categorical cross-entropy loss.\n",
    "        \n",
    "        Parameters:\n",
    "            y_pred: predicted probabilities (after softmax)\n",
    "            y_true: true labels (one-hot encoded or class indices)\n",
    "        \n",
    "        Returns:\n",
    "            loss: categorical cross-entropy\n",
    "        \"\"\"\n",
    "        # Number of samples\n",
    "        samples = len(y_pred)\n",
    "        \n",
    "        # Clip predictions to prevent log(0)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # Handle both one-hot encoded and sparse labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            # Sparse labels (class indices)\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        elif len(y_true.shape) == 2:\n",
    "            # One-hot encoded labels\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "        \n",
    "        # Calculate loss\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return np.mean(negative_log_likelihoods)\n",
    "    \n",
    "    def backward(self, y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradient of categorical cross-entropy loss.\n",
    "        \n",
    "        Parameters:\n",
    "            y_pred: predicted probabilities\n",
    "            y_true: true labels (one-hot encoded or class indices)\n",
    "        \"\"\"\n",
    "        # Number of samples\n",
    "        samples = len(y_pred)\n",
    "        # Number of labels\n",
    "        labels = len(y_pred[0])\n",
    "        \n",
    "        # If labels are sparse, convert to one-hot\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "        \n",
    "        # Calculate gradient\n",
    "        self.dinputs = -y_true / y_pred\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# OPTIMIZER\n",
    "# ============================================================\n",
    "\n",
    "class OptimizerSGD:\n",
    "    \"\"\"\n",
    "    Stochastic Gradient Descent optimizer.\n",
    "    Updates weights using: w = w - learning_rate * gradient\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, decay=0.0, momentum=0.0, adaptive=None, epsilon=1e-7):\n",
    "        \"\"\"\n",
    "        Optimizer supporting learning rate decay, momentum and adaptive gradient (Adagrad).\n",
    "\n",
    "        Parameters:\n",
    "            learning_rate: initial learning rate\n",
    "            decay: learning rate decay per epoch (0.0 means no decay)\n",
    "            momentum: momentum coefficient (0.0 means vanilla SGD)\n",
    "            adaptive: None or 'adagrad' to enable Adagrad-style updates\n",
    "            epsilon: small value to avoid division by zero for adaptive methods\n",
    "        \"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "        self.adaptive = adaptive\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        \"\"\"\n",
    "        Call before forward/backward pass for this epoch/iteration.\n",
    "        Updates the current learning rate using decay as specified.\n",
    "        \"\"\"\n",
    "        if self.decay:\n",
    "            # apply decay based on number of iterations so far\n",
    "            self.current_learning_rate = self.learning_rate / (1.0 + self.decay * self.iterations)\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        \"\"\"\n",
    "        Update layer parameters using configured method(s).\n",
    "\n",
    "        This supports vanilla SGD, momentum, and Adagrad.\n",
    "        \"\"\"\n",
    "        # Ensure gradients exist\n",
    "        if not hasattr(layer, 'dweights') or not hasattr(layer, 'dbiases'):\n",
    "            raise AttributeError(\"Layer must have dweights and dbiases for optimizer to update parameters\")\n",
    "\n",
    "        # ----- Momentum -----\n",
    "        if self.momentum:\n",
    "            # Create momentum buffers if not present\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "            # Update momentum with current gradients\n",
    "            layer.weight_momentums = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.bias_momentums = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "\n",
    "            # Apply updates\n",
    "            layer.weights += layer.weight_momentums\n",
    "            layer.biases += layer.bias_momentums\n",
    "\n",
    "        else:\n",
    "            # ----- Adaptive gradient (Adagrad) -----\n",
    "            if self.adaptive == 'adagrad':\n",
    "                if not hasattr(layer, 'weight_cache'):\n",
    "                    layer.weight_cache = np.zeros_like(layer.weights)\n",
    "                    layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "                # Accumulate squared gradients\n",
    "                layer.weight_cache += layer.dweights ** 2\n",
    "                layer.bias_cache += layer.dbiases ** 2\n",
    "\n",
    "                # Compute adjusted learning rates per-parameter\n",
    "                weight_adjustment = (self.current_learning_rate / (np.sqrt(layer.weight_cache) + self.epsilon)) * layer.dweights\n",
    "                bias_adjustment = (self.current_learning_rate / (np.sqrt(layer.bias_cache) + self.epsilon)) * layer.dbiases\n",
    "\n",
    "                # Update parameters (note the minus since gradient descent)\n",
    "                layer.weights -= weight_adjustment\n",
    "                layer.biases -= bias_adjustment\n",
    "\n",
    "            else:\n",
    "                # ----- Vanilla SGD -----\n",
    "                layer.weights -= self.current_learning_rate * layer.dweights\n",
    "                layer.biases -= self.current_learning_rate * layer.dbiases\n",
    "\n",
    "    def post_update_params(self):\n",
    "        \"\"\"\n",
    "        Call after parameters have been updated for an iteration.\n",
    "        Increments the internal iteration counter.\n",
    "        \"\"\"\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# EXAMPLE USAGE\n",
    "# ============================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Scikit-learn's Iris dataset for a small multi-class example\n",
    "    from sklearn.datasets import load_iris\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"NEURAL NETWORK TRAINING: IRIS DATASET (3-CLASS)\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "\n",
    "    iris = load_iris()\n",
    "    X = iris.data  # shape (150, 4)\n",
    "    y = iris.target  # shape (150,) values in {0,1,2}\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    # Train/test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # Convert labels to one-hot for training\n",
    "    num_classes = len(np.unique(y))\n",
    "    y_train_onehot = np.eye(num_classes)[y_train]\n",
    "\n",
    "    print(f\"[INFO] Dataset: Iris | X.shape={X.shape}, y.shape={y.shape}\")\n",
    "    print(f\"       >> Train: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
    "    print(f\"       >> Test : X_test={X_test.shape}, y_test={y_test.shape}\\n\")\n",
    "\n",
    "    # Build network: 4 -> 8 (ReLU) -> 3 (Softmax)\n",
    "    print(\"[INFO] Building Neural Network Architecture...\")\n",
    "    layer1 = Layer_Dense(4, 8, name=\"Hidden_Layer_1\")\n",
    "    activation1 = ActivationReLU(name=\"ReLU_1\")\n",
    "\n",
    "    layer2 = Layer_Dense(8, num_classes, name=\"Output_Layer\")\n",
    "    activation2 = ActivationSoftmax(name=\"Softmax_Output\")\n",
    "\n",
    "    # Loss function and optimizer (categorical)\n",
    "    loss_function = LossCategoricalCrossentropy()\n",
    "    optimizer = OptimizerSGD(learning_rate=0.1, decay=1e-3, momentum=0.9)\n",
    "\n",
    "    print(f\"[INFO] Loss Function: {loss_function.name}\")\n",
    "    print(f\"[INFO] Optimizer: SGD (lr={optimizer.learning_rate}, decay={optimizer.decay}, momentum={optimizer.momentum})\")\n",
    "    print(f\"[INFO] Training for 1000 epochs...\\n\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Training loop\n",
    "    epochs = 1000\n",
    "    for epoch in range(epochs):\n",
    "        # Pre-update (decay) before FP/BP\n",
    "        if hasattr(optimizer, 'pre_update_params'):\n",
    "            optimizer.pre_update_params()\n",
    "\n",
    "        # Forward pass\n",
    "        layer1.forward(X_train)\n",
    "        activation1.forward(layer1.output)\n",
    "\n",
    "        layer2.forward(activation1.output)\n",
    "        activation2.forward(layer2.output)\n",
    "\n",
    "        # Loss\n",
    "        loss = loss_function.forward(activation2.output, y_train_onehot)\n",
    "\n",
    "        # Calculate categorical accuracy on training set\n",
    "        preds = np.argmax(activation2.output, axis=1)\n",
    "        accuracy = np.mean(preds == y_train)\n",
    "\n",
    "        if epoch % 100 == 0 or epoch == epochs - 1:\n",
    "            print(f\"Epoch {epoch:4d} | Loss: {loss:.6f} | Train Accuracy: {accuracy*100:.2f}% | LR: {optimizer.current_learning_rate:.5f}\")\n",
    "\n",
    "        # Backward pass\n",
    "        loss_function.backward(activation2.output, y_train_onehot)\n",
    "        activation2.backward(loss_function.dinputs)\n",
    "        layer2.backward(activation2.dinputs)\n",
    "\n",
    "        activation1.backward(layer2.dinputs)\n",
    "        layer1.backward(activation1.dinputs)\n",
    "\n",
    "        # Update params\n",
    "        optimizer.update_params(layer1)\n",
    "        optimizer.update_params(layer2)\n",
    "\n",
    "        if hasattr(optimizer, 'post_update_params'):\n",
    "            optimizer.post_update_params()\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"\\n[INFO] Training Complete!\\n\")\n",
    "\n",
    "    # Evaluate on test set\n",
    "    layer1.forward(X_test)\n",
    "    activation1.forward(layer1.output)\n",
    "    layer2.forward(activation1.output)\n",
    "    activation2.forward(layer2.output)\n",
    "\n",
    "    test_preds = np.argmax(activation2.output, axis=1)\n",
    "    test_accuracy = np.mean(test_preds == y_test)\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"TEST RESULTS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Test Accuracy: {test_accuracy*100:.2f}%\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # ============================================================\n",
    "    # COMPARISON: Momentum SGD vs Adagrad\n",
    "    # ============================================================\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"OPTIMIZER COMPARISON: Momentum SGD vs Adagrad\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    def build_model():\n",
    "        # Recreate fresh layers to avoid parameter sharing\n",
    "        l1 = Layer_Dense(4, 8, name=\"Hidden_Layer_1\")\n",
    "        a1 = ActivationReLU()\n",
    "        l2 = Layer_Dense(8, num_classes, name=\"Output_Layer\")\n",
    "        a2 = ActivationSoftmax()\n",
    "        return l1, a1, l2, a2\n",
    "\n",
    "    def train_with_optimizer(optimizer, epochs=500):\n",
    "        l1, a1, l2, a2 = build_model()\n",
    "        loss_fn = LossCategoricalCrossentropy()\n",
    "\n",
    "        loss_history = []\n",
    "        acc_history = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            if hasattr(optimizer, 'pre_update_params'):\n",
    "                optimizer.pre_update_params()\n",
    "\n",
    "            # forward\n",
    "            l1.forward(X_train)\n",
    "            a1.forward(l1.output)\n",
    "            l2.forward(a1.output)\n",
    "            a2.forward(l2.output)\n",
    "\n",
    "            loss = loss_fn.forward(a2.output, y_train_onehot)\n",
    "            preds = np.argmax(a2.output, axis=1)\n",
    "            acc = np.mean(preds == y_train)\n",
    "\n",
    "            loss_history.append(loss)\n",
    "            acc_history.append(acc)\n",
    "\n",
    "            # backward\n",
    "            loss_fn.backward(a2.output, y_train_onehot)\n",
    "            a2.backward(loss_fn.dinputs)\n",
    "            l2.backward(a2.dinputs)\n",
    "            a1.backward(l2.dinputs)\n",
    "            l1.backward(a1.dinputs)\n",
    "\n",
    "            # update\n",
    "            optimizer.update_params(l1)\n",
    "            optimizer.update_params(l2)\n",
    "\n",
    "            if hasattr(optimizer, 'post_update_params'):\n",
    "                optimizer.post_update_params()\n",
    "\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'loss_history': np.array(loss_history),\n",
    "            'acc_history': np.array(acc_history),\n",
    "            'final_accuracy': acc_history[-1],\n",
    "            'final_loss': loss_history[-1]\n",
    "        }\n",
    "\n",
    "    def detect_stabilization(loss_history, window=10, tol=1e-4):\n",
    "        \"\"\"\n",
    "        Detect first epoch where moving average change falls below tol for given window.\n",
    "        \"\"\"\n",
    "        if len(loss_history) < window * 2:\n",
    "            return len(loss_history)\n",
    "\n",
    "        # moving average differences\n",
    "        ma = np.convolve(loss_history, np.ones(window)/window, mode='valid')\n",
    "        diffs = np.abs(np.diff(ma))\n",
    "        for i, d in enumerate(diffs):\n",
    "            if d < tol:\n",
    "                # return epoch index in original space\n",
    "                return i + window\n",
    "        return len(loss_history)\n",
    "\n",
    "    # Configure optimizers to compare\n",
    "    opt_momentum = OptimizerSGD(learning_rate=0.1, decay=1e-3, momentum=0.9)\n",
    "    opt_adagrad = OptimizerSGD(learning_rate=0.1, decay=1e-3, momentum=0.0, adaptive='adagrad')\n",
    "\n",
    "    # Train both for 1000 epochs\n",
    "    res_mom = train_with_optimizer(opt_momentum, epochs=1000)\n",
    "    res_adag = train_with_optimizer(opt_adagrad, epochs=1000)\n",
    "\n",
    "    stab_mom = detect_stabilization(res_mom['loss_history'])\n",
    "    stab_adag = detect_stabilization(res_adag['loss_history'])\n",
    "\n",
    "    # Display comparison\n",
    "    print(\"=\" * 60, \"\\n\")\n",
    "    print(\"Optimizer      | Stabilize Epoch | Final Loss  | Final Acc (%)\")\n",
    "    print(\"-----------------------------------------------------------\")\n",
    "    print(f\"Momentum SGD   | {stab_mom:14d} | {res_mom['final_loss']:.6f} | {res_mom['final_accuracy']*100:12.2f}\")\n",
    "    print(f\"Adagrad        | {stab_adag:14d} | {res_adag['final_loss']:.6f} | {res_adag['final_accuracy']*100:12.2f}\")\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "    print(\"\\n[INFO] Comparison complete.\\n\")\n",
    "    print(\"=\" * 60)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

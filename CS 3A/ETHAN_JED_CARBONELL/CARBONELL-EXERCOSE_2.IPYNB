{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b48d5f6c",
   "metadata": {},
   "source": [
    "<style>\n",
    "    .info-card {\n",
    "        max-width: 650px;\n",
    "        margin: 25px auto;\n",
    "        padding: 25px 30px;\n",
    "        border: 1px solid #e0e0e0;\n",
    "        border-radius: 12px;\n",
    "        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.05);\n",
    "        font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial, sans-serif;\n",
    "        background-color: #fdfdfd;\n",
    "        color: #333;\n",
    "    }\n",
    "    .info-card .title {\n",
    "        color: #1a237e; /* Dark Indigo */\n",
    "        font-size: 24px;\n",
    "        font-weight: 600;\n",
    "        margin-top: 0;\n",
    "        margin-bottom: 15px;\n",
    "        text-align: center;\n",
    "        border-bottom: 2px solid #e8eaf6; /* Light Indigo */\n",
    "        padding-bottom: 10px;\n",
    "    }\n",
    "    .info-card .details-grid {\n",
    "        display: grid;\n",
    "        grid-template-columns: max-content 1fr;\n",
    "        gap: 12px 20px;\n",
    "        margin-top: 20px;\n",
    "        font-size: 16px;\n",
    "    }\n",
    "    .info-card .label {\n",
    "        font-weight: 600;\n",
    "        color: #555;\n",
    "        text-align: right;\n",
    "    }\n",
    "    .info-card .value {\n",
    "        font-weight: 400;\n",
    "        color: #222;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"info-card\">\n",
    "    <h2 class=\"title\">Unit 2 Exercise</h2>\n",
    "    <div class=\"details-grid\">\n",
    "        <div class=\"label\">Name:</div>\n",
    "        <div class=\"value\">Ethan Jed V. Carbonell</div>\n",
    "        <div class=\"label\">Date:</div>\n",
    "        <div class=\"value\">September 12, 2025</div>\n",
    "        <div class=\"label\">Year & Section:</div>\n",
    "        <div class=\"value\">BSCS 3A AI</div>\n",
    "        <div></div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "244cfee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Dense_Layer:\n",
    "    def __init__(self, n_inputs, n_neurons, activation_function=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_inputs (int): The number of input features (or neurons from the previous layer).\n",
    "            n_neurons (int): The number of neurons in this layer.\n",
    "            activation_function (str, optional): The name of the activation function to use.\n",
    "                                                 Supported: 'relu', 'sigmoid', 'softmax'.\n",
    "        \"\"\"\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "        \n",
    "        self.activation_function_name = activation_function\n",
    "        self.inputs = None\n",
    "        self.output = None\n",
    "        self.activated_output = None\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            inputs (np.array): Input data or outputs from the previous layer.\n",
    "        \n",
    "        Returns:\n",
    "            np.array: The raw output of the layer (Z = X.W + b).\n",
    "        \"\"\"\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        return self.output\n",
    "\n",
    "    def activate(self, raw_output=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            raw_output (np.array, optional): The raw output from the forward pass. \n",
    "                                              If None, uses the layer's stored self.output.\n",
    "        \n",
    "        Returns:\n",
    "            np.array: The output of the layer after activation.\n",
    "        \"\"\"\n",
    "        if raw_output is None:\n",
    "            raw_output = self.output\n",
    "            \n",
    "        if self.activation_function_name == 'relu':\n",
    "            self.activated_output = np.maximum(0, raw_output)\n",
    "            \n",
    "        elif self.activation_function_name == 'sigmoid':\n",
    "            self.activated_output = 1 / (1 + np.exp(-raw_output))\n",
    "            \n",
    "        elif self.activation_function_name == 'softmax':\n",
    "            exp_values = np.exp(raw_output - np.max(raw_output, axis=1, keepdims=True))\n",
    "            probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "            self.activated_output = probabilities\n",
    "            \n",
    "        else:\n",
    "            # defaults to linear activation\n",
    "            self.activated_output = raw_output\n",
    "\n",
    "        return self.activated_output\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_loss(y_pred, y_true):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            y_pred (np.array): Predicted probabilities from the output layer.\n",
    "            y_true (np.array): Ground truth labels.\n",
    "        \n",
    "        Returns:\n",
    "            float: The mean loss for the batch.\n",
    "        \"\"\"\n",
    "        samples = len(y_pred)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "\n",
    "        if len(y_true.shape) == 1: # Sparse\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        elif len(y_true.shape) == 2: # One-hot\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid shape for y_true. Must be 1D or 2D.\")\n",
    "\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        \n",
    "        data_loss = np.mean(negative_log_likelihoods)\n",
    "        return data_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7bbb4f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# This is the class developed in the previous step. It is included here for completeness.\n",
    "class Dense_Layer:\n",
    "    \"\"\"A class representing a dense (fully connected) layer in a neural network.\"\"\"\n",
    "\n",
    "    def __init__(self, n_inputs, n_neurons, weights=None, biases=None):\n",
    "        \"\"\"\n",
    "        (1) A function to setup/accept the inputs and weights.\n",
    "        \"\"\"\n",
    "        if weights is None or biases is None:\n",
    "            self.weights = 0.10 * np.random.randn(n_inputs, n_neurons)\n",
    "            self.biases = np.zeros((1, n_neurons))\n",
    "        else:\n",
    "            self.weights = np.array(weights)\n",
    "            self.biases = np.array(biases).reshape(1, -1)\n",
    "            \n",
    "        if self.weights.shape != (n_inputs, n_neurons):\n",
    "            raise ValueError(f\"Shape of weights {self.weights.shape} does not match expected shape {(n_inputs, n_neurons)}\")\n",
    "        if self.biases.shape != (1, n_neurons):\n",
    "            raise ValueError(f\"Shape of biases {self.biases.shape} does not match expected shape {(1, n_neurons)}\")\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        (2) A function to perform the weighted sum + bias.\n",
    "        \"\"\"\n",
    "        self.inputs = np.array(inputs)\n",
    "        self.raw_output = np.dot(self.inputs, self.weights) + self.biases\n",
    "        return self.raw_output\n",
    "\n",
    "    def activate(self, activation_function):\n",
    "        \"\"\"\n",
    "        (3) A function to perform the selected activation function.\n",
    "        \"\"\"\n",
    "        self.activation_name = activation_function.lower()\n",
    "        \n",
    "        if self.activation_name == 'relu':\n",
    "            self.output = np.maximum(0, self.raw_output)\n",
    "        elif self.activation_name == 'sigmoid':\n",
    "            self.output = 1 / (1 + np.exp(-self.raw_output))\n",
    "        elif self.activation_name == 'softmax':\n",
    "            exp_values = np.exp(self.raw_output - np.max(self.raw_output, axis=1, keepdims=True))\n",
    "            self.output = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function. Choose from 'relu', 'sigmoid', 'softmax'.\")\n",
    "        \n",
    "        return self.output\n",
    "\n",
    "    @staticmethod\n",
    "    def calculate_categorical_loss(y_pred, y_true):\n",
    "        \"\"\"\n",
    "        (4) A function to calculate the loss (Categorical Cross-Entropy).\n",
    "        \"\"\"\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return np.mean(negative_log_likelihoods)\n",
    "        \n",
    "    @staticmethod\n",
    "    def calculate_bce_loss(y_pred, y_true):\n",
    "        \"\"\"\n",
    "        (4) A function to calculate the loss for binary classification (Binary Cross-Entropy).\n",
    "        \"\"\"\n",
    "        # Convert y_true to a numpy array to ensure vectorized operations work\n",
    "        y_true_np = np.array(y_true)\n",
    "        \n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        loss = -(y_true_np * np.log(y_pred_clipped) + (1 - y_true_np) * np.log(1 - y_pred_clipped))\n",
    "        return np.mean(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5d22e3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iris Dataset Neural Network\n",
      "\n",
      "Input data: [[5.1 3.5 1.4 0.2]]\n",
      "Target output: [[0.7 0.2 0.1]]\n",
      "\n",
      "Layer 1 [ReLU]  \n",
      "Output: [[3.93 0.15 0.85]]\n",
      "\n",
      "Layer 2 [Sigmoid]  \n",
      "Output: [[0.99378157 0.99187781]]\n",
      "\n",
      "Layer 3 [Softmax]  \n",
      "Output: [[0.0265075  0.96865119 0.00484132]]\n",
      "\n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "FINAL RESULTS\n",
      "\n",
      "Final Predicted Output (Probabilities): [[0.0265075  0.96865119 0.00484132]]\n",
      "Predicted Iris Species: Iris-versicolor\n",
      "\n",
      "Loss (Categorical Cross-Entropy): 1.5475\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Iris Dataset Neural Network\\n\")\n",
    "    inputs = [[5.1, 3.5, 1.4, 0.2]]\n",
    "    \n",
    "    target_output = [[0.7, 0.2, 0.1]]\n",
    "\n",
    "    weights1 = [\n",
    "        [0.2, 0.5, -0.3],\n",
    "        [0.1, -0.2, 0.4],\n",
    "        [-0.4, 0.3, 0.2],\n",
    "        [0.6, -0.1, 0.5]\n",
    "    ]\n",
    "    biases1 = [3.0, -2.1, 0.6]\n",
    "\n",
    "    weights2 = [\n",
    "        [0.3, -0.5],\n",
    "        [0.7, 0.2],\n",
    "        [-0.6, 0.4]\n",
    "    ]\n",
    "    biases2 = [4.3, 6.4]\n",
    "\n",
    "    weights3 = [\n",
    "        [0.5, -0.3, 0.8],\n",
    "        [-0.2, 0.6, -0.4]\n",
    "    ]\n",
    "    biases3 = [-1.5, 2.1, -3.3]\n",
    "    \n",
    "    print(f\"Input data: {np.array(inputs)}\")\n",
    "    print(f\"Target output: {np.array(target_output)}\\n\")\n",
    "\n",
    "    layer1 = Dense_Layer(n_inputs=4, n_neurons=3, weights=weights1, biases=biases1)\n",
    "    layer2 = Dense_Layer(n_inputs=3, n_neurons=2, weights=weights2, biases=biases2)\n",
    "    layer3 = Dense_Layer(n_inputs=2, n_neurons=3, weights=weights3, biases=biases3)\n",
    "\n",
    "    \n",
    "    print(\"Layer 1 [ReLU]  \")\n",
    "    layer1.forward(inputs)\n",
    "    layer1_output = layer1.activate('relu')\n",
    "    print(f\"Output: {layer1_output}\\n\")\n",
    "    \n",
    "    print(\"Layer 2 [Sigmoid]  \")\n",
    "    layer2.forward(layer1_output)\n",
    "    layer2_output = layer2.activate('sigmoid')\n",
    "    print(f\"Output: {layer2_output}\\n\")\n",
    "\n",
    "    print(\"Layer 3 [Softmax]  \")\n",
    "    layer3.forward(layer2_output)\n",
    "    final_output = layer3.activate('softmax')\n",
    "    print(f\"Output: {final_output}\\n\")\n",
    "\n",
    "        \n",
    "    print(\"\\n----------------------------------------------------------------------------\\nFINAL RESULTS\\n\")\n",
    "    print(f\"Final Predicted Output (Probabilities): {final_output}\")\n",
    "    \n",
    "    iris_species = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
    "    predicted_class_index = np.argmax(final_output)\n",
    "    predicted_species = iris_species[predicted_class_index]\n",
    "    \n",
    "    print(f\"Predicted Iris Species: {predicted_species}\\n\")\n",
    "    \n",
    "    loss = Dense_Layer.calculate_categorical_loss(final_output, target_output)\n",
    "    print(f\"Loss (Categorical Cross-Entropy): {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea27d484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Breast Cancer Dataset Neural Network\n",
      "\n",
      "Input data: [[14.1   20.3    0.095]]\n",
      "Target output: 1 (Malignant)\n",
      "\n",
      "Layer 1 [ReLU]  \n",
      "Output: [[3.2235 3.4755 0.    ]]\n",
      "\n",
      "Layer 2 [Sigmoid]  \n",
      "Output: [[0.79232544 0.49267552]]\n",
      "\n",
      "Layer 3 [Sigmoid]  \n",
      "\n",
      "----------------------------------------------------------------------------\n",
      "FINAL RESULTS\n",
      "\n",
      "Final Predicted Output (Probability): 0.6244\n",
      "\n",
      "Predicted Class: 1 (Malignant)\n",
      "\n",
      "Loss (Binary Cross-Entropy): 0.4710\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    print(\"Breast Cancer Dataset Neural Network\\n\")\n",
    "\n",
    "    inputs = [[14.1, 20.3, 0.095]]\n",
    "    \n",
    "    #1 for Malignant\n",
    "    target_output = [[1]]\n",
    "\n",
    "    weights1 = [\n",
    "        [0.5, -0.3, 0.8],\n",
    "        [-0.2, 0.4, -0.6],\n",
    "        [-0.7, 0.9, 0.1]\n",
    "    ]\n",
    "    biases1 = [0.3, -0.5, 0.6]\n",
    "\n",
    "    # Layer 2: 3 inputs, 2 neurons. Given weights are (2, 3), need to transpose to (3, 2).\n",
    "    weights2_raw = [\n",
    "        [0.6, -0.2, 0.4],\n",
    "        [-0.3, 0.5, 0.7]\n",
    "    ]\n",
    "    weights2 = np.array(weights2_raw).T\n",
    "    biases2 = [0.1, -0.8]\n",
    "\n",
    "    weights3_raw = [[0.7, -0.5]]\n",
    "    weights3 = np.array(weights3_raw).T\n",
    "    biases3 = [0.2]\n",
    "    \n",
    "    print(f\"Input data: {np.array(inputs)}\")\n",
    "    print(f\"Target output: {target_output[0][0]} (Malignant)\\n\")\n",
    "\n",
    "    layer1 = Dense_Layer(n_inputs=3, n_neurons=3, weights=weights1, biases=biases1)\n",
    "    layer2 = Dense_Layer(n_inputs=3, n_neurons=2, weights=weights2, biases=biases2)\n",
    "    layer3 = Dense_Layer(n_inputs=2, n_neurons=1, weights=weights3, biases=biases3)\n",
    "\n",
    "    \n",
    "    print(\"Layer 1 [ReLU]  \")\n",
    "    layer1.forward(inputs)\n",
    "    layer1_output = layer1.activate('relu')\n",
    "    print(f\"Output: {layer1_output}\\n\")\n",
    "    \n",
    "    print(\"Layer 2 [Sigmoid]  \")\n",
    "    layer2.forward(layer1_output)\n",
    "    layer2_output = layer2.activate('sigmoid')\n",
    "    print(f\"Output: {layer2_output}\\n\")\n",
    "\n",
    "    print(\"Layer 3 [Sigmoid]  \")\n",
    "    layer3.forward(layer2_output)\n",
    "    final_output = layer3.activate('sigmoid')\n",
    "    print(f\"Output: {final_output}\\n\")\n",
    "    \n",
    "    \n",
    "    print(\"\\n----------------------------------------------------------------------------\\nFINAL RESULTS\\n\")\n",
    "    print(f\"Final Predicted Output (Probability): {final_output[0][0]:.4f}\\n\")\n",
    "    \n",
    "    predicted_class = 1 if final_output[0][0] > 0.5 else 0\n",
    "    prediction_label = \"Malignant\" if predicted_class == 1 else \"Benign\"\n",
    "    \n",
    "    print(f\"Predicted Class: {predicted_class} ({prediction_label})\\n\")\n",
    "    \n",
    "    loss = Dense_Layer.calculate_bce_loss(final_output, target_output)\n",
    "    print(f\"Loss (Binary Cross-Entropy): {loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b56a37f6",
   "metadata": {},
   "source": [
    "<style>\n",
    "    .info-card {\n",
    "        max-width: 650px;\n",
    "        margin: 25px auto;\n",
    "        padding: 25px 30px;\n",
    "        border: 1px solid #e0e0e0;\n",
    "        border-radius: 12px;\n",
    "        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.05);\n",
    "        font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial, sans-serif;\n",
    "        background-color: #fdfdfd;\n",
    "        color: #333;\n",
    "    }\n",
    "    .info-card .title {\n",
    "        color: #1a237e; /* Dark Indigo */\n",
    "        font-size: 24px;\n",
    "        font-weight: 600;\n",
    "        margin-top: 0;\n",
    "        margin-bottom: 15px;\n",
    "        text-align: center;\n",
    "        border-bottom: 2px solid #e8eaf6; /* Light Indigo */\n",
    "        padding-bottom: 10px;\n",
    "    }\n",
    "    .info-card .details-grid {\n",
    "        display: grid;\n",
    "        grid-template-columns: max-content 1fr;\n",
    "        gap: 12px 20px;\n",
    "        margin-top: 20px;\n",
    "        font-size: 16px;\n",
    "    }\n",
    "    .info-card .label {\n",
    "        font-weight: 600;\n",
    "        color: #555;\n",
    "        text-align: right;\n",
    "    }\n",
    "    .info-card .value {\n",
    "        font-weight: 400;\n",
    "        color: #222;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"info-card\">\n",
    "    <h2 class=\"title\">Unit 4 Exercise</h2>\n",
    "    <div class=\"details-grid\">\n",
    "        <div class=\"label\">Name:</div>\n",
    "        <div class=\"value\">Ethan Jed V. Carbonell</div>\n",
    "        <div class=\"label\">Date:</div>\n",
    "        <div class=\"value\">October 17, 2025</div>\n",
    "        <div class=\"label\">Year & Section:</div>\n",
    "        <div class=\"value\">BSCS 3A AI</div>\n",
    "        <div></div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a1cf2c",
   "metadata": {},
   "source": [
    "## Library imports\n",
    "### Set np.random.seed to 0 for fair comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "427a8f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "\n",
    "nnfs.init()\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c1f91b",
   "metadata": {},
   "source": [
    "## Classes\n",
    "### Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "d299628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden Layers\n",
    "# Dense\n",
    "class Layer_Dense:\n",
    "    # Layer initialization\n",
    "    # randomly initialize weights and set biases to zero\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember the input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate the output values from inputs, weight and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass/Backpropagation\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters:\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0783f20",
   "metadata": {},
   "source": [
    "### ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "53e662ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU\n",
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember the input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate the output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Make a copy of the original values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "    \n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5d5516",
   "metadata": {},
   "source": [
    "### Softmax with Categorical Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "691a705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax_Loss_CategoricalCrossEntropy():\n",
    "    # Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        pass # No activation or loss objects needed separately\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Remember inputs for backward pass\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "        \n",
    "        # Calculate loss\n",
    "        # Clip data to prevent division by 0\n",
    "        y_pred_clipped = np.clip(self.output, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # Probabilities for target values - only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(len(self.output)), y_true]\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "            \n",
    "        # Calculate and return the mean loss\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return np.mean(negative_log_likelihoods)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        # If labels are one-hot encoded, turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "            \n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient using the simplified and stable formula\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5221df70",
   "metadata": {},
   "source": [
    "### Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "c3724831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD Optimizer (with learning rate decay and momentum)\n",
    "class Optimizer_SGD:\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If we use momentum\n",
    "        if self.momentum:\n",
    "            # If layer does not contain momentum arrays, create them\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                # If there is no momentum array for biases\n",
    "                # create it\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            \n",
    "            # Build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            weight_updates = \\\n",
    "                self.momentum * layer.weight_momentums - \\\n",
    "                self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "            \n",
    "            # Build bias updates\n",
    "            bias_updates = \\\n",
    "                self.momentum * layer.bias_momentums - \\\n",
    "                self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "            \n",
    "        # Vanilla SGD updates (as before momentum update)\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "        \n",
    "        # Update weights and biases using either\n",
    "        # vanilla or momentum updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# AdaGrad optimizer\n",
    "class Optimizer_Adagrad:\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "                \n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            \n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbiases**2\n",
    "        \n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "            layer.dweights / \\\n",
    "            (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "            layer.dbiases / \\\n",
    "            (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "            \n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595e86af",
   "metadata": {},
   "source": [
    "#### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "d63af870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "X, y = spiral_data(samples = 100, classes = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac5f02d",
   "metadata": {},
   "source": [
    "#### NN Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "50354965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense Layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# ReLU activation for the Dense layer above\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# 2nd dense layer with 64 input and 3 output values (for 3 classes)\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossEntropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d149139d",
   "metadata": {},
   "source": [
    "### Optimizer Selection & Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "41766c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with: SGD with Momentum\n",
      "epoch: 0, acc: 0.360, loss: 1.099, lr: 0.2000\n",
      "epoch: 100, acc: 0.407, loss: 1.079, lr: 0.1980\n",
      "epoch: 200, acc: 0.410, loss: 1.076, lr: 0.1961\n",
      "epoch: 300, acc: 0.403, loss: 1.072, lr: 0.1942\n",
      "epoch: 400, acc: 0.423, loss: 1.063, lr: 0.1923\n",
      "epoch: 500, acc: 0.447, loss: 1.039, lr: 0.1905\n",
      "epoch: 600, acc: 0.537, loss: 0.997, lr: 0.1887\n",
      "epoch: 700, acc: 0.610, loss: 0.933, lr: 0.1869\n",
      "epoch: 800, acc: 0.623, loss: 0.870, lr: 0.1852\n",
      "epoch: 900, acc: 0.657, loss: 0.818, lr: 0.1835\n",
      "epoch: 1000, acc: 0.713, loss: 0.785, lr: 0.1818\n"
     ]
    }
   ],
   "source": [
    "# Stochastic Gradient Descent (SGD)\n",
    "# print(\"Running with: Vanilla SGD\")\n",
    "# optimizer = Optimizer_SGD(learning_rate=1.0)\n",
    "\n",
    "# SGD with Learning Rate Decay\n",
    "# print(\"Running with: SGD w LR Decay\")\n",
    "# optimizer = Optimizer_SGD(learning_rate=1.0, decay=1e-3)\n",
    "\n",
    "# SGD with Momentum\n",
    "print(\"Running with: SGD with Momentum\")\n",
    "optimizer = Optimizer_SGD(learning_rate=0.2, decay=1e-4, momentum=0.9)\n",
    "\n",
    "# Adaptive Gradient (AdaGrad)\n",
    "# print(\"Running with: AdaGrad\")\n",
    "# optimizer = Optimizer_Adagrad(learning_rate=1.5, decay=0)\n",
    "\n",
    "epochs = 1001 # Set number of epochs\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "    # Pass the output of the dense layer through the activation function\n",
    "    activation1.forward(dense1.output)\n",
    "    # Pass on to the 2nd layer\n",
    "    dense2.forward(activation1.output)\n",
    "    # Activation function for the 2nd layer + Loss\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # --- Print progress every 100 epochs ---\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # Print progress\n",
    "    if not epoch % 100:\n",
    "        # Get predictions from the activation output\n",
    "        predictions = np.argmax(loss_activation.output, axis=1)\n",
    "        accuracy = np.mean(predictions == y)\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.3f}, ' +\n",
    "              f'lr: {optimizer.current_learning_rate:.4f}')\n",
    "\n",
    "    # Backward pass from loss\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update learning rate (if decay is used)\n",
    "    optimizer.pre_update_params()\n",
    "    # Update the weights and biases of each layer\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    # Increment iteration count\n",
    "    optimizer.post_update_params()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

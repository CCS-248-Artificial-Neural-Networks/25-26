{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b56a37f6",
   "metadata": {},
   "source": [
    "<style>\n",
    "    .info-card {\n",
    "        max-width: 650px;\n",
    "        margin: 25px auto;\n",
    "        padding: 25px 30px;\n",
    "        border: 1px solid #e0e0e0;\n",
    "        border-radius: 12px;\n",
    "        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.05);\n",
    "        font-family: -apple-system, BlinkMacSystemFont, \"Segoe UI\", Roboto, \"Helvetica Neue\", Arial, sans-serif;\n",
    "        background-color: #fdfdfd;\n",
    "        color: #333;\n",
    "    }\n",
    "    .info-card .title {\n",
    "        color: #1a237e; /* Dark Indigo */\n",
    "        font-size: 24px;\n",
    "        font-weight: 600;\n",
    "        margin-top: 0;\n",
    "        margin-bottom: 15px;\n",
    "        text-align: center;\n",
    "        border-bottom: 2px solid #e8eaf6; /* Light Indigo */\n",
    "        padding-bottom: 10px;\n",
    "    }\n",
    "    .info-card .details-grid {\n",
    "        display: grid;\n",
    "        grid-template-columns: max-content 1fr;\n",
    "        gap: 12px 20px;\n",
    "        margin-top: 20px;\n",
    "        font-size: 16px;\n",
    "    }\n",
    "    .info-card .label {\n",
    "        font-weight: 600;\n",
    "        color: #555;\n",
    "        text-align: right;\n",
    "    }\n",
    "    .info-card .value {\n",
    "        font-weight: 400;\n",
    "        color: #222;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"info-card\">\n",
    "    <h2 class=\"title\">Unit 4 Exercise</h2>\n",
    "    <div class=\"details-grid\">\n",
    "        <div class=\"label\">Name:</div>\n",
    "        <div class=\"value\">Ethan Jed V. Carbonell</div>\n",
    "        <div class=\"label\">Date:</div>\n",
    "        <div class=\"value\">October 17, 2025</div>\n",
    "        <div class=\"label\">Year & Section:</div>\n",
    "        <div class=\"value\">BSCS 3A AI</div>\n",
    "        <div></div>\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a1cf2c",
   "metadata": {},
   "source": [
    "## Library imports\n",
    "### Set np.random.seed to 0 for fair comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "427a8f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nnfs\n",
    "from nnfs.datasets import spiral_data\n",
    "\n",
    "\n",
    "nnfs.init()\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c1f91b",
   "metadata": {},
   "source": [
    "## Classes\n",
    "### Hidden Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "d299628f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hidden Layers\n",
    "# Dense\n",
    "class Layer_Dense:\n",
    "    # Layer initialization\n",
    "    # randomly initialize weights and set biases to zero\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember the input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate the output values from inputs, weight and biases\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    # Backward pass/Backpropagation\n",
    "    def backward(self, dvalues):\n",
    "        # Gradients on parameters:\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        # Gradient on values\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0783f20",
   "metadata": {},
   "source": [
    "### ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "53e662ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReLU\n",
    "class Activation_ReLU:\n",
    "    # Forward pass\n",
    "    def forward(self, inputs):\n",
    "        # Remember the input values\n",
    "        self.inputs = inputs\n",
    "        # Calculate the output values from inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues):\n",
    "        # Make a copy of the original values first\n",
    "        self.dinputs = dvalues.copy()\n",
    "    \n",
    "        # Zero gradient where input values were negative\n",
    "        self.dinputs[self.inputs <= 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5d5516",
   "metadata": {},
   "source": [
    "### Softmax with Categorical Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "691a705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation_Softmax_Loss_CategoricalCrossEntropy():\n",
    "    # Creates activation and loss function objects\n",
    "    def __init__(self):\n",
    "        pass # No activation or loss objects needed separately\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, inputs, y_true):\n",
    "        # Remember inputs for backward pass\n",
    "        self.inputs = inputs\n",
    "        \n",
    "        # Get unnormalized probabilities\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        # Normalize them for each sample\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "        \n",
    "        # Calculate loss\n",
    "        # Clip data to prevent division by 0\n",
    "        y_pred_clipped = np.clip(self.output, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        # Probabilities for target values - only if categorical labels\n",
    "        if len(y_true.shape) == 1:\n",
    "            correct_confidences = y_pred_clipped[range(len(self.output)), y_true]\n",
    "        # Mask values - only for one-hot encoded labels\n",
    "        elif len(y_true.shape) == 2:\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "            \n",
    "        # Calculate and return the mean loss\n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return np.mean(negative_log_likelihoods)\n",
    "\n",
    "    # Backward pass\n",
    "    def backward(self, dvalues, y_true):\n",
    "        # Number of samples\n",
    "        samples = len(dvalues)\n",
    "        \n",
    "        # If labels are one-hot encoded, turn them into discrete values\n",
    "        if len(y_true.shape) == 2:\n",
    "            y_true = np.argmax(y_true, axis=1)\n",
    "            \n",
    "        # Copy so we can safely modify\n",
    "        self.dinputs = dvalues.copy()\n",
    "        # Calculate gradient using the simplified and stable formula\n",
    "        self.dinputs[range(samples), y_true] -= 1\n",
    "        # Normalize gradient\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5221df70",
   "metadata": {},
   "source": [
    "### Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "c3724831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGD Optimizer (with learning rate decay and momentum)\n",
    "class Optimizer_SGD:\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=1., decay=0., momentum=0.):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum\n",
    "\n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "\n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If we use momentum\n",
    "        if self.momentum:\n",
    "            # If layer does not contain momentum arrays, create them\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                # If there is no momentum array for biases\n",
    "                # create it\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "            \n",
    "            # Build weight updates with momentum - take previous\n",
    "            # updates multiplied by retain factor and update with\n",
    "            # current gradients\n",
    "            weight_updates = \\\n",
    "                self.momentum * layer.weight_momentums - \\\n",
    "                self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "            \n",
    "            # Build bias updates\n",
    "            bias_updates = \\\n",
    "                self.momentum * layer.bias_momentums - \\\n",
    "                self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "            \n",
    "        # Vanilla SGD updates (as before momentum update)\n",
    "        else:\n",
    "            weight_updates = -self.current_learning_rate * layer.dweights\n",
    "            bias_updates = -self.current_learning_rate * layer.dbiases\n",
    "        \n",
    "        # Update weights and biases using either\n",
    "        # vanilla or momentum updates\n",
    "        layer.weights += weight_updates\n",
    "        layer.biases += bias_updates\n",
    "\n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "# AdaGrad optimizer\n",
    "class Optimizer_Adagrad:\n",
    "    # Initialize optimizer - set settings\n",
    "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay\n",
    "        self.iterations = 0\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "    # Call once before any parameter updates\n",
    "    def pre_update_params(self):\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate * \\\n",
    "                (1. / (1. + self.decay * self.iterations))\n",
    "                \n",
    "    # Update parameters\n",
    "    def update_params(self, layer):\n",
    "        # If layer does not contain cache arrays,\n",
    "        # create them\n",
    "        if not hasattr(layer, 'weight_cache'):\n",
    "            layer.weight_cache = np.zeros_like(layer.weights)\n",
    "            layer.bias_cache = np.zeros_like(layer.biases)\n",
    "            \n",
    "        # Update cache with squared current gradients\n",
    "        layer.weight_cache += layer.dweights**2\n",
    "        layer.bias_cache += layer.dbiases**2\n",
    "        \n",
    "        # Vanilla SGD parameter update + normalization\n",
    "        # with square rooted cache\n",
    "        layer.weights += -self.current_learning_rate * \\\n",
    "            layer.dweights / \\\n",
    "            (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "        layer.biases += -self.current_learning_rate * \\\n",
    "            layer.dbiases / \\\n",
    "            (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "            \n",
    "    # Call once after any parameter updates\n",
    "    def post_update_params(self):\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595e86af",
   "metadata": {},
   "source": [
    "#### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "d63af870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "X, y = spiral_data(samples = 100, classes = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac5f02d",
   "metadata": {},
   "source": [
    "#### NN Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "50354965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense Layer with 2 input features and 64 output values\n",
    "dense1 = Layer_Dense(2, 64)\n",
    "\n",
    "# ReLU activation for the Dense layer above\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# 2nd dense layer with 64 input and 3 output values (for 3 classes)\n",
    "dense2 = Layer_Dense(64, 3)\n",
    "\n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossEntropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d149139d",
   "metadata": {},
   "source": [
    "### Optimizer Selection & Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "41766c06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with: SGD with Momentum\n",
      "epoch: 0, acc: 0.360, loss: 1.099, lr: 0.2000\n",
      "epoch: 100, acc: 0.407, loss: 1.079, lr: 0.1980\n",
      "epoch: 200, acc: 0.410, loss: 1.076, lr: 0.1961\n",
      "epoch: 300, acc: 0.403, loss: 1.072, lr: 0.1942\n",
      "epoch: 400, acc: 0.423, loss: 1.063, lr: 0.1923\n",
      "epoch: 500, acc: 0.447, loss: 1.039, lr: 0.1905\n",
      "epoch: 600, acc: 0.537, loss: 0.997, lr: 0.1887\n",
      "epoch: 700, acc: 0.610, loss: 0.933, lr: 0.1869\n",
      "epoch: 800, acc: 0.623, loss: 0.870, lr: 0.1852\n",
      "epoch: 900, acc: 0.657, loss: 0.818, lr: 0.1835\n",
      "epoch: 1000, acc: 0.713, loss: 0.785, lr: 0.1818\n"
     ]
    }
   ],
   "source": [
    "# Stochastic Gradient Descent (SGD)\n",
    "# print(\"Running with: Vanilla SGD\")\n",
    "# optimizer = Optimizer_SGD(learning_rate=1.0)\n",
    "\n",
    "# SGD with Learning Rate Decay\n",
    "# print(\"Running with: SGD w LR Decay\")\n",
    "# optimizer = Optimizer_SGD(learning_rate=1.0, decay=1e-3)\n",
    "\n",
    "# SGD with Momentum\n",
    "print(\"Running with: SGD with Momentum\")\n",
    "optimizer = Optimizer_SGD(learning_rate=0.2, decay=1e-4, momentum=0.9)\n",
    "\n",
    "# Adaptive Gradient (AdaGrad)\n",
    "# print(\"Running with: AdaGrad\")\n",
    "# optimizer = Optimizer_Adagrad(learning_rate=1.5, decay=0)\n",
    "\n",
    "epochs = 1001 # Set number of epochs\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # Perform a forward pass of our training data through this layer\n",
    "    dense1.forward(X)\n",
    "    # Pass the output of the dense layer through the activation function\n",
    "    activation1.forward(dense1.output)\n",
    "    # Pass on to the 2nd layer\n",
    "    dense2.forward(activation1.output)\n",
    "    # Activation function for the 2nd layer + Loss\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # --- Print progress every 100 epochs ---\n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "\n",
    "    # Print progress\n",
    "    if not epoch % 100:\n",
    "        # Get predictions from the activation output\n",
    "        predictions = np.argmax(loss_activation.output, axis=1)\n",
    "        accuracy = np.mean(predictions == y)\n",
    "        print(f'epoch: {epoch}, ' +\n",
    "              f'acc: {accuracy:.3f}, ' +\n",
    "              f'loss: {loss:.3f}, ' +\n",
    "              f'lr: {optimizer.current_learning_rate:.4f}')\n",
    "\n",
    "    # Backward pass from loss\n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs)\n",
    "    dense1.backward(activation1.dinputs)\n",
    "\n",
    "    # Update learning rate (if decay is used)\n",
    "    optimizer.pre_update_params()\n",
    "    # Update the weights and biases of each layer\n",
    "    optimizer.update_params(dense1)\n",
    "    optimizer.update_params(dense2)\n",
    "    # Increment iteration count\n",
    "    optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabd04e4",
   "metadata": {},
   "source": [
    "# Results\n",
    "See output html file below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "fb0f7678",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    .pdf-mimic-container {\n",
       "        max-width: 800px;\n",
       "        margin: 40px auto;\n",
       "        padding: 50px 60px;\n",
       "        border: 1px solid #dcdcdc;\n",
       "        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.08);\n",
       "        background-color: #ffffff;\n",
       "        font-family: Georgia, 'Times New Roman', Times, serif;\n",
       "        font-size: 16px;\n",
       "        line-height: 1.6;\n",
       "        color: #333;\n",
       "    }\n",
       "    .pdf-mimic-container .header {\n",
       "        text-align: right;\n",
       "        font-size: 14px;\n",
       "        color: #777;\n",
       "        margin-bottom: 20px;\n",
       "        padding-bottom: 10px;\n",
       "        border-bottom: 1px solid #eee;\n",
       "    }\n",
       "    .pdf-mimic-container .footer {\n",
       "        text-align: center;\n",
       "        font-size: 14px;\n",
       "        color: #777;\n",
       "        margin-top: 40px;\n",
       "        padding-top: 10px;\n",
       "        border-top: 1px solid #eee;\n",
       "    }\n",
       "    .pdf-mimic-container h1 {\n",
       "        text-align: center;\n",
       "        font-size: 24px;\n",
       "        margin-top: 0;\n",
       "        margin-bottom: 10px;\n",
       "        font-weight: 600;\n",
       "        color: #222;\n",
       "    }\n",
       "    .pdf-mimic-container .author-info {\n",
       "        text-align: center;\n",
       "        font-size: 16px;\n",
       "        color: #555;\n",
       "        margin-bottom: 40px;\n",
       "    }\n",
       "    .pdf-mimic-container h2 {\n",
       "        font-size: 22px;\n",
       "        margin-top: 40px;\n",
       "        margin-bottom: 15px;\n",
       "        padding-bottom: 5px;\n",
       "        border-bottom: 2px solid #e0e0e0;\n",
       "        font-weight: 600;\n",
       "    }\n",
       "    .pdf-mimic-container h3 {\n",
       "        font-size: 18px;\n",
       "        margin-top: 25px;\n",
       "        margin-bottom: 10px;\n",
       "        font-weight: 600;\n",
       "    }\n",
       "    .pdf-mimic-container table {\n",
       "        width: 100%;\n",
       "        border-collapse: collapse;\n",
       "        margin-top: 20px;\n",
       "        margin-bottom: 20px;\n",
       "    }\n",
       "    .pdf-mimic-container caption {\n",
       "        caption-side: top;\n",
       "        text-align: left;\n",
       "        font-weight: bold;\n",
       "        padding-bottom: 10px;\n",
       "        font-size: 16px;\n",
       "    }\n",
       "    .pdf-mimic-container th, .pdf-mimic-container td {\n",
       "        padding: 10px 12px;\n",
       "        text-align: left;\n",
       "        border-bottom: 1px solid #e0e0e0;\n",
       "    }\n",
       "    .pdf-mimic-container thead th {\n",
       "        border-bottom: 2px solid #555;\n",
       "        font-weight: bold;\n",
       "    }\n",
       "    .pdf-mimic-container tbody tr:last-child td {\n",
       "        border-bottom: none;\n",
       "    }\n",
       "    .pdf-mimic-container ul {\n",
       "        padding-left: 25px;\n",
       "    }\n",
       "    .pdf-mimic-container li {\n",
       "        margin-bottom: 8px;\n",
       "    }\n",
       "    .pdf-mimic-container strong {\n",
       "        font-weight: bold;\n",
       "    }\n",
       "    .pdf-mimic-container code {\n",
       "        font-family: 'Courier New', Courier, monospace;\n",
       "        background-color: #f4f4f4;\n",
       "        padding: 2px 5px;\n",
       "        border-radius: 4px;\n",
       "        font-size: 0.9em;\n",
       "    }\n",
       "    .graph-placeholder {\n",
       "        border: 2px dashed #ccc;\n",
       "        background-color: #f9f9f9;\n",
       "        padding: 60px 20px;\n",
       "        margin: 25px 0;\n",
       "        text-align: center;\n",
       "        color: #888;\n",
       "        font-style: italic;\n",
       "        font-size: 1em;\n",
       "    }\n",
       "</style>\n",
       "\n",
       "<div class=\"pdf-mimic-container\">\n",
       "\n",
       "    <div class=\"header\">Optimizer Performance Analysis</div>\n",
       "\n",
       "    <h1>Unit 4 Exercise: A Comparative Analysis of Neural Network Optimizers</h1>\n",
       "    <div class=\"author-info\">\n",
       "        Ethan Jed V. Carbonell<br>\n",
       "        BSCS 3A AI<br>\n",
       "        October 17, 2025\n",
       "    </div>\n",
       "\n",
       "    <h2>1 Experimental Setup</h2>\n",
       "    <p>The experiment was conducted using a consistent model architecture and dataset to ensure a fair comparison between the optimizers.</p>\n",
       "\n",
       "    <h3>1.1 Dataset</h3>\n",
       "    <p>The model was trained on the <code>spiral_data</code> dataset, generated using the <code>nnfs</code> library. This dataset is a standard benchmark for classification tasks that are not linearly separable. The configuration used was:</p>\n",
       "    <ul>\n",
       "        <li><strong>Samples:</strong> 100</li>\n",
       "        <li><strong>Classes:</strong> 3</li>\n",
       "        <li><strong>Features:</strong> 2 (x, y coordinates)</li>\n",
       "    </ul>\n",
       "\n",
       "    <h3>1.2 Model Architecture</h3>\n",
       "    <p>A simple sequential feed-forward neural network was constructed with the following layers:</p>\n",
       "    <table>\n",
       "        <caption>Table 1: Neural Network Architecture</caption>\n",
       "        <thead>\n",
       "            <tr>\n",
       "                <th>Layer Type</th>\n",
       "                <th>Input Shape</th>\n",
       "                <th>Output Shape</th>\n",
       "                <th>Activation Function</th>\n",
       "            </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "            <tr>\n",
       "                <td>Input Layer</td>\n",
       "                <td>2</td>\n",
       "                <td>2</td>\n",
       "                <td>-</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td>Dense Layer 1</td>\n",
       "                <td>2</td>\n",
       "                <td>64</td>\n",
       "                <td>ReLU</td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td>Dense Layer 2</td>\n",
       "                <td>64</td>\n",
       "                <td>3</td>\n",
       "                <td>Softmax</td>\n",
       "            </tr>\n",
       "        </tbody>\n",
       "    </table>\n",
       "    <p>The <strong>Categorical Cross-Entropy</strong> loss function was used in combination with the Softmax activation on the final layer, as this is standard practice for multi-class classification problems.</p>\n",
       "    \n",
       "    <h3>1.3 Optimizer Configuration</h3>\n",
       "    <p>After a process of hyperparameter tuning, the final configurations for the two optimizers were selected as shown in Table 2.</p>\n",
       "    <table>\n",
       "        <caption>Table 2: Final Hyperparameter Settings</caption>\n",
       "        <thead>\n",
       "            <tr>\n",
       "                <th>Optimizer</th>\n",
       "                <th>Parameters</th>\n",
       "            </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "            <tr>\n",
       "                <td>SGD with Momentum</td>\n",
       "                <td><code>learning_rate=0.2</code>, <code>decay=1e-4</code>, <code>momentum=0.9</code></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td>AdaGrad</td>\n",
       "                <td><code>learning_rate=1.5</code>, <code>decay=0</code>, <code>epsilon=1e-7</code></td>\n",
       "            </tr>\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "    <h2>2 Hyperparameter Tuning Rationale</h2>\n",
       "    \n",
       "    <h3>2.1 SGD with Momentum</h3>\n",
       "    <p>The final parameters for the SGD optimizer were chosen after a process of balancing training speed against stability. Initial experiments with a higher learning rate (e.g., 0.5) demonstrated rapid learning but proved to be unstable, characterized by significant dips in accuracy and \"overshooting\" the optimal solution.</p>\n",
       "    <p>To correct this, a more conservative learning rate of <strong>0.2</strong> was selected. This eliminated the instability, resulting in a much more consistent increase in accuracy and a reliable convergence path. The decay rate of <strong>1e-4</strong> was found to be effective in gradually reducing the learning rate for fine-tuning in later epochs, while the standard momentum value of <strong>0.9</strong> was retained to help the optimizer overcome local minima and plateaus.</p>\n",
       "\n",
       "    <h3>2.2 AdaGrad</h3>\n",
       "    <p>The chosen configuration for the AdaGrad optimizer was designed specifically to counteract the algorithm's inherent weakness: premature learning rate decay. Initial attempts that included an external decay rate (e.g., <code>decay=1e-3</code>) resulted in a \"double decay\" effect, where the combination of external decay and AdaGrad's internal mechanism caused the learning rate to diminish too quickly, leading to training stagnation.</p>\n",
       "    <p>The most critical step was setting <strong><code>decay=0</code></strong>, which allowed AdaGrad's adaptive learning rate to function as intended. To further combat the inevitable decay from AdaGrad's internal cache, the initial learning rate was increased to <strong>1.5</strong>. This provided the optimizer with a larger initial \"budget,\" enabling it to learn effectively for a longer period before the learning rate became too small to make meaningful updates, ultimately leading to a much-improved final accuracy.</p>\n",
       "\n",
       "    <h2>3 Results</h2>\n",
       "    <p>The model was trained for 1001 epochs with each optimizer. The following tables show the accuracy, loss, and learning rate at 100-epoch intervals.</p>\n",
       "    \n",
       "    <table>\n",
       "        <caption>Table 3: Training Progress with SGD with Momentum</caption>\n",
       "        <thead>\n",
       "            <tr>\n",
       "                <th>Epoch</th>\n",
       "                <th>Accuracy</th>\n",
       "                <th>Loss</th>\n",
       "                <th>Learning Rate</th>\n",
       "            </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "            <tr><td>0</td><td>0.360</td><td>1.099</td><td>0.2000</td></tr>\n",
       "            <tr><td>100</td><td>0.407</td><td>1.079</td><td>0.1980</td></tr>\n",
       "            <tr><td>200</td><td>0.410</td><td>1.076</td><td>0.1961</td></tr>\n",
       "            <tr><td>300</td><td>0.403</td><td>1.072</td><td>0.1942</td></tr>\n",
       "            <tr><td>400</td><td>0.423</td><td>1.063</td><td>0.1923</td></tr>\n",
       "            <tr><td>500</td><td>0.447</td><td>1.039</td><td>0.1905</td></tr>\n",
       "            <tr><td>600</td><td>0.537</td><td>0.997</td><td>0.1887</td></tr>\n",
       "            <tr><td>700</td><td>0.610</td><td>0.933</td><td>0.1869</td></tr>\n",
       "            <tr><td>800</td><td>0.623</td><td>0.870</td><td>0.1852</td></tr>\n",
       "            <tr><td>900</td><td>0.657</td><td>0.818</td><td>0.1835</td></tr>\n",
       "            <tr><td>1000</td><td>0.713</td><td>0.785</td><td>0.1818</td></tr>\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "    <table>\n",
       "        <caption>Table 4: Training Progress with AdaGrad</caption>\n",
       "        <thead>\n",
       "            <tr>\n",
       "                <th>Epoch</th>\n",
       "                <th>Accuracy</th>\n",
       "                <th>Loss</th>\n",
       "                <th>Learning Rate</th>\n",
       "            </tr>\n",
       "        </thead>\n",
       "        <tbody>\n",
       "            <tr><td>0</td><td>0.360</td><td>1.099</td><td>1.5000</td></tr>\n",
       "            <tr><td>100</td><td>0.520</td><td>1.006</td><td>1.5000</td></tr>\n",
       "            <tr><td>200</td><td>0.550</td><td>0.924</td><td>1.5000</td></tr>\n",
       "            <tr><td>300</td><td>0.607</td><td>0.860</td><td>1.5000</td></tr>\n",
       "            <tr><td>400</td><td>0.647</td><td>0.801</td><td>1.5000</td></tr>\n",
       "            <tr><td>500</td><td>0.633</td><td>0.757</td><td>1.5000</td></tr>\n",
       "            <tr><td>600</td><td>0.650</td><td>0.725</td><td>1.5000</td></tr>\n",
       "            <tr><td>700</td><td>0.697</td><td>0.682</td><td>1.5000</td></tr>\n",
       "            <tr><td>800</td><td>0.677</td><td>0.682</td><td>1.5000</td></tr>\n",
       "            <tr><td>900</td><td>0.740</td><td>0.612</td><td>1.5000</td></tr>\n",
       "            <tr><td>1000</td><td>0.730</td><td>0.603</td><td>1.5000</td></tr>\n",
       "        </tbody>\n",
       "    </table>\n",
       "\n",
       "    <h2>4 Analysis and Comparison</h2>\n",
       "    <h3>4.1 Loss Stabilization</h3>\n",
       "    <p>Comparing the loss values in Tables 3 and 4 reveals a clear difference in convergence speed.</p>\n",
       "\n",
       "    <ul>\n",
       "        <li><strong>AdaGrad:</strong> The loss decreased very rapidly in the initial epochs. By epoch 300, the loss had already dropped to 0.860. It continued to decrease and began to stabilize around epoch 700-800, fluctuating near a value of 0.682 before making a final drop. The rapid initial progress is characteristic of adaptive optimizers on this type of problem.</li>\n",
       "        <li><strong>SGD with Momentum:</strong> The loss decreased much more slowly and steadily. It took until epoch 800 for the loss to reach 0.870, a level AdaGrad surpassed within the first 300 epochs. The loss did not show clear signs of stabilization by epoch 1000, suggesting that it was still converging, albeit slowly.</li>\n",
       "    </ul>\n",
       "    <p><strong>Conclusion:</strong> AdaGrad stabilized the loss significantly faster than SGD with Momentum.</p>\n",
       "\n",
       "    <h3>4.2 Model Accuracy</h3>\n",
       "    <p>The primary metric for model performance is its final accuracy.</p>\n",
       "\n",
       "    <ul>\n",
       "        <li><strong>AdaGrad:</strong> Achieved a higher final accuracy of <strong>73.0%</strong> at epoch 1000, with a peak accuracy of <strong>74.0%</strong> at epoch 900. It also reached key accuracy milestones much faster, exceeding 60% accuracy by epoch 300.</li>\n",
       "        <li><strong>SGD with Momentum:</strong> Reached a final accuracy of <strong>71.3%</strong> at epoch 1000. Its progress was slower, requiring around 700 epochs to surpass the 60% accuracy mark.</li>\n",
       "    </ul>\n",
       "    <p><strong>Conclusion:</strong> For this specific task and hyperparameter configuration, AdaGrad produced a model with slightly higher accuracy and converged to that accuracy much more quickly.</p>\n",
       "\n",
       "    <h2>5 Conclusion</h2>\n",
       "    <p>This experiment successfully demonstrated the functional differences between the SGD with Momentum and AdaGrad optimizers. AdaGrad exhibited the key strengths of an adaptive optimizer: rapid initial convergence and strong performance without the need for manual learning rate scheduling. It achieved a final accuracy of 73.0% and stabilized the model's loss relatively early in the training process.</p>\n",
       "    <p>Conversely, SGD with Momentum provided a slower but steady convergence, reaching a final accuracy of 71.3%. While effective, it required more epochs to achieve a comparable level of performance.</p>\n",
       "    <p>The results suggest that for problems like the spiral dataset, an adaptive optimizer like AdaGrad can offer a significant advantage in training efficiency. However, the tuning process for AdaGrad proved more nuanced, as its performance was highly sensitive to the interaction between its internal adaptive mechanism and any external learning rate decay.</p>\n",
       "\n",
       "    <div class=\"footer\">\n",
       "        Ethan Jed V. Carbonell\n",
       "    </div>\n",
       "\n",
       "</div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "\n",
    "# Store the entire HTML and CSS content in a multi-line string\n",
    "html_content = \"\"\"\n",
    "<style>\n",
    "    .pdf-mimic-container {\n",
    "        max-width: 800px;\n",
    "        margin: 40px auto;\n",
    "        padding: 50px 60px;\n",
    "        border: 1px solid #dcdcdc;\n",
    "        box-shadow: 0 4px 12px rgba(0, 0, 0, 0.08);\n",
    "        background-color: #ffffff;\n",
    "        font-family: Georgia, 'Times New Roman', Times, serif;\n",
    "        font-size: 16px;\n",
    "        line-height: 1.6;\n",
    "        color: #333;\n",
    "    }\n",
    "    .pdf-mimic-container .header {\n",
    "        text-align: right;\n",
    "        font-size: 14px;\n",
    "        color: #777;\n",
    "        margin-bottom: 20px;\n",
    "        padding-bottom: 10px;\n",
    "        border-bottom: 1px solid #eee;\n",
    "    }\n",
    "    .pdf-mimic-container .footer {\n",
    "        text-align: center;\n",
    "        font-size: 14px;\n",
    "        color: #777;\n",
    "        margin-top: 40px;\n",
    "        padding-top: 10px;\n",
    "        border-top: 1px solid #eee;\n",
    "    }\n",
    "    .pdf-mimic-container h1 {\n",
    "        text-align: center;\n",
    "        font-size: 24px;\n",
    "        margin-top: 0;\n",
    "        margin-bottom: 10px;\n",
    "        font-weight: 600;\n",
    "        color: #222;\n",
    "    }\n",
    "    .pdf-mimic-container .author-info {\n",
    "        text-align: center;\n",
    "        font-size: 16px;\n",
    "        color: #555;\n",
    "        margin-bottom: 40px;\n",
    "    }\n",
    "    .pdf-mimic-container h2 {\n",
    "        font-size: 22px;\n",
    "        margin-top: 40px;\n",
    "        margin-bottom: 15px;\n",
    "        padding-bottom: 5px;\n",
    "        border-bottom: 2px solid #e0e0e0;\n",
    "        font-weight: 600;\n",
    "    }\n",
    "    .pdf-mimic-container h3 {\n",
    "        font-size: 18px;\n",
    "        margin-top: 25px;\n",
    "        margin-bottom: 10px;\n",
    "        font-weight: 600;\n",
    "    }\n",
    "    .pdf-mimic-container table {\n",
    "        width: 100%;\n",
    "        border-collapse: collapse;\n",
    "        margin-top: 20px;\n",
    "        margin-bottom: 20px;\n",
    "    }\n",
    "    .pdf-mimic-container caption {\n",
    "        caption-side: top;\n",
    "        text-align: left;\n",
    "        font-weight: bold;\n",
    "        padding-bottom: 10px;\n",
    "        font-size: 16px;\n",
    "    }\n",
    "    .pdf-mimic-container th, .pdf-mimic-container td {\n",
    "        padding: 10px 12px;\n",
    "        text-align: left;\n",
    "        border-bottom: 1px solid #e0e0e0;\n",
    "    }\n",
    "    .pdf-mimic-container thead th {\n",
    "        border-bottom: 2px solid #555;\n",
    "        font-weight: bold;\n",
    "    }\n",
    "    .pdf-mimic-container tbody tr:last-child td {\n",
    "        border-bottom: none;\n",
    "    }\n",
    "    .pdf-mimic-container ul {\n",
    "        padding-left: 25px;\n",
    "    }\n",
    "    .pdf-mimic-container li {\n",
    "        margin-bottom: 8px;\n",
    "    }\n",
    "    .pdf-mimic-container strong {\n",
    "        font-weight: bold;\n",
    "    }\n",
    "    .pdf-mimic-container code {\n",
    "        font-family: 'Courier New', Courier, monospace;\n",
    "        background-color: #f4f4f4;\n",
    "        padding: 2px 5px;\n",
    "        border-radius: 4px;\n",
    "        font-size: 0.9em;\n",
    "    }\n",
    "    .graph-placeholder {\n",
    "        border: 2px dashed #ccc;\n",
    "        background-color: #f9f9f9;\n",
    "        padding: 60px 20px;\n",
    "        margin: 25px 0;\n",
    "        text-align: center;\n",
    "        color: #888;\n",
    "        font-style: italic;\n",
    "        font-size: 1em;\n",
    "    }\n",
    "</style>\n",
    "\n",
    "<div class=\"pdf-mimic-container\">\n",
    "\n",
    "    <div class=\"header\">Optimizer Performance Analysis</div>\n",
    "\n",
    "    <h1>Unit 4 Exercise: A Comparative Analysis of Neural Network Optimizers</h1>\n",
    "    <div class=\"author-info\">\n",
    "        Ethan Jed V. Carbonell<br>\n",
    "        BSCS 3A AI<br>\n",
    "        October 17, 2025\n",
    "    </div>\n",
    "\n",
    "    <h2>1 Experimental Setup</h2>\n",
    "    <p>The experiment was conducted using a consistent model architecture and dataset to ensure a fair comparison between the optimizers.</p>\n",
    "\n",
    "    <h3>1.1 Dataset</h3>\n",
    "    <p>The model was trained on the <code>spiral_data</code> dataset, generated using the <code>nnfs</code> library. This dataset is a standard benchmark for classification tasks that are not linearly separable. The configuration used was:</p>\n",
    "    <ul>\n",
    "        <li><strong>Samples:</strong> 100</li>\n",
    "        <li><strong>Classes:</strong> 3</li>\n",
    "        <li><strong>Features:</strong> 2 (x, y coordinates)</li>\n",
    "    </ul>\n",
    "\n",
    "    <h3>1.2 Model Architecture</h3>\n",
    "    <p>A simple sequential feed-forward neural network was constructed with the following layers:</p>\n",
    "    <table>\n",
    "        <caption>Table 1: Neural Network Architecture</caption>\n",
    "        <thead>\n",
    "            <tr>\n",
    "                <th>Layer Type</th>\n",
    "                <th>Input Shape</th>\n",
    "                <th>Output Shape</th>\n",
    "                <th>Activation Function</th>\n",
    "            </tr>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "            <tr>\n",
    "                <td>Input Layer</td>\n",
    "                <td>2</td>\n",
    "                <td>2</td>\n",
    "                <td>-</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>Dense Layer 1</td>\n",
    "                <td>2</td>\n",
    "                <td>64</td>\n",
    "                <td>ReLU</td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>Dense Layer 2</td>\n",
    "                <td>64</td>\n",
    "                <td>3</td>\n",
    "                <td>Softmax</td>\n",
    "            </tr>\n",
    "        </tbody>\n",
    "    </table>\n",
    "    <p>The <strong>Categorical Cross-Entropy</strong> loss function was used in combination with the Softmax activation on the final layer, as this is standard practice for multi-class classification problems.</p>\n",
    "    \n",
    "    <h3>1.3 Optimizer Configuration</h3>\n",
    "    <p>After a process of hyperparameter tuning, the final configurations for the two optimizers were selected as shown in Table 2.</p>\n",
    "    <table>\n",
    "        <caption>Table 2: Final Hyperparameter Settings</caption>\n",
    "        <thead>\n",
    "            <tr>\n",
    "                <th>Optimizer</th>\n",
    "                <th>Parameters</th>\n",
    "            </tr>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "            <tr>\n",
    "                <td>SGD with Momentum</td>\n",
    "                <td><code>learning_rate=0.2</code>, <code>decay=1e-4</code>, <code>momentum=0.9</code></td>\n",
    "            </tr>\n",
    "            <tr>\n",
    "                <td>AdaGrad</td>\n",
    "                <td><code>learning_rate=1.5</code>, <code>decay=0</code>, <code>epsilon=1e-7</code></td>\n",
    "            </tr>\n",
    "        </tbody>\n",
    "    </table>\n",
    "\n",
    "    <h2>2 Hyperparameter Tuning Rationale</h2>\n",
    "    \n",
    "    <h3>2.1 SGD with Momentum</h3>\n",
    "    <p>The final parameters for the SGD optimizer were chosen after a process of balancing training speed against stability. Initial experiments with a higher learning rate (e.g., 0.5) demonstrated rapid learning but proved to be unstable, characterized by significant dips in accuracy and \"overshooting\" the optimal solution.</p>\n",
    "    <p>To correct this, a more conservative learning rate of <strong>0.2</strong> was selected. This eliminated the instability, resulting in a much more consistent increase in accuracy and a reliable convergence path. The decay rate of <strong>1e-4</strong> was found to be effective in gradually reducing the learning rate for fine-tuning in later epochs, while the standard momentum value of <strong>0.9</strong> was retained to help the optimizer overcome local minima and plateaus.</p>\n",
    "\n",
    "    <h3>2.2 AdaGrad</h3>\n",
    "    <p>The chosen configuration for the AdaGrad optimizer was designed specifically to counteract the algorithm's inherent weakness: premature learning rate decay. Initial attempts that included an external decay rate (e.g., <code>decay=1e-3</code>) resulted in a \"double decay\" effect, where the combination of external decay and AdaGrad's internal mechanism caused the learning rate to diminish too quickly, leading to training stagnation.</p>\n",
    "    <p>The most critical step was setting <strong><code>decay=0</code></strong>, which allowed AdaGrad's adaptive learning rate to function as intended. To further combat the inevitable decay from AdaGrad's internal cache, the initial learning rate was increased to <strong>1.5</strong>. This provided the optimizer with a larger initial \"budget,\" enabling it to learn effectively for a longer period before the learning rate became too small to make meaningful updates, ultimately leading to a much-improved final accuracy.</p>\n",
    "\n",
    "    <h2>3 Results</h2>\n",
    "    <p>The model was trained for 1001 epochs with each optimizer. The following tables show the accuracy, loss, and learning rate at 100-epoch intervals.</p>\n",
    "    \n",
    "    <table>\n",
    "        <caption>Table 3: Training Progress with SGD with Momentum</caption>\n",
    "        <thead>\n",
    "            <tr>\n",
    "                <th>Epoch</th>\n",
    "                <th>Accuracy</th>\n",
    "                <th>Loss</th>\n",
    "                <th>Learning Rate</th>\n",
    "            </tr>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "            <tr><td>0</td><td>0.360</td><td>1.099</td><td>0.2000</td></tr>\n",
    "            <tr><td>100</td><td>0.407</td><td>1.079</td><td>0.1980</td></tr>\n",
    "            <tr><td>200</td><td>0.410</td><td>1.076</td><td>0.1961</td></tr>\n",
    "            <tr><td>300</td><td>0.403</td><td>1.072</td><td>0.1942</td></tr>\n",
    "            <tr><td>400</td><td>0.423</td><td>1.063</td><td>0.1923</td></tr>\n",
    "            <tr><td>500</td><td>0.447</td><td>1.039</td><td>0.1905</td></tr>\n",
    "            <tr><td>600</td><td>0.537</td><td>0.997</td><td>0.1887</td></tr>\n",
    "            <tr><td>700</td><td>0.610</td><td>0.933</td><td>0.1869</td></tr>\n",
    "            <tr><td>800</td><td>0.623</td><td>0.870</td><td>0.1852</td></tr>\n",
    "            <tr><td>900</td><td>0.657</td><td>0.818</td><td>0.1835</td></tr>\n",
    "            <tr><td>1000</td><td>0.713</td><td>0.785</td><td>0.1818</td></tr>\n",
    "        </tbody>\n",
    "    </table>\n",
    "\n",
    "    <table>\n",
    "        <caption>Table 4: Training Progress with AdaGrad</caption>\n",
    "        <thead>\n",
    "            <tr>\n",
    "                <th>Epoch</th>\n",
    "                <th>Accuracy</th>\n",
    "                <th>Loss</th>\n",
    "                <th>Learning Rate</th>\n",
    "            </tr>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "            <tr><td>0</td><td>0.360</td><td>1.099</td><td>1.5000</td></tr>\n",
    "            <tr><td>100</td><td>0.520</td><td>1.006</td><td>1.5000</td></tr>\n",
    "            <tr><td>200</td><td>0.550</td><td>0.924</td><td>1.5000</td></tr>\n",
    "            <tr><td>300</td><td>0.607</td><td>0.860</td><td>1.5000</td></tr>\n",
    "            <tr><td>400</td><td>0.647</td><td>0.801</td><td>1.5000</td></tr>\n",
    "            <tr><td>500</td><td>0.633</td><td>0.757</td><td>1.5000</td></tr>\n",
    "            <tr><td>600</td><td>0.650</td><td>0.725</td><td>1.5000</td></tr>\n",
    "            <tr><td>700</td><td>0.697</td><td>0.682</td><td>1.5000</td></tr>\n",
    "            <tr><td>800</td><td>0.677</td><td>0.682</td><td>1.5000</td></tr>\n",
    "            <tr><td>900</td><td>0.740</td><td>0.612</td><td>1.5000</td></tr>\n",
    "            <tr><td>1000</td><td>0.730</td><td>0.603</td><td>1.5000</td></tr>\n",
    "        </tbody>\n",
    "    </table>\n",
    "\n",
    "    <h2>4 Analysis and Comparison</h2>\n",
    "    <h3>4.1 Loss Stabilization</h3>\n",
    "    <p>Comparing the loss values in Tables 3 and 4 reveals a clear difference in convergence speed.</p>\n",
    "\n",
    "    <ul>\n",
    "        <li><strong>AdaGrad:</strong> The loss decreased very rapidly in the initial epochs. By epoch 300, the loss had already dropped to 0.860. It continued to decrease and began to stabilize around epoch 700-800, fluctuating near a value of 0.682 before making a final drop. The rapid initial progress is characteristic of adaptive optimizers on this type of problem.</li>\n",
    "        <li><strong>SGD with Momentum:</strong> The loss decreased much more slowly and steadily. It took until epoch 800 for the loss to reach 0.870, a level AdaGrad surpassed within the first 300 epochs. The loss did not show clear signs of stabilization by epoch 1000, suggesting that it was still converging, albeit slowly.</li>\n",
    "    </ul>\n",
    "    <p><strong>Conclusion:</strong> AdaGrad stabilized the loss significantly faster than SGD with Momentum.</p>\n",
    "\n",
    "    <h3>4.2 Model Accuracy</h3>\n",
    "    <p>The primary metric for model performance is its final accuracy.</p>\n",
    "\n",
    "    <ul>\n",
    "        <li><strong>AdaGrad:</strong> Achieved a higher final accuracy of <strong>73.0%</strong> at epoch 1000, with a peak accuracy of <strong>74.0%</strong> at epoch 900. It also reached key accuracy milestones much faster, exceeding 60% accuracy by epoch 300.</li>\n",
    "        <li><strong>SGD with Momentum:</strong> Reached a final accuracy of <strong>71.3%</strong> at epoch 1000. Its progress was slower, requiring around 700 epochs to surpass the 60% accuracy mark.</li>\n",
    "    </ul>\n",
    "    <p><strong>Conclusion:</strong> For this specific task and hyperparameter configuration, AdaGrad produced a model with slightly higher accuracy and converged to that accuracy much more quickly.</p>\n",
    "\n",
    "    <h2>5 Conclusion</h2>\n",
    "    <p>This experiment successfully demonstrated the functional differences between the SGD with Momentum and AdaGrad optimizers. AdaGrad exhibited the key strengths of an adaptive optimizer: rapid initial convergence and strong performance without the need for manual learning rate scheduling. It achieved a final accuracy of 73.0% and stabilized the model's loss relatively early in the training process.</p>\n",
    "    <p>Conversely, SGD with Momentum provided a slower but steady convergence, reaching a final accuracy of 71.3%. While effective, it required more epochs to achieve a comparable level of performance.</p>\n",
    "    <p>The results suggest that for problems like the spiral dataset, an adaptive optimizer like AdaGrad can offer a significant advantage in training efficiency. However, the tuning process for AdaGrad proved more nuanced, as its performance was highly sensitive to the interaction between its internal adaptive mechanism and any external learning rate decay.</p>\n",
    "\n",
    "    <div class=\"footer\">\n",
    "        Ethan Jed V. Carbonell\n",
    "    </div>\n",
    "\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(html_content))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a30c6ed3",
   "metadata": {},
   "source": [
    "# Unit 4 – Neural Network Training and Optimization\n",
    "\n",
    "Arthur John Pagayon\n",
    "BSCS 3-A AI\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this exercise, I build a complete neural network from scratch and train it on the Spiral dataset. I explore different optimization techniques and compare their effectiveness.\n",
    "\n",
    "**IMPORTANT: Install the following Python packages FIRST before running:**\n",
    "1. **Numpy** - For numerical computations\n",
    "2. **NNFS** - For the Spiral dataset\n",
    "3. **scikit-learn** - (Optional) For additional datasets\n",
    "\n",
    "## Key Learning Objectives\n",
    "- Understand how backpropagation works\n",
    "- Implement gradient descent with momentum\n",
    "- Explore adaptive learning rates (Adagrad)\n",
    "- Compare optimizer performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0bc0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f37fe9",
   "metadata": {},
   "source": [
    "## Step 1: Define Core Classes for Modularity\n",
    "\n",
    "I'm implementing the fundamental building blocks of a neural network. Each class represents a specific layer type or activation function, making the code organized and reusable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1170794f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate spiral data for multi-class classification\n",
    "def spiral_data(samples, classes):\n",
    "    \"\"\"\n",
    "    Creates a spiral dataset with specified number of samples and classes.\n",
    "    This is a classic non-linearly separable dataset used to test neural networks.\n",
    "    \"\"\"\n",
    "    X = np.zeros((samples * classes, 2))\n",
    "    y = np.zeros(samples * classes, dtype='uint8')\n",
    "    for class_number in range(classes):\n",
    "        ix = range(samples * class_number, samples * (class_number + 1))\n",
    "        r = np.linspace(0.0, 1, samples)  # radius increases\n",
    "        # Create spiral pattern with noise\n",
    "        t = np.linspace(class_number * 4, (class_number + 1) * 4, samples) + np.random.randn(samples) * 0.2\n",
    "        X[ix] = np.c_[r * np.sin(t * 2.5), r * np.cos(t * 2.5)]\n",
    "        y[ix] = class_number\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Dense/Fully-connected layer\n",
    "class Layer_Dense:\n",
    "    \"\"\"\n",
    "    A fully connected layer performs: output = input @ weights + biases\n",
    "    It stores inputs for the backward pass (backpropagation)\n",
    "    \"\"\"\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        # Initialize weights with small random values\n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
    "        # Initialize biases to zero\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Compute output: Z = X·W + b\"\"\"\n",
    "        self.inputs = inputs\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        \"\"\"\n",
    "        Backward pass: compute gradients\n",
    "        - dweights: gradient w.r.t. weights\n",
    "        - dbiases: gradient w.r.t. biases  \n",
    "        - dinputs: gradient w.r.t. inputs (for previous layer)\n",
    "        \"\"\"\n",
    "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "        self.dinputs = np.dot(dvalues, self.weights.T)\n",
    "\n",
    "\n",
    "# ReLU activation: max(0, x) - introduces non-linearity\n",
    "class Activation_ReLU:\n",
    "    \"\"\"\n",
    "    ReLU (Rectified Linear Unit) activation function.\n",
    "    Outputs x if x > 0, else 0. Helps networks learn non-linear patterns.\n",
    "    \"\"\"\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        self.output = np.maximum(0, inputs)\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        \"\"\"Gradient flows only for positive inputs\"\"\"\n",
    "        self.dinputs = dvalues.copy()\n",
    "        self.dinputs[self.inputs <= 0] = 0\n",
    "\n",
    "\n",
    "# Softmax activation: converts to probability distribution\n",
    "class Activation_Softmax:\n",
    "    \"\"\"\n",
    "    Softmax activation for multi-class classification.\n",
    "    Converts raw outputs to probabilities that sum to 1.\n",
    "    Formula: softmax(z) = exp(z) / sum(exp(z))\n",
    "    \"\"\"\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        # Subtract max for numerical stability\n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        self.output = probabilities\n",
    "\n",
    "    def backward(self, dvalues):\n",
    "        \"\"\"Compute Jacobian matrix for each sample\"\"\"\n",
    "        self.dinputs = np.empty_like(dvalues)\n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)):\n",
    "            single_output = single_output.reshape(-1, 1)\n",
    "            # Jacobian = diag(p) - p·p^T\n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "\n",
    "\n",
    "# Categorical Cross-Entropy loss for multi-class classification\n",
    "class Loss_CategoricalCrossEntropy:\n",
    "    \"\"\"\n",
    "    Loss function: -sum(y_true * log(y_pred))\n",
    "    Measures how far the predicted probabilities are from the true labels.\n",
    "    \"\"\"\n",
    "    def forward(self, y_pred, y_true):\n",
    "        samples = y_pred.shape[0]\n",
    "        # Clip predictions to avoid log(0)\n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
    "        \n",
    "        if len(y_true.shape) == 1:\n",
    "            # Sparse labels (class indices)\n",
    "            correct_confidences = y_pred_clipped[range(samples), y_true]\n",
    "        elif len(y_true.shape) == 2:\n",
    "            # One-hot encoded labels\n",
    "            correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)\n",
    "        \n",
    "        negative_log_likelihoods = -np.log(correct_confidences)\n",
    "        return negative_log_likelihoods\n",
    "\n",
    "    def backward(self, dvalues, y_true):\n",
    "        \"\"\"Backward pass for loss\"\"\"\n",
    "        samples = len(dvalues)\n",
    "        labels = len(dvalues[0])\n",
    "        \n",
    "        # Convert to one-hot if sparse\n",
    "        if len(y_true.shape) == 1:\n",
    "            y_true = np.eye(labels)[y_true]\n",
    "        \n",
    "        # Gradient: -y_true / y_pred\n",
    "        self.dinputs = -y_true / dvalues\n",
    "        self.dinputs = self.dinputs / samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650dc533",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Optimizer with multiple techniques\n",
    "class Optimizer:\n",
    "    \"\"\"\n",
    "    SGD Optimizer with support for:\n",
    "    - Learning rate decay: reduces lr over time\n",
    "    - Momentum: accelerates gradient in consistent directions\n",
    "    - Adagrad: adaptive learning rate per parameter\n",
    "    \n",
    "    These techniques help optimization converge faster and reach better solutions.\n",
    "    \"\"\"\n",
    "    def __init__(self, learning_rate=1.0, decay=0.0, momentum=0.0, use_adagrad=False, epsilon=1e-7):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.current_learning_rate = learning_rate\n",
    "        self.decay = decay  # Learning rate decay rate\n",
    "        self.iterations = 0\n",
    "        self.momentum = momentum  # Momentum factor (0 = no momentum)\n",
    "        self.use_adagrad = use_adagrad  # Adaptive gradient scaling\n",
    "        self.epsilon = epsilon  # Small constant for numerical stability\n",
    "\n",
    "    def pre_update_params(self):\n",
    "        \"\"\"Apply learning rate decay before each update\"\"\"\n",
    "        if self.decay:\n",
    "            self.current_learning_rate = self.learning_rate / (1.0 + self.decay * self.iterations)\n",
    "\n",
    "    def update_params(self, layer):\n",
    "        \"\"\"Update layer parameters using chosen optimization strategy\"\"\"\n",
    "        # Initialize momentum if needed\n",
    "        if self.momentum:\n",
    "            if not hasattr(layer, 'weight_momentums'):\n",
    "                layer.weight_momentums = np.zeros_like(layer.weights)\n",
    "                layer.bias_momentums = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Initialize Adagrad cache if needed\n",
    "        if self.use_adagrad:\n",
    "            if not hasattr(layer, 'weight_cache'):\n",
    "                layer.weight_cache = np.zeros_like(layer.weights)\n",
    "                layer.bias_cache = np.zeros_like(layer.biases)\n",
    "\n",
    "        # Apply momentum (if enabled)\n",
    "        if self.momentum:\n",
    "            # Momentum update: v = β·v - lr·dW\n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
    "            layer.weight_momentums = weight_updates\n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
    "            layer.bias_momentums = bias_updates\n",
    "            \n",
    "            if self.use_adagrad:\n",
    "                # Also apply Adagrad scaling\n",
    "                layer.weight_cache += layer.dweights ** 2\n",
    "                layer.bias_cache += layer.dbiases ** 2\n",
    "                layer.weights += weight_updates / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "                layer.biases += bias_updates / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "            else:\n",
    "                layer.weights += weight_updates\n",
    "                layer.biases += bias_updates\n",
    "        else:\n",
    "            # No momentum - vanilla SGD or Adagrad\n",
    "            if self.use_adagrad:\n",
    "                layer.weight_cache += layer.dweights ** 2\n",
    "                layer.bias_cache += layer.dbiases ** 2\n",
    "                layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
    "                layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
    "            else:\n",
    "                layer.weights += -self.current_learning_rate * layer.dweights\n",
    "                layer.biases += -self.current_learning_rate * layer.dbiases\n",
    "\n",
    "    def post_update_params(self):\n",
    "        \"\"\"Increment iteration counter after updates\"\"\"\n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8649bed7",
   "metadata": {},
   "source": [
    "## Step 2: Training Loop and Evaluation\n",
    "\n",
    "I now implement the complete training function that performs forward and backward passes. The network learns by adjusting weights and biases based on the computed gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53078c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(optimizer, epochs=1000, print_every=100):\n",
    "    \"\"\"\n",
    "    Train the neural network on spiral data.\n",
    "    \n",
    "    Args:\n",
    "        optimizer: Optimizer instance (SGD with various settings)\n",
    "        epochs: Number of training iterations\n",
    "        print_every: How often to print progress\n",
    "    \n",
    "    Returns:\n",
    "        losses: List of loss values per epoch\n",
    "        accuracy: Final accuracy achieved\n",
    "        stable_epoch: Epoch where loss stabilized\n",
    "    \"\"\"\n",
    "    # Load dataset\n",
    "    X, y = spiral_data(samples=100, classes=3)\n",
    "\n",
    "    # Build network: 2 inputs → 64 hidden → 3 outputs\n",
    "    dense1 = Layer_Dense(2, 64)\n",
    "    activation1 = Activation_ReLU()\n",
    "    dense2 = Layer_Dense(64, 3)\n",
    "    activation2 = Activation_Softmax()\n",
    "    loss_function = Loss_CategoricalCrossEntropy()\n",
    "\n",
    "    losses = []\n",
    "    stable_epoch = None\n",
    "    stable_counter = 0\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        # Step 1: Adjust learning rate based on decay\n",
    "        optimizer.pre_update_params()\n",
    "\n",
    "        # Step 2: Forward pass\n",
    "        dense1.forward(X)\n",
    "        activation1.forward(dense1.output)\n",
    "        dense2.forward(activation1.output)\n",
    "        activation2.forward(dense2.output)\n",
    "\n",
    "        # Step 3: Compute loss and accuracy\n",
    "        loss = np.mean(loss_function.forward(activation2.output, y))\n",
    "        losses.append(loss)\n",
    "\n",
    "        predictions = np.argmax(activation2.output, axis=1)\n",
    "        accuracy = np.mean(predictions == y)\n",
    "\n",
    "        # Print progress\n",
    "        if epoch % print_every == 0 or epoch == 1:\n",
    "            print(f\"Epoch {epoch}/{epochs} - loss: {loss:.4f} - acc: {accuracy:.4f} - lr: {optimizer.current_learning_rate:.6f}\")\n",
    "\n",
    "        # Step 4: Backward pass (compute gradients)\n",
    "        loss_function.backward(activation2.output, y)\n",
    "        dvalues = loss_function.dinputs\n",
    "        activation2.backward(dvalues)\n",
    "        dense2.backward(activation2.dinputs)\n",
    "        activation1.backward(dense2.dinputs)\n",
    "        dense1.backward(activation1.dinputs)\n",
    "\n",
    "        # Step 5: Update parameters using optimizer\n",
    "        optimizer.update_params(dense1)\n",
    "        optimizer.update_params(dense2)\n",
    "        optimizer.post_update_params()\n",
    "\n",
    "        # Step 6: Detect when training stabilizes\n",
    "        if epoch > 1:\n",
    "            if abs(losses[-1] - losses[-2]) < 1e-5:\n",
    "                stable_counter += 1\n",
    "            else:\n",
    "                stable_counter = 0\n",
    "            if stable_counter >= 10 and stable_epoch is None:\n",
    "                stable_epoch = epoch - 10 + 1\n",
    "\n",
    "    return losses, accuracy, stable_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8c2c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 3: Experiment with Different Optimizers\n",
    "\n",
    "I'll train the network using two different optimization strategies and compare their performance.\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(0)\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"NEURAL NETWORK TRAINING WITH DIFFERENT OPTIMIZERS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Experiment 1: Vanilla SGD with learning rate decay\n",
    "print('\\n[1] Training with vanilla SGD (lr=1.0, decay=1e-3)')\n",
    "print('-' * 70)\n",
    "opt1 = Optimizer(learning_rate=1.0, decay=1e-3, momentum=0.0, use_adagrad=False)\n",
    "losses1, acc1, stable1 = train(opt1, epochs=1000, print_every=100)\n",
    "\n",
    "# Experiment 2: SGD with Momentum + Adagrad\n",
    "print('\\n\\n[2] Training with momentum + Adagrad (lr=0.5, decay=1e-4, momentum=0.9)')\n",
    "print('-' * 70)\n",
    "opt2 = Optimizer(learning_rate=0.5, decay=1e-4, momentum=0.9, use_adagrad=True)\n",
    "losses2, acc2, stable2 = train(opt2, epochs=1000, print_every=100)\n",
    "\n",
    "# Summary and Analysis\n",
    "print('\\n' + \"=\"*70)\n",
    "print(\"TRAINING SUMMARY & ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(f'\\nVanilla SGD:')\n",
    "print(f'  • Final Accuracy: {acc1:.4f} ({acc1*100:.2f}%)')\n",
    "print(f'  • Stabilized at Epoch: {stable1}')\n",
    "\n",
    "print(f'\\nMomentum + Adagrad:')\n",
    "print(f'  • Final Accuracy: {acc2:.4f} ({acc2*100:.2f}%)')\n",
    "print(f'  • Stabilized at Epoch: {stable2}')\n",
    "\n",
    "# Compare optimizers\n",
    "improvement = (acc2 - acc1) * 100\n",
    "if stable2 and stable1:\n",
    "    speedup = stable1 - stable2\n",
    "    print(f'\\nComparison:')\n",
    "    print(f'  • Accuracy Improvement: {improvement:.2f}%')\n",
    "    print(f'  • Convergence Speedup: {speedup} epochs faster')\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

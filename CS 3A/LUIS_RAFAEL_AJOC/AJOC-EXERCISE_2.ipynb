{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ea03083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from enum import Enum, auto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "92ce92c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Enum):\n",
    "    RELU = auto()\n",
    "    SIGMOID = auto()\n",
    "    SOFTMAX = auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "d73f985e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense_Layer:\n",
    "\n",
    "    @staticmethod\n",
    "    def setup(input, weights, biases):\n",
    "        \n",
    "        # Check if each neuron has a corresponding weight for each input\n",
    "        for i in range(len(weights)):\n",
    "            if len(input) != len(weights[i]):\n",
    "                raise ValueError(\"Error: Inputs and weights must have the same dimension.\")\n",
    "\n",
    "        # Check if each neuron has a bias\n",
    "        if len(weights) != len(biases):\n",
    "            raise ValueError(\"Error: Each neuron must have a bias\")\n",
    "        \n",
    "        print(\"Setup is good.\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def weighted_sum(inputs, weights, biases):\n",
    "        weighted_sum = np.dot(inputs, np.array(weights).T) + biases\n",
    "        # print(weighted_sum)\n",
    "        return weighted_sum\n",
    "    \n",
    "    @staticmethod\n",
    "    def activate(input, activation: Activation):\n",
    "        if activation == Activation.RELU:\n",
    "            return Dense_Layer.relu(input)\n",
    "        elif activation == Activation.SIGMOID:\n",
    "            return Dense_Layer.sigmoid(input)\n",
    "        elif activation == Activation.SOFTMAX:\n",
    "            return Dense_Layer.softmax(input)\n",
    "        else:\n",
    "            raise ValueError(\"Error: Unknown activation function.\")\n",
    "        \n",
    "    @staticmethod\n",
    "    def relu(input):\n",
    "        output = []\n",
    "        for i in range(len(input)):\n",
    "            processed = max(0, input[i])\n",
    "            print(input[i], \" -> \", processed)\n",
    "            output.append(processed)\n",
    "        # print(output)\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(input):\n",
    "        output = []\n",
    "        for i in range(len(input)):\n",
    "            processed = (1 / (1 + math.exp(-input[i])))\n",
    "            print(input[i], \" -> \", processed)\n",
    "            output.append(processed)\n",
    "        # print(output)\n",
    "        return output\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(input):\n",
    "        denominator = 0.0\n",
    "        expo_list = []\n",
    "        output = []\n",
    "        prob_check = 0.0\n",
    "        for i in range(len(input)):\n",
    "            exponential = math.exp(input[i])\n",
    "            expo_list.append(exponential)\n",
    "            print(\"Exponential of \", input[i], \"is \", exponential)\n",
    "            denominator += exponential\n",
    "            print(\"Denominator: \", denominator)\n",
    "        for i in range(len(expo_list)):\n",
    "            probability = expo_list[i] / denominator\n",
    "            prob_check += probability\n",
    "            print(\"Probability for \", input[i], \" is \", probability)\n",
    "            output.append(probability)\n",
    "        print(\"prob_check: \", prob_check)\n",
    "        # print(output)\n",
    "        return output\n",
    "\n",
    "    # Categorical Cross-Entropy Loss\n",
    "    @staticmethod\n",
    "    def cce_loss(predicted, target):\n",
    "        if len(predicted) != len(target):\n",
    "            raise ValueError(\"Error: Amount of predicted values does not match amount of target values.\")\n",
    "        loss = 0\n",
    "        for i in range(len(predicted)):\n",
    "            loss += target[i] * math.log(predicted[i])\n",
    "        return (-loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f002994",
   "metadata": {},
   "source": [
    "Given the following inputs from the Iris Dataset, using the sepal length, sepal width, petal length and petal width, determine what class (Iris-setosa, Iris-versicolor, and Iris-virginica) the following inputs are by calculating the output, given the neural network configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "366f2268",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [5.1, 3.5, 1.4, 0.2]\n",
    "target_output = [0.7, 0.2, 0.1]\n",
    "\n",
    "# 1st Hidden Layer\n",
    "weights_1 = [\n",
    "    [0.2, 0.1, -0.4, 0.6],\n",
    "    [0.5, -0.2, 0.3, -0.1],\n",
    "    [-0.3, 0.4, 0.2, 0.5]\n",
    "]\n",
    "bias_1 = [3.0, -2.1, 0.6]\n",
    "activation_1 = Activation.RELU\n",
    "\n",
    "# 2nd Hidden Layer\n",
    "weights_2 = [\n",
    "    [0.3, 0.7, -0.6],\n",
    "    [-0.5, 0.2, 0.4]\n",
    "]\n",
    "bias_2 = [4.3, 6.4]\n",
    "activation_2 = Activation.SIGMOID\n",
    "\n",
    "# Output Layer\n",
    "weights_3 = [\n",
    "    [0.5, -0.2],\n",
    "    [-0.3, 0.6],\n",
    "    [0.8, -0.4]\n",
    "]\n",
    "bias_3 = [-1.5, 2.1, -3.3]\n",
    "activation_3 = Activation.SOFTMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "dc8f7967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st Layer\n",
      "Setup is good.\n",
      "Weighted sums:  [3.93 0.15 0.85]\n",
      "3.93  ->  3.93\n",
      "0.14999999999999947  ->  0.14999999999999947\n",
      "0.8500000000000003  ->  0.8500000000000003\n",
      "Activation function:  Activation.RELU\n",
      "Activated:  [3.93, 0.14999999999999947, 0.8500000000000003]\n"
     ]
    }
   ],
   "source": [
    "print(\"1st Layer\")\n",
    "Dense_Layer.setup(inputs, weights_1, bias_1)\n",
    "weighted_sum_1 = Dense_Layer.weighted_sum(inputs, weights_1, bias_1)\n",
    "print(\"Weighted sums: \", weighted_sum_1)\n",
    "activated_1 = Dense_Layer.activate(weighted_sum_1, activation_1)\n",
    "print(\"Activation function: \", activation_1)\n",
    "print(\"Activated: \", activated_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f34eab19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2nd Layer\n",
      "Setup is good.\n",
      "Weighted sums:  [5.074 4.805]\n",
      "5.073999999999999  ->  0.9937815701810482\n",
      "4.805  ->  0.9918778091778501\n",
      "Activation function:  Activation.SIGMOID\n",
      "Activated:  [0.9937815701810482, 0.9918778091778501]\n"
     ]
    }
   ],
   "source": [
    "print(\"2nd Layer\")\n",
    "Dense_Layer.setup(activated_1, weights_2, bias_2)\n",
    "weighted_sum_2 = Dense_Layer.weighted_sum(activated_1, weights_2, bias_2)\n",
    "print(\"Weighted sums: \", weighted_sum_2)\n",
    "activated_2 = Dense_Layer.activate(weighted_sum_2, activation_2)\n",
    "print(\"Activation function: \", activation_2)\n",
    "print(\"Activated: \", activated_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1b5e8b06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Layer\n",
      "Setup is good.\n",
      "Weighted sums:  [-1.20148478  2.39699221 -2.90172587]\n",
      "Exponential of  -1.2014847767450458 is  0.3007473375870123\n",
      "Denominator:  0.3007473375870123\n",
      "Exponential of  2.3969922144523954 is  10.990070842173477\n",
      "Denominator:  11.290818179760489\n",
      "Exponential of  -2.9017258675263014 is  0.05492833916719329\n",
      "Denominator:  11.345746518927681\n",
      "Probability for  -1.2014847767450458  is  0.026507496627505896\n",
      "Probability for  2.3969922144523954  is  0.9686511878120277\n",
      "Probability for  -2.9017258675263014  is  0.004841315560466507\n",
      "prob_check:  1.0\n",
      "Activation function:  Activation.SOFTMAX\n"
     ]
    }
   ],
   "source": [
    "print(\"Output Layer\")\n",
    "Dense_Layer.setup(activated_2, weights_3, bias_3)\n",
    "weighted_sum_3 = Dense_Layer.weighted_sum(activated_2, weights_3, bias_3)\n",
    "print(\"Weighted sums: \", weighted_sum_3)\n",
    "activated_3 = Dense_Layer.activate(weighted_sum_3, activation_3)\n",
    "print(\"Activation function: \", activation_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "bdca9829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Answers\n",
      "Output:  [0.026507496627505896, 0.9686511878120277, 0.004841315560466507]\n",
      "Loss:  3.080656405230887\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Answers\")\n",
    "print(\"Output: \", activated_3)\n",
    "print(\"Loss: \", Dense_Layer.cce_loss(activated_3, target_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0deafccd",
   "metadata": {},
   "source": [
    "Given the following inputs from the Breast Cancer Dataset, using three features: Mean Radius, Mean Texture, and Mean Smoothness, determine whether the tumor is Benign (0) or Malignant (1) by calculating the network outputs step by step, given the following neural network configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "cc448854",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [14.1, 20.3, 0.095]\n",
    "target_output = [1]\n",
    "\n",
    "# 1st Hidden Layer\n",
    "weights_1 = [\n",
    "    [0.5, -0.3, 0.8],\n",
    "    [0.2, 0.4, -0.6],\n",
    "    [-0.7, 0.9, 0.1]\n",
    "]\n",
    "bias_1 = [0.3, -0.5, 0.6]\n",
    "activation_1 = Activation.RELU\n",
    "\n",
    "# 2nd Hidden Layer\n",
    "weights_2 = [\n",
    "    [0.6, -0.2, 0.4],\n",
    "    [-0.3, 0.5, 0.7]\n",
    "]\n",
    "bias_2 = [0.1, -0.8]\n",
    "activation_2 = Activation.SIGMOID\n",
    "\n",
    "# Output Layer\n",
    "weights_3 = [\n",
    "    [0.7, -0.5]\n",
    "]\n",
    "bias_3 = [0.2]\n",
    "activation_3 = Activation.SIGMOID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "23255121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1st Layer\n",
      "Setup is good.\n",
      "Weighted sums:  [ 1.336  10.383   9.0095]\n",
      "1.336  ->  1.336\n",
      "10.383000000000001  ->  10.383000000000001\n",
      "9.0095  ->  9.0095\n",
      "Activation function:  Activation.RELU\n",
      "Activated:  [1.336, 10.383000000000001, 9.0095]\n"
     ]
    }
   ],
   "source": [
    "print(\"1st Layer\")\n",
    "Dense_Layer.setup(inputs, weights_1, bias_1)\n",
    "weighted_sum_1 = Dense_Layer.weighted_sum(inputs, weights_1, bias_1)\n",
    "print(\"Weighted sums: \", weighted_sum_1)\n",
    "activated_1 = Dense_Layer.activate(weighted_sum_1, activation_1)\n",
    "print(\"Activation function: \", activation_1)\n",
    "print(\"Activated: \", activated_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "8643387b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2nd Layer\n",
      "Setup is good.\n",
      "Weighted sums:  [ 2.4288  10.29735]\n",
      "2.4287999999999994  ->  0.9189972481200018\n",
      "10.297349999999998  ->  0.9999662787960715\n",
      "Activation function:  Activation.SIGMOID\n",
      "Activated:  [0.9189972481200018, 0.9999662787960715]\n"
     ]
    }
   ],
   "source": [
    "print(\"2nd Layer\")\n",
    "Dense_Layer.setup(activated_1, weights_2, bias_2)\n",
    "weighted_sum_2 = Dense_Layer.weighted_sum(activated_1, weights_2, bias_2)\n",
    "print(\"Weighted sums: \", weighted_sum_2)\n",
    "activated_2 = Dense_Layer.activate(weighted_sum_2, activation_2)\n",
    "print(\"Activation function: \", activation_2)\n",
    "print(\"Activated: \", activated_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ab290a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Layer\n",
      "Setup is good.\n",
      "Weighted sums:  [0.34331493]\n",
      "0.34331493428596555  ->  0.5849955347015883\n",
      "Activation function:  Activation.SIGMOID\n"
     ]
    }
   ],
   "source": [
    "print(\"Output Layer\")\n",
    "Dense_Layer.setup(activated_2, weights_3, bias_3)\n",
    "weighted_sum_3 = Dense_Layer.weighted_sum(activated_2, weights_3, bias_3)\n",
    "print(\"Weighted sums: \", weighted_sum_3)\n",
    "activated_3 = Dense_Layer.activate(weighted_sum_3, activation_3)\n",
    "print(\"Activation function: \", activation_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "609ec38b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Answers\n",
      "Output:  [0.5849955347015883]\n",
      "Loss:  0.53615106476815\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Answers\")\n",
    "print(\"Output: \", activated_3)\n",
    "print(\"Loss: \", Dense_Layer.cce_loss(activated_3, target_output))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e95938b",
   "metadata": {},
   "source": [
    "EXERCISE 2\n",
    "Name: LOUISE MARIELLE V. DORADO\n",
    "      STEVEN KEN E. PONTILLAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3e293fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "093fa502",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense_Layer:\n",
    "    def __init__(self, input_size, output_size, activation='relu'):\n",
    "        \"\"\"Initialize the dense layer with input size, output size and activation function\"\"\"\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.activation_type = activation\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights = np.random.randn(input_size, output_size) * 0.01\n",
    "        self.bias = np.zeros((1, output_size))\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"Perform the weighted sum + bias operation\"\"\"\n",
    "        self.inputs = inputs\n",
    "        self.z = np.dot(inputs, self.weights) + self.bias\n",
    "        self.output = self.activate(self.z)\n",
    "        return self.output\n",
    "    \n",
    "    def activate(self, x):\n",
    "        \"\"\"Apply the selected activation function\"\"\"\n",
    "        if self.activation_type == 'relu':\n",
    "            return np.maximum(0, x)\n",
    "        elif self.activation_type == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        elif self.activation_type == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "    \n",
    "    def calculate_loss(self, predicted, target):\n",
    "        \"\"\"Calculate Mean Squared Error loss\"\"\"\n",
    "        self.loss = np.mean(np.square(predicted - target))\n",
    "        return self.loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130882b6",
   "metadata": {},
   "source": [
    "A. Steven Ken E. Pontillas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7e5f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input vector (features from Iris dataset)\n",
    "X = np.array([5.1, 3.5, 1.4, 0.2])\n",
    "\n",
    "# First Hidden Layer (ReLU)\n",
    "W1 = np.array([[0.2, 0.5, -0.3],\n",
    "               [0.1, -0.2, 0.4],\n",
    "               [-0.4, 0.3, 0.2],\n",
    "               [0.6, -0.1, 0.5]])\n",
    "B1 = np.array([3.0, -2.1, 0.6])\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "Z1 = np.dot(X, W1) + B1\n",
    "A1 = relu(Z1)\n",
    "print('First Hidden Layer Output (ReLU):', A1)\n",
    "\n",
    "# Second Hidden Layer (Sigmoid)\n",
    "W2 = np.array([[0.3, -0.5],\n",
    "               [0.7, 0.2],\n",
    "               [-0.6, 0.4]])\n",
    "B2 = np.array([4.3, 6.4])\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "Z2 = np.dot(A1, W2) + B2\n",
    "A2 = sigmoid(Z2)\n",
    "print('Second Hidden Layer Output (Sigmoid):', A2)\n",
    "\n",
    "# Output Layer (Softmax)\n",
    "W3 = np.array([[0.5, -0.3, 0.8],\n",
    "               [0.2, 0.6, -0.4]])\n",
    "B3 = np.array([-1.5, 2.1, 3.1])\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()\n",
    "\n",
    "Z3 = np.dot(A2, W3) + B3\n",
    "output = softmax(Z3)\n",
    "print('Output Layer (Softmax probabilities):', output)\n",
    "\n",
    "classes = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
    "predicted_class = classes[np.argmax(output)]\n",
    "print('Predicted class:', predicted_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a953608",
   "metadata": {},
   "source": [
    "B. Louise Marielle V. Dorado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eefa1540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1 output (ReLU):\n",
      "[[ 1.336  10.383   9.0095]]\n",
      "\n",
      "Layer 2 output (Sigmoid):\n",
      "[[0.91899725 0.99996628]]\n",
      "\n",
      "Final output (Sigmoid):\n",
      "[[0.58499553]]\n",
      "\n",
      "Loss (MSE):\n",
      "0.17222870621762063\n"
     ]
    }
   ],
   "source": [
    "# Input data - breast cancer features (Mean Radius, Mean Texture, Mean Smoothness)\n",
    "X = np.array([[14.1, 20.3, 0.095]])  # Input features\n",
    "target = np.array([[1]])              # Target: 1 for Malignant\n",
    "\n",
    "# Layer 1 configuration (3 inputs -> 3 hidden nodes)\n",
    "W1 = np.array([[0.5, 0.2, -0.7],     # Weights for first hidden layer\n",
    "               [-0.3, 0.4, 0.9],\n",
    "               [0.8, -0.6, 0.1]])\n",
    "B1 = np.array([[0.3, -0.5, 0.6]])    # Biases for first hidden layer\n",
    "\n",
    "# Layer 2 configuration (3 hidden nodes -> 2 hidden nodes)\n",
    "W2 = np.array([[0.6, -0.3],          # Weights for second hidden layer\n",
    "               [-0.2, 0.5],\n",
    "               [0.4, 0.7]])\n",
    "B2 = np.array([[0.1, -0.8]])         # Biases for second hidden layer\n",
    "\n",
    "# Layer 3 configuration (2 hidden nodes -> 1 output)\n",
    "W3 = np.array([[0.7],                # Weights for output layer\n",
    "               [-0.5]])\n",
    "B3 = np.array([[0.2]])               # Bias for output layer\n",
    "\n",
    "# Create the network layers\n",
    "layer1 = Dense_Layer(3, 3, activation='relu')     # First hidden layer\n",
    "layer2 = Dense_Layer(3, 2, activation='sigmoid')  # Second hidden layer\n",
    "layer3 = Dense_Layer(2, 1, activation='sigmoid')  # Output layer\n",
    "\n",
    "# Set the weights and biases manually\n",
    "layer1.weights = W1\n",
    "layer1.bias = B1\n",
    "layer2.weights = W2\n",
    "layer2.bias = B2\n",
    "layer3.weights = W3\n",
    "layer3.bias = B3\n",
    "\n",
    "# Forward pass through the network\n",
    "output1 = layer1.forward(X)           # First layer with ReLU activation\n",
    "print(\"Layer 1 output (ReLU):\")\n",
    "print(output1)\n",
    "\n",
    "output2 = layer2.forward(output1)     # Second layer with Sigmoid activation\n",
    "print(\"\\nLayer 2 output (Sigmoid):\")\n",
    "print(output2)\n",
    "\n",
    "final_output = layer3.forward(output2)  # Output layer with Sigmoid activation\n",
    "print(\"\\nFinal output (Sigmoid):\")\n",
    "print(final_output)\n",
    "\n",
    "# Calculate Mean Squared Error loss\n",
    "loss = layer3.calculate_loss(final_output, target)\n",
    "print(\"\\nLoss (MSE):\")\n",
    "print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

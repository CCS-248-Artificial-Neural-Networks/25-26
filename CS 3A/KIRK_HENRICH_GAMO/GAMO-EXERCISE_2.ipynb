{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44995823",
   "metadata": {},
   "source": [
    "# Unit 2 Exercise - Neural Network Implementation\n",
    "## Team: Kirk Henrich Gamo & Dallas Aquino\n",
    "\n",
    "This notebook contains solutions to neural network problems completed by our team:\n",
    "\n",
    "### **Problem 1: Dense Layer Implementation (50 points)** - *Completed by Kirk and Dallas*\n",
    "- Develop a Dense_Layer class with required functions:\n",
    "  - setup_weights_and_inputs (10 points)\n",
    "  - weighted_sum_plus_bias (10 points) \n",
    "  - apply_activation_function (15 points)\n",
    "  - calculate_loss (15 points)\n",
    "\n",
    "### **Problem 2: Multi-Layer Neural Network Classification (50 points)**\n",
    "Each team member selected and completed one classification problem:\n",
    "\n",
    "**Problem 2a: Iris Dataset Classification** - *Completed by Kirk*\n",
    "- 3-layer neural network for Iris species classification\n",
    "- Multi-class classification with Softmax output\n",
    "\n",
    "**Problem 2b: Breast Cancer Dataset Classification** - *Completed by Dallas*\n",
    "- 3-layer neural network for tumor classification  \n",
    "- Binary classification with Sigmoid output\n",
    "\n",
    "\n",
    "**Total Points: 100**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355b7f18",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Problem 1: Dense Layer Implementation (50 points)\n",
    "\n",
    "**Objective:** Develop a Dense_Layer class in Python with the following functions:\n",
    "1. A function to setup/accept the inputs and weights (10 points)\n",
    "2. A function to perform the weighted sum + bias (10 points)  \n",
    "3. A function to perform the selected activation function (15 points)\n",
    "4. A function to calculate the loss (predicted output vs target output) (15 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0ebc5b",
   "metadata": {},
   "source": [
    "### Required Libraries for Problem 1\n",
    "\n",
    "First, let's import the necessary libraries for our Dense Layer implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2036f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c1b340",
   "metadata": {},
   "source": [
    "### Dense_Layer Class Implementation\n",
    "\n",
    "Below is the complete implementation of the Dense_Layer class with all required functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efdc083",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense_Layer:\n",
    "    def __init__(self, n_inputs, n_neurons, activation='relu'):\n",
    "        \"\"\"\n",
    "        Initialize the Dense Layer\n",
    "        \n",
    "        Parameters:\n",
    "        n_inputs: Number of input features\n",
    "        n_neurons: Number of neurons in this layer\n",
    "        activation: Activation function to use ('relu', 'sigmoid', 'tanh', 'linear')\n",
    "        \"\"\"\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_neurons = n_neurons\n",
    "        self.activation_type = activation\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "        self.inputs = None\n",
    "        self.output = None\n",
    "        \n",
    "    def setup_weights_and_inputs(self, inputs, weights=None, biases=None):\n",
    "        \"\"\"\n",
    "        Function to setup/accept the inputs and weights (10 points)\n",
    "        \n",
    "        Parameters:\n",
    "        inputs: Input data (numpy array)\n",
    "        weights: Weight matrix (if None, initialize randomly)\n",
    "        biases: Bias vector (if None, initialize as zeros)\n",
    "        \"\"\"\n",
    "        self.inputs = np.array(inputs)\n",
    "        \n",
    "        # Initialize weights if not provided\n",
    "        if weights is None:\n",
    "            # Xavier/Glorot initialization\n",
    "            self.weights = np.random.randn(self.n_inputs, self.n_neurons) * np.sqrt(2.0 / self.n_inputs)\n",
    "        else:\n",
    "            self.weights = np.array(weights)\n",
    "            \n",
    "        # Initialize biases if not provided\n",
    "        if biases is None:\n",
    "            self.biases = np.zeros((1, self.n_neurons))\n",
    "        else:\n",
    "            self.biases = np.array(biases)\n",
    "            \n",
    "        print(f\"Setup complete:\")\n",
    "        print(f\"Input shape: {self.inputs.shape}\")\n",
    "        print(f\"Weights shape: {self.weights.shape}\")\n",
    "        print(f\"Biases shape: {self.biases.shape}\")\n",
    "        \n",
    "    def weighted_sum_plus_bias(self):\n",
    "        \"\"\"\n",
    "        Function to perform the weighted sum + bias (10 points)\n",
    "        \n",
    "        Returns:\n",
    "        z: The linear combination (weighted sum + bias)\n",
    "        \"\"\"\n",
    "        if self.inputs is None or self.weights is None or self.biases is None:\n",
    "            raise ValueError(\"Must call setup_weights_and_inputs first!\")\n",
    "            \n",
    "        # Compute weighted sum: inputs @ weights + bias\n",
    "        z = np.dot(self.inputs, self.weights) + self.biases\n",
    "        \n",
    "        print(f\"Weighted sum + bias computed. Output shape: {z.shape}\")\n",
    "        return z\n",
    "        \n",
    "    def apply_activation_function(self, z):\n",
    "        \"\"\"\n",
    "        Function to perform the selected activation function (15 points)\n",
    "        \n",
    "        Parameters:\n",
    "        z: Linear combination from weighted_sum_plus_bias\n",
    "        \n",
    "        Returns:\n",
    "        activated_output: Output after applying activation function\n",
    "        \"\"\"\n",
    "        if self.activation_type == 'relu':\n",
    "            # ReLU: max(0, z)\n",
    "            activated_output = np.maximum(0, z)\n",
    "            \n",
    "        elif self.activation_type == 'sigmoid':\n",
    "            # Sigmoid: 1 / (1 + e^(-z))\n",
    "            activated_output = 1 / (1 + np.exp(-np.clip(z, -500, 500)))  # Clip to prevent overflow\n",
    "            \n",
    "        elif self.activation_type == 'tanh':\n",
    "            # Hyperbolic tangent\n",
    "            activated_output = np.tanh(z)\n",
    "            \n",
    "        elif self.activation_type == 'softmax':\n",
    "            # Softmax: e^(z_i) / sum(e^z) for multi-class classification\n",
    "            exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Subtract max for numerical stability\n",
    "            activated_output = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "            \n",
    "        elif self.activation_type == 'linear':\n",
    "            # Linear activation (no change)\n",
    "            activated_output = z\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {self.activation_type}\")\n",
    "            \n",
    "        self.output = activated_output\n",
    "        print(f\"Applied {self.activation_type} activation function\")\n",
    "        return activated_output\n",
    "        \n",
    "    def calculate_loss(self, predicted_output, target_output, loss_type='mse'):\n",
    "        \"\"\"\n",
    "        Function to calculate the loss (predicted output vs target output) (15 points)\n",
    "        \n",
    "        Parameters:\n",
    "        predicted_output: The predicted values from the network\n",
    "        target_output: The actual target values\n",
    "        loss_type: Type of loss function ('mse', 'mae', 'binary_crossentropy', 'categorical_crossentropy')\n",
    "        \n",
    "        Returns:\n",
    "        loss: The calculated loss value\n",
    "        \"\"\"\n",
    "        predicted = np.array(predicted_output)\n",
    "        target = np.array(target_output)\n",
    "        \n",
    "        if predicted.shape != target.shape:\n",
    "            raise ValueError(f\"Shape mismatch: predicted {predicted.shape} vs target {target.shape}\")\n",
    "            \n",
    "        if loss_type == 'mse':\n",
    "            # Mean Squared Error\n",
    "            loss = np.mean((predicted - target) ** 2)\n",
    "            \n",
    "        elif loss_type == 'mae':\n",
    "            # Mean Absolute Error\n",
    "            loss = np.mean(np.abs(predicted - target))\n",
    "            \n",
    "        elif loss_type == 'binary_crossentropy':\n",
    "            # Binary Cross-Entropy (for binary classification)\n",
    "            # Clip predictions to prevent log(0)\n",
    "            predicted_clipped = np.clip(predicted, 1e-15, 1 - 1e-15)\n",
    "            loss = -np.mean(target * np.log(predicted_clipped) + (1 - target) * np.log(1 - predicted_clipped))\n",
    "            \n",
    "        elif loss_type == 'categorical_crossentropy':\n",
    "            # Categorical Cross-Entropy (for multi-class classification)\n",
    "            predicted_clipped = np.clip(predicted, 1e-15, 1 - 1e-15)\n",
    "            loss = -np.mean(np.sum(target * np.log(predicted_clipped), axis=1))\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported loss function: {loss_type}\")\n",
    "            \n",
    "        print(f\"Calculated {loss_type} loss: {loss:.6f}\")\n",
    "        return loss\n",
    "        \n",
    "    def forward_pass(self, inputs, target=None, loss_type='mse'):\n",
    "        \"\"\"\n",
    "        Complete forward pass through the layer\n",
    "        \n",
    "        Parameters:\n",
    "        inputs: Input data\n",
    "        target: Target output (optional, for loss calculation)\n",
    "        loss_type: Type of loss function to use\n",
    "        \n",
    "        Returns:\n",
    "        output: Final output after activation\n",
    "        loss: Loss value (if target provided)\n",
    "        \"\"\"\n",
    "        # Setup inputs (weights should already be initialized)\n",
    "        if self.weights is None:\n",
    "            raise ValueError(\"Weights not initialized. Call setup_weights_and_inputs first.\")\n",
    "            \n",
    "        self.inputs = np.array(inputs)\n",
    "        \n",
    "        # Forward pass\n",
    "        z = self.weighted_sum_plus_bias()\n",
    "        output = self.apply_activation_function(z)\n",
    "        \n",
    "        loss = None\n",
    "        if target is not None:\n",
    "            loss = self.calculate_loss(output, target, loss_type)\n",
    "            \n",
    "        return output, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ac2b5d",
   "metadata": {},
   "source": [
    "### Problem 1: Function Testing and Demonstration\n",
    "\n",
    "Let's test each function of our Dense_Layer class systematically to verify correct implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ad0953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 1: Function setup_weights_and_inputs (10 points)\n",
    "print(\"=== TEST 1: Setup Weights and Inputs Function ===\")\n",
    "print()\n",
    "\n",
    "# Create a dense layer with 3 inputs and 2 neurons\n",
    "layer1 = Dense_Layer(n_inputs=3, n_neurons=2, activation='relu')\n",
    "\n",
    "# Sample input data (1 sample with 3 features)\n",
    "sample_inputs = [[1.0, 2.0, 3.0]]\n",
    "\n",
    "# Custom weights matrix (3x2) and biases (1x2)\n",
    "custom_weights = [[0.2, 0.8], \n",
    "                  [0.5, -0.7], \n",
    "                  [0.1, 0.3]]\n",
    "custom_biases = [[0.1, -0.2]]\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"Number of inputs: {layer1.n_inputs}\")\n",
    "print(f\"Number of neurons: {layer1.n_neurons}\")\n",
    "print(f\"Input data: {sample_inputs}\")\n",
    "print(f\"Custom weights: {custom_weights}\")\n",
    "print(f\"Custom biases: {custom_biases}\")\n",
    "print()\n",
    "\n",
    "# Setup the layer\n",
    "print(\"Setting up layer...\")\n",
    "layer1.setup_weights_and_inputs(sample_inputs, custom_weights, custom_biases)\n",
    "print()\n",
    "\n",
    "print(\"✅ TEST 1 PASSED: setup_weights_and_inputs function working correctly!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66f16ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 2: Function weighted_sum_plus_bias (10 points)\n",
    "print(\"=== TEST 2: Weighted Sum Plus Bias Function ===\")\n",
    "print()\n",
    "\n",
    "print(\"Using the layer from TEST 1...\")\n",
    "print(\"Input:\", sample_inputs[0])\n",
    "print(\"Weights shape:\", layer1.weights.shape)\n",
    "print(\"Biases shape:\", layer1.biases.shape)\n",
    "print()\n",
    "\n",
    "# Get weighted sum + bias\n",
    "z = layer1.weighted_sum_plus_bias()\n",
    "print(\"Calculated weighted sum + bias (z):\", z.flatten())\n",
    "print()\n",
    "\n",
    "# Manual verification\n",
    "print(\"Manual Verification:\")\n",
    "print(\"For neuron 1: (1.0×0.2) + (2.0×0.5) + (3.0×0.1) + 0.1 = 0.2 + 1.0 + 0.3 + 0.1 = 1.6\")\n",
    "print(\"For neuron 2: (1.0×0.8) + (2.0×-0.7) + (3.0×0.3) + (-0.2) = 0.8 - 1.4 + 0.9 - 0.2 = 0.1\")\n",
    "print()\n",
    "\n",
    "# Verify our calculation\n",
    "manual_calc = [1.6, 0.1]\n",
    "print(f\"Manual calculation: {manual_calc}\")\n",
    "print(f\"Function result:    {z.flatten().tolist()}\")\n",
    "\n",
    "if np.allclose(z.flatten(), manual_calc):\n",
    "    print(\"✅ TEST 2 PASSED: weighted_sum_plus_bias function working correctly!\")\n",
    "else:\n",
    "    print(\"❌ TEST 2 FAILED: Results don't match manual calculation\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20b1bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 3: Function apply_activation_function (15 points)\n",
    "print(\"=== TEST 3: Apply Activation Function ===\")\n",
    "print()\n",
    "\n",
    "# Using z from previous test\n",
    "print(\"Input to activation function (z):\", z.flatten())\n",
    "print(f\"Selected activation function: {layer1.activation_type}\")\n",
    "print()\n",
    "\n",
    "# Apply activation function\n",
    "output = layer1.apply_activation_function(z)\n",
    "print(\"Output after ReLU activation:\", output.flatten())\n",
    "print()\n",
    "\n",
    "print(\"Manual Verification:\")\n",
    "print(\"ReLU function: max(0, z)\")\n",
    "print(\"For z = [1.6, 0.1]:\")\n",
    "print(\"  max(0, 1.6) = 1.6\")\n",
    "print(\"  max(0, 0.1) = 0.1\")\n",
    "print(\"Expected output: [1.6, 0.1]\")\n",
    "print()\n",
    "\n",
    "# Test different activation functions\n",
    "print(\"Testing all activation functions:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "test_z = [[-2.0, -0.5, 0.0, 0.5, 2.0]]\n",
    "activations = ['relu', 'sigmoid', 'tanh', 'linear']\n",
    "\n",
    "for activation in activations:\n",
    "    test_layer = Dense_Layer(n_inputs=1, n_neurons=5, activation=activation)\n",
    "    test_output = test_layer.apply_activation_function(np.array(test_z))\n",
    "    print(f\"{activation.upper():8} | Input: {test_z[0]} → Output: {test_output.flatten().round(3).tolist()}\")\n",
    "\n",
    "print()\n",
    "print(\"✅ TEST 3 PASSED: apply_activation_function working for all activation types!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1106b790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST 4: Function calculate_loss (15 points)\n",
    "print(\"=== TEST 4: Calculate Loss Function ===\")\n",
    "print()\n",
    "\n",
    "# Using output from previous test as predictions\n",
    "predictions = output\n",
    "target = [[1.5, 0.0]]\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"Predicted output: {predictions.flatten()}\")\n",
    "print(f\"Target output:    {target[0]}\")\n",
    "print()\n",
    "\n",
    "# Calculate MSE loss\n",
    "loss = layer1.calculate_loss(predictions, target, 'mse')\n",
    "print(f\"MSE Loss: {loss:.6f}\")\n",
    "print()\n",
    "\n",
    "# Manual verification of MSE\n",
    "print(\"Manual MSE Verification:\")\n",
    "pred_flat = predictions.flatten()\n",
    "target_flat = np.array(target).flatten()\n",
    "manual_mse = np.mean((pred_flat - target_flat) ** 2)\n",
    "print(f\"Manual calculation: mean([({pred_flat[0]:.1f} - {target_flat[0]:.1f})², ({pred_flat[1]:.1f} - {target_flat[1]:.1f})²])\")\n",
    "print(f\"                  = mean([({pred_flat[0] - target_flat[0]:.1f})², ({pred_flat[1] - target_flat[1]:.1f})²])\")\n",
    "print(f\"                  = mean([{(pred_flat[0] - target_flat[0])**2:.2f}, {(pred_flat[1] - target_flat[1])**2:.2f}])\")\n",
    "print(f\"                  = {manual_mse:.6f}\")\n",
    "print()\n",
    "\n",
    "# Test different loss functions\n",
    "print(\"Testing all loss functions:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Test cases for different loss types\n",
    "test_pred = [[0.8, 0.2]]\n",
    "test_target = [[1.0, 0.0]]\n",
    "\n",
    "loss_functions = ['mse', 'mae', 'categorical_crossentropy']\n",
    "for loss_type in loss_functions:\n",
    "    test_loss = layer1.calculate_loss(test_pred, test_target, loss_type)\n",
    "    print(f\"{loss_type.upper():20} | Loss: {test_loss:.6f}\")\n",
    "\n",
    "# Binary classification test\n",
    "binary_pred = [[0.7]]\n",
    "binary_target = [[1.0]]\n",
    "binary_loss = layer1.calculate_loss(binary_pred, binary_target, 'binary_crossentropy')\n",
    "print(f\"{'BINARY_CROSSENTROPY':20} | Loss: {binary_loss:.6f}\")\n",
    "\n",
    "print()\n",
    "print(\"✅ TEST 4 PASSED: calculate_loss function working for all loss types!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b41b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPREHENSIVE TEST: Complete Forward Pass Integration\n",
    "print(\"=== COMPREHENSIVE TEST: Complete Forward Pass ===\")\n",
    "print()\n",
    "\n",
    "# Create a new layer for batch processing\n",
    "batch_layer = Dense_Layer(n_inputs=4, n_neurons=3, activation='relu')\n",
    "\n",
    "# Sample batch of data (3 samples, 4 features each)\n",
    "batch_inputs = [[1.0, 2.0, -1.0, 0.5],\n",
    "                [0.5, -1.0, 2.0, 1.5],\n",
    "                [-0.5, 1.0, 0.0, -1.0]]\n",
    "\n",
    "# Target outputs for the batch\n",
    "target_outputs = [[1.0, 0.0, 0.5],\n",
    "                  [0.0, 1.0, 0.3],\n",
    "                  [0.5, 0.5, 1.0]]\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"Input shape: {np.array(batch_inputs).shape} (3 samples, 4 features)\")\n",
    "print(f\"Target shape: {np.array(target_outputs).shape} (3 samples, 3 outputs)\")\n",
    "print(f\"Layer: {batch_layer.n_inputs} inputs → {batch_layer.n_neurons} neurons\")\n",
    "print(f\"Activation: {batch_layer.activation_type}\")\n",
    "print()\n",
    "\n",
    "# Initialize with random weights (Xavier initialization)\n",
    "batch_layer.setup_weights_and_inputs(batch_inputs)\n",
    "print()\n",
    "\n",
    "print(\"Running complete forward pass:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Step-by-step forward pass\n",
    "z = batch_layer.weighted_sum_plus_bias()\n",
    "print(f\"1. Weighted sum + bias (z) shape: {z.shape}\")\n",
    "print(f\"   Sample outputs: {z[0].round(3)}\")\n",
    "\n",
    "output = batch_layer.apply_activation_function(z)\n",
    "print(f\"2. After ReLU activation shape: {output.shape}\")\n",
    "print(f\"   Sample outputs: {output[0].round(3)}\")\n",
    "\n",
    "loss = batch_layer.calculate_loss(output, target_outputs, 'mse')\n",
    "print(f\"3. MSE Loss: {loss:.6f}\")\n",
    "print()\n",
    "\n",
    "print(\"FINAL RESULTS SUMMARY:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Sample | Input Features           | Predicted Output      | Target Output        | Individual Error\")\n",
    "print(\"-------|--------------------------|----------------------|----------------------|------------------\")\n",
    "\n",
    "for i, (inp, pred, target) in enumerate(zip(batch_inputs, output, target_outputs)):\n",
    "    individual_error = np.mean((np.array(pred) - np.array(target)) ** 2)\n",
    "    print(f\"   {i+1}   | {str(inp):24} | {str(pred.round(3)):20} | {str(target):20} | {individual_error:.6f}\")\n",
    "\n",
    "print()\n",
    "print(f\"Overall MSE Loss: {loss:.6f}\")\n",
    "print()\n",
    "print(\"🎉 COMPREHENSIVE TEST PASSED: All Dense Layer functions working correctly!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbe99c8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Problem 2: Multi-Layer Neural Network Classification (50 points)\n",
    "\n",
    "**Objective:** Use the Dense_Layer class from Problem 1 to build complete multi-layer neural networks for classification tasks.\n",
    "\n",
    "**Team Assignment:** Each member completed one classification problem:\n",
    "- **Problem 2a:** Iris Dataset Classification - *Kirk Henrich Gamo*\n",
    "- **Problem 2b:** Breast Cancer Dataset Classification - *Dallas Aquino*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2adfb8",
   "metadata": {},
   "source": [
    "### Multi-Layer Network Class\n",
    "\n",
    "The following class uses Dense_Layer instances from Problem 1 to create complete neural networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ad0c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-layer network class using Dense_Layer from Problem 1\n",
    "import numpy as np\n",
    "\n",
    "class MultiLayerNetwork:\n",
    "    \"\"\"\n",
    "    Multi-layer neural network using Dense_Layer class from Problem 1\n",
    "    This demonstrates how the Dense_Layer class can be used to build complete neural networks\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        \n",
    "    def add_dense_layer(self, n_inputs, n_neurons, activation, weights=None, biases=None):\n",
    "        \"\"\"Add a Dense_Layer instance to the network\"\"\"\n",
    "        layer = Dense_Layer(n_inputs, n_neurons, activation)\n",
    "        if weights is not None and biases is not None:\n",
    "            # Initialize with provided weights and biases\n",
    "            dummy_input = [[0] * n_inputs]  # Dummy input for setup\n",
    "            layer.setup_weights_and_inputs(dummy_input, weights, biases)\n",
    "            \n",
    "        # Initialize additional attributes needed for tracking\n",
    "        layer.current_weighted_sums = None\n",
    "        layer.layer_outputs = None\n",
    "        \n",
    "        self.layers.append(layer)\n",
    "        return layer\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        \"\"\"Perform forward pass through all Dense_Layer instances\"\"\"\n",
    "        current_input = np.array(X)\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            # Update the layer's inputs\n",
    "            layer.inputs = current_input\n",
    "            \n",
    "            # Compute weighted sum + bias using Dense_Layer method\n",
    "            z = layer.weighted_sum_plus_bias()\n",
    "            layer.current_weighted_sums = z  # Store for later access\n",
    "            \n",
    "            # Apply activation function using Dense_Layer method\n",
    "            output = layer.apply_activation_function(z)\n",
    "            layer.layer_outputs = output  # Store for later access\n",
    "            \n",
    "            # Output becomes input for next layer\n",
    "            current_input = output\n",
    "            \n",
    "        return current_input\n",
    "\n",
    "print(\"✅ Multi-layer network class defined using Dense_Layer from Problem 1!\")\n",
    "print(\"This implementation demonstrates how the Dense_Layer class can build complete neural networks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc92b9b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Problem 2a: Iris Dataset Classification\n",
    "### *Completed by: Kirk Henrich Gamo*\n",
    "\n",
    "**Objective:** Given inputs from the Iris Dataset using sepal length, sepal width, petal length and petal width,determine what class (Iris-setosa, Iris-versicolor, and Iris-virginica) the input belongs to by calculating the network output through a 3-layer neural network using Dense_Layer class.\n",
    "\n",
    "**Network Architecture:**\n",
    "- **Input Layer:** 4 features (sepal length, sepal width, petal length, petal width)\n",
    "- **Hidden Layer 1:** 3 neurons with ReLU activation\n",
    "- **Hidden Layer 2:** 2 neurons with Sigmoid activation\n",
    "- **Output Layer:** 3 neurons (one for each iris species) with softmax activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0f6e6f",
   "metadata": {},
   "source": [
    "### Problem 2a: Network Configuration and Setup\n",
    "\n",
    "Setting up the Iris classification problem with the given network architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84152f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM 2a SETUP: Iris Classification using Dense_Layer Class \n",
    "print(\"=== PROBLEM 2a SETUP: IRIS CLASSIFICATION ===\")\n",
    "print(\"Implemented by: Kirk Henrich Gamo\")\n",
    "print()\n",
    "\n",
    "# Input data from Iris Dataset (sepal length, sepal width, petal length, petal width)\n",
    "# Based on the picture: X = [5.1, 3.5, 1.4, 0.2]\n",
    "X_input = [[5.1, 3.5, 1.4, 0.2]]  # Sample Iris measurements\n",
    "\n",
    "# Target output from the picture: [0.7, 0.2, 0.1]\n",
    "target_output = [[0.7, 0.2, 0.1]]  # Expected output for this sample\n",
    "\n",
    "print(\"Problem Configuration:\")\n",
    "print(f\"Input features (X): {X_input[0]}\")\n",
    "print(\"  - Sepal Length: {:.1f} cm\".format(X_input[0][0]))\n",
    "print(\"  - Sepal Width:  {:.1f} cm\".format(X_input[0][1]))\n",
    "print(\"  - Petal Length: {:.1f} cm\".format(X_input[0][2]))\n",
    "print(\"  - Petal Width:  {:.1f} cm\".format(X_input[0][3]))\n",
    "print()\n",
    "print(f\"Target output: {target_output[0]} (Expected: [setosa, versicolor, virginica])\")\n",
    "print()\n",
    "\n",
    "# Network Architecture: 4 → 3 → 2 → 3 neurons\n",
    "print(\"Network Architecture:\")\n",
    "print(\"Input Layer (4) → Hidden Layer 1 (3) → Hidden Layer 2 (2) → Output Layer (3)\")\n",
    "print()\n",
    "\n",
    "\n",
    "# First Hidden Layer: W1 (4x3) and B1 (1x3) - ReLU activation \n",
    "W1 = [[ 0.2,  0.5, -0.3],\n",
    "      [ 0.1, -0.2,  0.4],\n",
    "      [-0.4,  0.3,  0.2],\n",
    "      [ 0.6, -0.1,  0.5]]\n",
    "B1 = [[3.0, -2.1, 0.6]]\n",
    "\n",
    "# Second Hidden Layer: W2 (3x2) and B2 (1x2) - Sigmoid activation \n",
    "W2 = [[ 0.3, -0.5],\n",
    "      [ 0.7,  0.2],\n",
    "      [-0.6,  0.4]]\n",
    "B2 = [[4.3, 6.4]]\n",
    "\n",
    "# Output Layer: W3 (2x3) and B3 (1x3) - Softmax activation \n",
    "W3 = [[ 0.5, -0.3,  0.8],\n",
    "      [-0.2,  0.6, -0.4]]\n",
    "B3 = [[-1.5, 2.1, -3.3]]\n",
    "\n",
    "print(\"Layer Configurations (CORRECTED from the picture):\")\n",
    "print(f\"Layer 1: {np.array(W1).shape} weights, {np.array(B1).shape} biases, ReLU activation\")\n",
    "print(f\"Layer 2: {np.array(W2).shape} weights, {np.array(B2).shape} biases, Sigmoid activation\")\n",
    "print(f\"Layer 3: {np.array(W3).shape} weights, {np.array(B3).shape} biases, Softmax activation\")\n",
    "print()\n",
    "\n",
    "# Create network using Dense_Layer class\n",
    "network = MultiLayerNetwork()\n",
    "\n",
    "# Add layers using Dense_Layer instances with CORRECT dimensions\n",
    "layer1 = network.add_dense_layer(4, 3, 'relu', W1, B1)\n",
    "layer2 = network.add_dense_layer(3, 2, 'sigmoid', W2, B2)\n",
    "layer3 = network.add_dense_layer(2, 3, 'softmax', W3, B3)\n",
    "\n",
    "print(\"✅ Network created successfully using Dense_Layer class from Problem 1!\")\n",
    "print(f\"Number of Dense_Layer instances: {len(network.layers)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8279801",
   "metadata": {},
   "source": [
    "### Problem 2a: Forward Pass Execution\n",
    "\n",
    "Running the complete forward pass through the multi-layer network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4caabc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM 2a FORWARD PASS: Complete Network Computation using Dense_Layer Class\n",
    "print(\"=== PROBLEM 2a FORWARD PASS COMPUTATION ===\")\n",
    "print()\n",
    "\n",
    "print(\"Input sample:\", X_input[0])\n",
    "print()\n",
    "\n",
    "# Forward pass through all Dense_Layer instances\n",
    "final_output = network.forward_pass(X_input)\n",
    "\n",
    "print(\"Layer-by-Layer Computation Using Dense_Layer Class:\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Layer 1 computation\n",
    "print(\"LAYER 1 (4 → 3 neurons, ReLU activation):\")\n",
    "layer1_output = network.layers[0].layer_outputs\n",
    "print(f\"Input: {network.layers[0].inputs.flatten()}\")\n",
    "print(f\"Weighted Sum: {network.layers[0].current_weighted_sums.flatten()}\")\n",
    "print(f\"After ReLU: {layer1_output.flatten()}\")\n",
    "print(\"→ Passed to Layer 2\")\n",
    "print()\n",
    "\n",
    "# Layer 2 computation  \n",
    "print(\"LAYER 2 (3 → 2 neurons, Sigmoid activation):\")\n",
    "layer2_output = network.layers[1].layer_outputs\n",
    "print(f\"Input: {network.layers[1].inputs.flatten()}\")\n",
    "print(f\"Weighted Sum: {network.layers[1].current_weighted_sums.flatten()}\")\n",
    "print(f\"After Sigmoid: {layer2_output.flatten()}\")\n",
    "print(\"→ Passed to Layer 3\")\n",
    "print()\n",
    "\n",
    "# Layer 3 computation (Final output)\n",
    "print(\"LAYER 3 (2 → 3 neurons, Softmax activation - FINAL OUTPUT):\")\n",
    "layer3_output = network.layers[2].layer_outputs\n",
    "print(f\"Input: {network.layers[2].inputs.flatten()}\")\n",
    "print(f\"Weighted Sum: {network.layers[2].current_weighted_sums.flatten()}\")\n",
    "print(f\"After Softmax: {layer3_output.flatten()}\")\n",
    "print()\n",
    "\n",
    "# Since layer 3 now uses softmax activation, the outputs are already probabilities\n",
    "final_probs = layer3_output.flatten()\n",
    "predicted_class = np.argmax(final_probs)\n",
    "class_names = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print(\"FINAL CLASSIFICATION RESULTS:\")\n",
    "print(\"Class Probabilities (from Softmax layer):\")\n",
    "for i, (class_name, prob) in enumerate(zip(class_names, final_probs)):\n",
    "    marker = \" ← PREDICTED\" if i == predicted_class else \"\"\n",
    "    print(f\"  {class_name:15}: {prob:.4f} ({prob*100:.1f}%){marker}\")\n",
    "\n",
    "print()\n",
    "print(f\"Predicted Class: {class_names[predicted_class]}\")\n",
    "print(f\"Actual Class:    {class_names[np.argmax(target_output[0])]}\")\n",
    "print(f\"Classification: {'✅ Correct' if predicted_class == np.argmax(target_output[0]) else '❌ Incorrect'}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eede4ed3",
   "metadata": {},
   "source": [
    "### Problem 2a: Verification and Loss Calculation\n",
    "\n",
    "Manual step-by-step verification using individual Dense_Layer instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a47c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM 2a VERIFICATION: Manual Step-by-Step Calculation using Dense_Layer Class\n",
    "print(\"=== PROBLEM 2a VERIFICATION: MANUAL CALCULATION ===\")\n",
    "print(\"Verifying Dense_Layer implementation step by step\")\n",
    "print()\n",
    "\n",
    "# Manual verification using Dense_Layer class functions\n",
    "print(\"MANUAL VERIFICATION USING DENSE_LAYER CLASS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create individual layers for verification with CORRECT dimensions\n",
    "layer1_verify = Dense_Layer(4, 3, 'relu')\n",
    "layer2_verify = Dense_Layer(3, 2, 'sigmoid')  \n",
    "layer3_verify = Dense_Layer(2, 3, 'softmax')\n",
    "\n",
    "# Layer 1 verification\n",
    "print(\"LAYER 1 VERIFICATION (4 → 3, ReLU):\")\n",
    "layer1_verify.setup_weights_and_inputs(X_input, W1, B1)\n",
    "z1 = layer1_verify.weighted_sum_plus_bias()\n",
    "a1 = layer1_verify.apply_activation_function(z1)\n",
    "print(f\"✓ Input: {layer1_verify.inputs.flatten()}\")\n",
    "print(f\"✓ Weighted Sum: {z1.flatten()}\")\n",
    "print(f\"✓ ReLU Output: {a1.flatten()}\")\n",
    "print()\n",
    "\n",
    "# Layer 2 verification  \n",
    "print(\"LAYER 2 VERIFICATION (3 → 2, Sigmoid):\")\n",
    "layer2_verify.setup_weights_and_inputs(a1, W2, B2)\n",
    "z2 = layer2_verify.weighted_sum_plus_bias()\n",
    "a2 = layer2_verify.apply_activation_function(z2)\n",
    "print(f\"✓ Input: {layer2_verify.inputs.flatten()}\")\n",
    "print(f\"✓ Weighted Sum: {z2.flatten()}\")\n",
    "print(f\"✓ Sigmoid Output: {a2.flatten()}\")\n",
    "print()\n",
    "\n",
    "# Layer 3 verification (Final)\n",
    "print(\"LAYER 3 VERIFICATION (2 → 3, Softmax - FINAL):\")\n",
    "layer3_verify.setup_weights_and_inputs(a2, W3, B3)\n",
    "z3 = layer3_verify.weighted_sum_plus_bias()\n",
    "a3 = layer3_verify.apply_activation_function(z3)\n",
    "print(f\"✓ Input: {layer3_verify.inputs.flatten()}\")\n",
    "print(f\"✓ Weighted Sum: {z3.flatten()}\")\n",
    "print(f\"✓ Final Output (Softmax): {a3.flatten()}\")\n",
    "print()\n",
    "\n",
    "# Compare with network results\n",
    "print(\"=\" * 50)\n",
    "print(\"COMPARISON WITH NETWORK RESULTS:\")\n",
    "manual_result = a3.flatten()\n",
    "network_result = layer3_output.flatten()\n",
    "\n",
    "print(f\"Manual calculation:  {manual_result}\")\n",
    "print(f\"Network calculation: {network_result}\")\n",
    "print(f\"Difference: {np.abs(manual_result - network_result)}\")\n",
    "print(f\"Match: {'✅ PERFECT MATCH' if np.allclose(manual_result, network_result, atol=1e-6) else '❌ MISMATCH'}\")\n",
    "print()\n",
    "\n",
    "# Loss calculation using Dense_Layer class\n",
    "print(\"LOSS CALCULATION:\")\n",
    "loss_categorical = layer3_verify.calculate_loss(a3, target_output, 'categorical_crossentropy')\n",
    "loss_mse = layer3_verify.calculate_loss(a3, target_output, 'mse')\n",
    "print(f\"Categorical Cross-Entropy Loss: {loss_categorical:.6f}\")\n",
    "print(f\"Mean Squared Error (MSE) Loss: {loss_mse:.6f}\")\n",
    "print(f\"Target: {target_output[0]}, Prediction: {a3.flatten()}\")\n",
    "print()\n",
    "print(\"✅ Problem 2a completed successfully using Dense_Layer class from Problem 1!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee426d9e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Problem 2b: Breast Cancer Dataset Classification\n",
    "### *Completed by: Dallas Aquino*\n",
    "\n",
    "**Objective:** Given inputs from the Breast Cancer Dataset using three features (Mean Radius, Mean Texture, and Mean Smoothness), determine whether the tumor is Benign (0) or Malignant (1) by calculating the network outputs step by step.\n",
    "\n",
    "**Network Architecture:**\n",
    "- **Input Layer:** 3 features (Mean Radius, Mean Texture, Mean Smoothness)\n",
    "- **Hidden Layer 1:** 3 neurons with ReLU activation\n",
    "- **Hidden Layer 2:** 2 neurons with Sigmoid activation\n",
    "- **Output Layer:** 1 neuron (Binary classification) with Sigmoid activation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb7932e",
   "metadata": {},
   "source": [
    "### Problem 2b: Network Configuration and Setup\n",
    "\n",
    "Setting up the Breast Cancer classification problem with the given network architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceeb981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM 2b SETUP: Breast Cancer Classification using Dense_Layer Class\n",
    "print(\"=== PROBLEM 2b SETUP: BREAST CANCER CLASSIFICATION ===\")\n",
    "print(\"Implemented by: Dallas Aquino\")\n",
    "print()\n",
    "\n",
    "# Input data from Breast Cancer Dataset (Mean Radius, Mean Texture, Mean Smoothness)\n",
    "X_breast = [[14.1, 20.3, 0.095]]  # Sample breast cancer measurements\n",
    "\n",
    "# Target output (Binary classification: 0 = Benign, 1 = Malignant)\n",
    "target_breast = [[1]]  # This sample should be Malignant (1)\n",
    "\n",
    "print(\"Problem Configuration:\")\n",
    "print(f\"Input features (X): {X_breast[0]}\")\n",
    "print(\"  - Mean Radius:     {:.1f}\".format(X_breast[0][0]))\n",
    "print(\"  - Mean Texture:    {:.1f}\".format(X_breast[0][1]))\n",
    "print(\"  - Mean Smoothness: {:.3f}\".format(X_breast[0][2]))\n",
    "print()\n",
    "print(f\"Target output: {target_breast[0][0]} (1 = Malignant)\")\n",
    "print()\n",
    "\n",
    "# Network Architecture: 3 → 3 → 2 → 1 neurons\n",
    "print(\"Network Architecture:\")\n",
    "print(\"Input Layer (3) → Hidden Layer 1 (3) → Hidden Layer 2 (2) → Output Layer (1)\")\n",
    "print()\n",
    "\n",
    "# Define weights and biases for each layer EXACTLY as shown in the picture\n",
    "# Layer 1: 3 inputs → 3 neurons (ReLU) \n",
    "W1_breast = [[ 0.5, -0.3,  0.8],\n",
    "             [ 0.2,  0.4, -0.6], \n",
    "             [-0.7,  0.9,  0.1]]\n",
    "B1_breast = [[0.3, -0.5, 0.6]]\n",
    "\n",
    "# Layer 2: 3 inputs → 2 neurons (Sigmoid) - FIXED DIMENSIONS\n",
    "W2_breast = [[ 0.6, -0.3],\n",
    "             [-0.2,  0.5],\n",
    "             [ 0.4,  0.7]]\n",
    "B2_breast = [[0.1, -0.8]]\n",
    "\n",
    "# Layer 3: 2 inputs → 1 neuron (Sigmoid) - FIXED DIMENSIONS\n",
    "W3_breast = [[0.7], \n",
    "             [-0.5]]\n",
    "B3_breast = [[0.2]]\n",
    "\n",
    "print(\"Layer Configurations:\")\n",
    "print(f\"Layer 1: {np.array(W1_breast).shape} weights, {np.array(B1_breast).shape} biases, ReLU activation\")\n",
    "print(f\"Layer 2: {np.array(W2_breast).shape} weights, {np.array(B2_breast).shape} biases, Sigmoid activation\")\n",
    "print(f\"Layer 3: {np.array(W3_breast).shape} weights, {np.array(B3_breast).shape} biases, Sigmoid activation\")\n",
    "print()\n",
    "\n",
    "# Create network using Dense_Layer class\n",
    "network_breast = MultiLayerNetwork()\n",
    "\n",
    "# Add Dense_Layer instances with CORRECT dimensions from picture\n",
    "layer1_breast = network_breast.add_dense_layer(3, 3, 'relu', W1_breast, B1_breast)\n",
    "layer2_breast = network_breast.add_dense_layer(3, 2, 'sigmoid', W2_breast, B2_breast)\n",
    "layer3_breast = network_breast.add_dense_layer(2, 1, 'sigmoid', W3_breast, B3_breast)\n",
    "\n",
    "print(\"✅ Network created successfully using Dense_Layer class from Problem 1!\")\n",
    "print(f\"Number of Dense_Layer instances: {len(network_breast.layers)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4726ea0c",
   "metadata": {},
   "source": [
    "### Problem 2b: Forward Pass Execution\n",
    "\n",
    "Running the complete forward pass through the multi-layer Breast Cancer classification network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0de0679",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM 2b FORWARD PASS: Complete Network Computation using Dense_Layer Class\n",
    "print(\"=== PROBLEM 2b FORWARD PASS COMPUTATION ===\")\n",
    "print()\n",
    "\n",
    "print(\"Input sample:\", X_breast[0])\n",
    "print()\n",
    "\n",
    "# Forward pass through all Dense_Layer instances\n",
    "final_output_breast = network_breast.forward_pass(X_breast)\n",
    "\n",
    "print(\"Layer-by-Layer Computation Using Dense_Layer Class:\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Layer 1 computation\n",
    "print(\"LAYER 1 (3 → 3 neurons, ReLU activation):\")\n",
    "layer1_output_breast = network_breast.layers[0].layer_outputs\n",
    "print(f\"Input: {network_breast.layers[0].inputs.flatten()}\")\n",
    "print(f\"Weighted Sum: {network_breast.layers[0].current_weighted_sums.flatten()}\")\n",
    "print(f\"After ReLU: {layer1_output_breast.flatten()}\")\n",
    "print(\"→ Passed to Layer 2\")\n",
    "print()\n",
    "\n",
    "# Layer 2 computation  \n",
    "print(\"LAYER 2 (3 → 2 neurons, Sigmoid activation):\")\n",
    "layer2_output_breast = network_breast.layers[1].layer_outputs\n",
    "print(f\"Input: {network_breast.layers[1].inputs.flatten()}\")\n",
    "print(f\"Weighted Sum: {network_breast.layers[1].current_weighted_sums.flatten()}\")\n",
    "print(f\"After Sigmoid: {layer2_output_breast.flatten()}\")\n",
    "print(\"→ Passed to Layer 3\")\n",
    "print()\n",
    "\n",
    "# Layer 3 computation (Final output)\n",
    "print(\"LAYER 3 (2 → 1 neuron, Sigmoid activation - FINAL OUTPUT):\")\n",
    "layer3_output_breast = network_breast.layers[2].layer_outputs\n",
    "print(f\"Input: {network_breast.layers[2].inputs.flatten()}\")\n",
    "print(f\"Weighted Sum: {network_breast.layers[2].current_weighted_sums.flatten()}\")\n",
    "print(f\"After Sigmoid: {layer3_output_breast.flatten()}\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print(\"FINAL RESULTS:\")\n",
    "final_prediction_breast = layer3_output_breast[0][0]\n",
    "print(f\"Network Prediction: {final_prediction_breast:.6f}\")\n",
    "print(f\"Target Value: {target_breast[0][0]}\")\n",
    "print(f\"Prediction Interpretation: {'Malignant' if final_prediction_breast > 0.5 else 'Benign'}\")\n",
    "print(f\"Classification: {'✅ Correct' if (final_prediction_breast > 0.5) == (target_breast[0][0] == 1) else '❌ Incorrect'}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1265f261",
   "metadata": {},
   "source": [
    "### Problem 2b: Verification and Loss Calculation\n",
    "\n",
    "Manual step-by-step verification using individual Dense_Layer instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf5c2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROBLEM 2b VERIFICATION: Manual Step-by-Step Calculation using Dense_Layer Class\n",
    "print(\"=== PROBLEM 2b VERIFICATION: MANUAL CALCULATION ===\")\n",
    "print(\"Verifying Dense_Layer implementation step by step\")\n",
    "print()\n",
    "\n",
    "# Manual verification using Dense_Layer class functions\n",
    "print(\"MANUAL VERIFICATION USING DENSE_LAYER CLASS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create individual layers for verification with CORRECT dimensions\n",
    "layer1_verify = Dense_Layer(3, 3, 'relu')\n",
    "layer2_verify = Dense_Layer(3, 2, 'sigmoid')  \n",
    "layer3_verify = Dense_Layer(2, 1, 'sigmoid')\n",
    "\n",
    "# Layer 1 verification\n",
    "print(\"LAYER 1 VERIFICATION (3 → 3, ReLU):\")\n",
    "layer1_verify.setup_weights_and_inputs(X_breast, W1_breast, B1_breast)\n",
    "z1 = layer1_verify.weighted_sum_plus_bias()\n",
    "a1 = layer1_verify.apply_activation_function(z1)\n",
    "print(f\"✓ Input: {layer1_verify.inputs.flatten()}\")\n",
    "print(f\"✓ Weighted Sum: {z1.flatten()}\")\n",
    "print(f\"✓ ReLU Output: {a1.flatten()}\")\n",
    "print()\n",
    "\n",
    "# Layer 2 verification  \n",
    "print(\"LAYER 2 VERIFICATION (3 → 2, Sigmoid):\")\n",
    "layer2_verify.setup_weights_and_inputs(a1, W2_breast, B2_breast)\n",
    "z2 = layer2_verify.weighted_sum_plus_bias()\n",
    "a2 = layer2_verify.apply_activation_function(z2)\n",
    "print(f\"✓ Input: {layer2_verify.inputs.flatten()}\")\n",
    "print(f\"✓ Weighted Sum: {z2.flatten()}\")\n",
    "print(f\"✓ Sigmoid Output: {a2.flatten()}\")\n",
    "print()\n",
    "\n",
    "# Layer 3 verification (Final)\n",
    "print(\"LAYER 3 VERIFICATION (2 → 1, Sigmoid - FINAL):\")\n",
    "layer3_verify.setup_weights_and_inputs(a2, W3_breast, B3_breast)\n",
    "z3 = layer3_verify.weighted_sum_plus_bias()\n",
    "a3 = layer3_verify.apply_activation_function(z3)\n",
    "print(f\"✓ Input: {layer3_verify.inputs.flatten()}\")\n",
    "print(f\"✓ Weighted Sum: {z3.flatten()}\")\n",
    "print(f\"✓ Final Output: {a3.flatten()}\")\n",
    "print()\n",
    "\n",
    "# Compare with network results\n",
    "print(\"=\" * 50)\n",
    "print(\"COMPARISON WITH NETWORK RESULTS:\")\n",
    "manual_result = a3[0][0]\n",
    "network_result = final_prediction_breast  # From previous cell\n",
    "\n",
    "print(f\"Manual calculation:  {manual_result:.8f}\")\n",
    "print(f\"Network calculation: {network_result:.8f}\")\n",
    "print(f\"Difference: {abs(manual_result - network_result):.10f}\")\n",
    "print(f\"Match: {'✅ PERFECT MATCH' if abs(manual_result - network_result) < 1e-6 else '❌ MISMATCH'}\")\n",
    "print()\n",
    "\n",
    "# Loss calculation using Dense_Layer class\n",
    "print(\"LOSS CALCULATION:\")\n",
    "loss_binary = layer3_verify.calculate_loss(a3, target_breast, 'binary_crossentropy')\n",
    "loss_mse = layer3_verify.calculate_loss(a3, target_breast, 'mse')\n",
    "print(f\"Binary Cross-Entropy Loss: {loss_binary:.6f}\")\n",
    "print(f\"Mean Squared Error (MSE) Loss: {loss_mse:.6f}\")\n",
    "print(f\"Target: {target_breast[0][0]}, Prediction: {a3[0][0]:.6f}\")\n",
    "print()\n",
    "print(\"✅ Problem 2b completed successfully using Dense_Layer class from Problem 1!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

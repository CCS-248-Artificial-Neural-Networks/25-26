{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44995823",
   "metadata": {},
   "source": [
    "# Unit 2 Exercise - Neural Network Implementation\n",
    "## Team: Kirk Henrich Gamo & Dallas Aquino\n",
    "\n",
    "This notebook contains solutions to neural network problems completed by our team:\n",
    "\n",
    "### **Problem 1: Dense Layer Implementation (50 points)** - *Completed by Kirk and Dallas*\n",
    "- Develop a Dense_Layer class with required functions:\n",
    "  - setup_weights_and_inputs (10 points)\n",
    "  - weighted_sum_plus_bias (10 points) \n",
    "  - apply_activation_function (15 points)\n",
    "  - calculate_loss (15 points)\n",
    "\n",
    "### **Problem 2: Multi-Layer Neural Network Classification (50 points)**\n",
    "Each team member selected and completed one classification problem:\n",
    "\n",
    "**Problem 2a: Iris Dataset Classification** - *Completed by Kirk*\n",
    "- 3-layer neural network for Iris species classification\n",
    "- Multi-class classification with Softmax output\n",
    "\n",
    "**Problem 2b: Breast Cancer Dataset Classification** - *Completed by Dallas*\n",
    "- 3-layer neural network for tumor classification  \n",
    "- Binary classification with Sigmoid output\n",
    "\n",
    "\n",
    "**Total Points: 100**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355b7f18",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Problem 1: Dense Layer Implementation (50 points)\n",
    "\n",
    "**Objective:** Develop a Dense_Layer class in Python with the following functions:\n",
    "1. A function to setup/accept the inputs and weights (10 points)\n",
    "2. A function to perform the weighted sum + bias (10 points)  \n",
    "3. A function to perform the selected activation function (15 points)\n",
    "4. A function to calculate the loss (predicted output vs target output) (15 points)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0ebc5b",
   "metadata": {},
   "source": [
    "### Required Libraries for Problem 1\n",
    "\n",
    "First, let's import the necessary libraries for our Dense Layer implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b2036f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c1b340",
   "metadata": {},
   "source": [
    "### Dense_Layer Class Implementation\n",
    "\n",
    "Below is the complete implementation of the Dense_Layer class with all required functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "1efdc083",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense_Layer:\n",
    "    def __init__(self, n_inputs, n_neurons, activation='relu'):\n",
    "        \"\"\"\n",
    "        Initialize the Dense Layer\n",
    "        \n",
    "        Parameters:\n",
    "        n_inputs: Number of input features\n",
    "        n_neurons: Number of neurons in this layer\n",
    "        activation: Activation function to use ('relu', 'sigmoid', 'tanh', 'linear')\n",
    "        \"\"\"\n",
    "        self.n_inputs = n_inputs\n",
    "        self.n_neurons = n_neurons\n",
    "        self.activation_type = activation\n",
    "        \n",
    "        # Initialize weights and biases\n",
    "        self.weights = None\n",
    "        self.biases = None\n",
    "        self.inputs = None\n",
    "        self.output = None\n",
    "        \n",
    "    def setup_weights_and_inputs(self, inputs, weights=None, biases=None):\n",
    "        \"\"\"\n",
    "        Function to setup/accept the inputs and weights (10 points)\n",
    "        \n",
    "        Parameters:\n",
    "        inputs: Input data (numpy array)\n",
    "        weights: Weight matrix (if None, initialize randomly)\n",
    "        biases: Bias vector (if None, initialize as zeros)\n",
    "        \"\"\"\n",
    "        self.inputs = np.array(inputs)\n",
    "        \n",
    "        # Initialize weights if not provided\n",
    "        if weights is None:\n",
    "            # Xavier/Glorot initialization\n",
    "            self.weights = np.random.randn(self.n_inputs, self.n_neurons) * np.sqrt(2.0 / self.n_inputs)\n",
    "        else:\n",
    "            self.weights = np.array(weights)\n",
    "            \n",
    "        # Initialize biases if not provided\n",
    "        if biases is None:\n",
    "            self.biases = np.zeros((1, self.n_neurons))\n",
    "        else:\n",
    "            self.biases = np.array(biases)\n",
    "            \n",
    "        print(f\"Setup complete:\")\n",
    "        print(f\"Input shape: {self.inputs.shape}\")\n",
    "        print(f\"Weights shape: {self.weights.shape}\")\n",
    "        print(f\"Biases shape: {self.biases.shape}\")\n",
    "        \n",
    "    def weighted_sum_plus_bias(self):\n",
    "        \"\"\"\n",
    "        Function to perform the weighted sum + bias (10 points)\n",
    "        \n",
    "        Returns:\n",
    "        z: The linear combination (weighted sum + bias)\n",
    "        \"\"\"\n",
    "        if self.inputs is None or self.weights is None or self.biases is None:\n",
    "            raise ValueError(\"Must call setup_weights_and_inputs first!\")\n",
    "            \n",
    "        # Compute weighted sum: inputs @ weights + bias\n",
    "        z = np.dot(self.inputs, self.weights) + self.biases\n",
    "        \n",
    "        print(f\"Weighted sum + bias computed. Output shape: {z.shape}\")\n",
    "        return z\n",
    "        \n",
    "    def apply_activation_function(self, z):\n",
    "        \"\"\"\n",
    "        Function to perform the selected activation function (15 points)\n",
    "        \n",
    "        Parameters:\n",
    "        z: Linear combination from weighted_sum_plus_bias\n",
    "        \n",
    "        Returns:\n",
    "        activated_output: Output after applying activation function\n",
    "        \"\"\"\n",
    "        if self.activation_type == 'relu':\n",
    "            # ReLU: max(0, z)\n",
    "            activated_output = np.maximum(0, z)\n",
    "            \n",
    "        elif self.activation_type == 'sigmoid':\n",
    "            # Sigmoid: 1 / (1 + e^(-z))\n",
    "            activated_output = 1 / (1 + np.exp(-np.clip(z, -500, 500)))  # Clip to prevent overflow\n",
    "            \n",
    "        elif self.activation_type == 'tanh':\n",
    "            # Hyperbolic tangent\n",
    "            activated_output = np.tanh(z)\n",
    "            \n",
    "        elif self.activation_type == 'softmax':\n",
    "            # Softmax: e^(z_i) / sum(e^z) for multi-class classification\n",
    "            exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))  # Subtract max for numerical stability\n",
    "            activated_output = exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "            \n",
    "        elif self.activation_type == 'linear':\n",
    "            # Linear activation (no change)\n",
    "            activated_output = z\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported activation function: {self.activation_type}\")\n",
    "            \n",
    "        self.output = activated_output\n",
    "        print(f\"Applied {self.activation_type} activation function\")\n",
    "        return activated_output\n",
    "        \n",
    "    def calculate_loss(self, predicted_output, target_output, loss_type='mse'):\n",
    "        \"\"\"\n",
    "        Function to calculate the loss (predicted output vs target output) (15 points)\n",
    "        \n",
    "        Parameters:\n",
    "        predicted_output: The predicted values from the network\n",
    "        target_output: The actual target values\n",
    "        loss_type: Type of loss function ('mse', 'mae', 'binary_crossentropy', 'categorical_crossentropy')\n",
    "        \n",
    "        Returns:\n",
    "        loss: The calculated loss value\n",
    "        \"\"\"\n",
    "        predicted = np.array(predicted_output)\n",
    "        target = np.array(target_output)\n",
    "        \n",
    "        if predicted.shape != target.shape:\n",
    "            raise ValueError(f\"Shape mismatch: predicted {predicted.shape} vs target {target.shape}\")\n",
    "            \n",
    "        if loss_type == 'mse':\n",
    "            # Mean Squared Error\n",
    "            loss = np.mean((predicted - target) ** 2)\n",
    "            \n",
    "        elif loss_type == 'mae':\n",
    "            # Mean Absolute Error\n",
    "            loss = np.mean(np.abs(predicted - target))\n",
    "            \n",
    "        elif loss_type == 'binary_crossentropy':\n",
    "            # Binary Cross-Entropy (for binary classification)\n",
    "            # Clip predictions to prevent log(0)\n",
    "            predicted_clipped = np.clip(predicted, 1e-15, 1 - 1e-15)\n",
    "            loss = -np.mean(target * np.log(predicted_clipped) + (1 - target) * np.log(1 - predicted_clipped))\n",
    "            \n",
    "        elif loss_type == 'categorical_crossentropy':\n",
    "            # Categorical Cross-Entropy (for multi-class classification)\n",
    "            predicted_clipped = np.clip(predicted, 1e-15, 1 - 1e-15)\n",
    "            loss = -np.mean(np.sum(target * np.log(predicted_clipped), axis=1))\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported loss function: {loss_type}\")\n",
    "            \n",
    "        print(f\"Calculated {loss_type} loss: {loss:.6f}\")\n",
    "        return loss\n",
    "        \n",
    "    def forward_pass(self, inputs, target=None, loss_type='mse'):\n",
    "        \"\"\"\n",
    "        Complete forward pass through the layer\n",
    "        \n",
    "        Parameters:\n",
    "        inputs: Input data\n",
    "        target: Target output (optional, for loss calculation)\n",
    "        loss_type: Type of loss function to use\n",
    "        \n",
    "        Returns:\n",
    "        output: Final output after activation\n",
    "        loss: Loss value (if target provided)\n",
    "        \"\"\"\n",
    "        # Setup inputs (weights should already be initialized)\n",
    "        if self.weights is None:\n",
    "            raise ValueError(\"Weights not initialized. Call setup_weights_and_inputs first.\")\n",
    "            \n",
    "        self.inputs = np.array(inputs)\n",
    "        \n",
    "        # Forward pass\n",
    "        z = self.weighted_sum_plus_bias()\n",
    "        output = self.apply_activation_function(z)\n",
    "        \n",
    "        loss = None\n",
    "        if target is not None:\n",
    "            loss = self.calculate_loss(output, target, loss_type)\n",
    "            \n",
    "        return output, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ac2b5d",
   "metadata": {},
   "source": [
    "### Problem 1: Function Testing and Demonstration\n",
    "\n",
    "Let's test each function of our Dense_Layer class systematically to verify correct implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "47ad0953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEST 1: Setup Weights and Inputs Function ===\n",
      "\n",
      "Configuration:\n",
      "Number of inputs: 3\n",
      "Number of neurons: 2\n",
      "Input data: [[1.0, 2.0, 3.0]]\n",
      "Custom weights: [[0.2, 0.8], [0.5, -0.7], [0.1, 0.3]]\n",
      "Custom biases: [[0.1, -0.2]]\n",
      "\n",
      "Setting up layer...\n",
      "Setup complete:\n",
      "Input shape: (1, 3)\n",
      "Weights shape: (3, 2)\n",
      "Biases shape: (1, 2)\n",
      "\n",
      "✅ TEST 1 PASSED: setup_weights_and_inputs function working correctly!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# TEST 1: Function setup_weights_and_inputs (10 points)\n",
    "print(\"=== TEST 1: Setup Weights and Inputs Function ===\")\n",
    "print()\n",
    "\n",
    "# Create a dense layer with 3 inputs and 2 neurons\n",
    "layer1 = Dense_Layer(n_inputs=3, n_neurons=2, activation='relu')\n",
    "\n",
    "# Sample input data (1 sample with 3 features)\n",
    "sample_inputs = [[1.0, 2.0, 3.0]]\n",
    "\n",
    "# Custom weights matrix (3x2) and biases (1x2)\n",
    "custom_weights = [[0.2, 0.8], \n",
    "                  [0.5, -0.7], \n",
    "                  [0.1, 0.3]]\n",
    "custom_biases = [[0.1, -0.2]]\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"Number of inputs: {layer1.n_inputs}\")\n",
    "print(f\"Number of neurons: {layer1.n_neurons}\")\n",
    "print(f\"Input data: {sample_inputs}\")\n",
    "print(f\"Custom weights: {custom_weights}\")\n",
    "print(f\"Custom biases: {custom_biases}\")\n",
    "print()\n",
    "\n",
    "# Setup the layer\n",
    "print(\"Setting up layer...\")\n",
    "layer1.setup_weights_and_inputs(sample_inputs, custom_weights, custom_biases)\n",
    "print()\n",
    "\n",
    "print(\"✅ TEST 1 PASSED: setup_weights_and_inputs function working correctly!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d66f16ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEST 2: Weighted Sum Plus Bias Function ===\n",
      "\n",
      "Using the layer from TEST 1...\n",
      "Input: [1.0, 2.0, 3.0]\n",
      "Weights shape: (3, 2)\n",
      "Biases shape: (1, 2)\n",
      "\n",
      "Weighted sum + bias computed. Output shape: (1, 2)\n",
      "Calculated weighted sum + bias (z): [1.6 0.1]\n",
      "\n",
      "Manual Verification:\n",
      "For neuron 1: (1.0×0.2) + (2.0×0.5) + (3.0×0.1) + 0.1 = 0.2 + 1.0 + 0.3 + 0.1 = 1.6\n",
      "For neuron 2: (1.0×0.8) + (2.0×-0.7) + (3.0×0.3) + (-0.2) = 0.8 - 1.4 + 0.9 - 0.2 = 0.1\n",
      "\n",
      "Manual calculation: [1.6, 0.1]\n",
      "Function result:    [1.6, 0.10000000000000009]\n",
      "✅ TEST 2 PASSED: weighted_sum_plus_bias function working correctly!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# TEST 2: Function weighted_sum_plus_bias (10 points)\n",
    "print(\"=== TEST 2: Weighted Sum Plus Bias Function ===\")\n",
    "print()\n",
    "\n",
    "print(\"Using the layer from TEST 1...\")\n",
    "print(\"Input:\", sample_inputs[0])\n",
    "print(\"Weights shape:\", layer1.weights.shape)\n",
    "print(\"Biases shape:\", layer1.biases.shape)\n",
    "print()\n",
    "\n",
    "# Get weighted sum + bias\n",
    "z = layer1.weighted_sum_plus_bias()\n",
    "print(\"Calculated weighted sum + bias (z):\", z.flatten())\n",
    "print()\n",
    "\n",
    "# Manual verification\n",
    "print(\"Manual Verification:\")\n",
    "print(\"For neuron 1: (1.0×0.2) + (2.0×0.5) + (3.0×0.1) + 0.1 = 0.2 + 1.0 + 0.3 + 0.1 = 1.6\")\n",
    "print(\"For neuron 2: (1.0×0.8) + (2.0×-0.7) + (3.0×0.3) + (-0.2) = 0.8 - 1.4 + 0.9 - 0.2 = 0.1\")\n",
    "print()\n",
    "\n",
    "# Verify our calculation\n",
    "manual_calc = [1.6, 0.1]\n",
    "print(f\"Manual calculation: {manual_calc}\")\n",
    "print(f\"Function result:    {z.flatten().tolist()}\")\n",
    "\n",
    "if np.allclose(z.flatten(), manual_calc):\n",
    "    print(\"✅ TEST 2 PASSED: weighted_sum_plus_bias function working correctly!\")\n",
    "else:\n",
    "    print(\"❌ TEST 2 FAILED: Results don't match manual calculation\")\n",
    "\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d20b1bf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEST 3: Apply Activation Function ===\n",
      "\n",
      "Input to activation function (z): [1.6 0.1]\n",
      "Selected activation function: relu\n",
      "\n",
      "Applied relu activation function\n",
      "Output after ReLU activation: [1.6 0.1]\n",
      "\n",
      "Manual Verification:\n",
      "ReLU function: max(0, z)\n",
      "For z = [1.6, 0.1]:\n",
      "  max(0, 1.6) = 1.6\n",
      "  max(0, 0.1) = 0.1\n",
      "Expected output: [1.6, 0.1]\n",
      "\n",
      "Testing all activation functions:\n",
      "----------------------------------------\n",
      "Applied relu activation function\n",
      "RELU     | Input: [-2.0, -0.5, 0.0, 0.5, 2.0] → Output: [0.0, 0.0, 0.0, 0.5, 2.0]\n",
      "Applied sigmoid activation function\n",
      "SIGMOID  | Input: [-2.0, -0.5, 0.0, 0.5, 2.0] → Output: [0.119, 0.378, 0.5, 0.622, 0.881]\n",
      "Applied tanh activation function\n",
      "TANH     | Input: [-2.0, -0.5, 0.0, 0.5, 2.0] → Output: [-0.964, -0.462, 0.0, 0.462, 0.964]\n",
      "Applied linear activation function\n",
      "LINEAR   | Input: [-2.0, -0.5, 0.0, 0.5, 2.0] → Output: [-2.0, -0.5, 0.0, 0.5, 2.0]\n",
      "\n",
      "✅ TEST 3 PASSED: apply_activation_function working for all activation types!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# TEST 3: Function apply_activation_function (15 points)\n",
    "print(\"=== TEST 3: Apply Activation Function ===\")\n",
    "print()\n",
    "\n",
    "# Using z from previous test\n",
    "print(\"Input to activation function (z):\", z.flatten())\n",
    "print(f\"Selected activation function: {layer1.activation_type}\")\n",
    "print()\n",
    "\n",
    "# Apply activation function\n",
    "output = layer1.apply_activation_function(z)\n",
    "print(\"Output after ReLU activation:\", output.flatten())\n",
    "print()\n",
    "\n",
    "print(\"Manual Verification:\")\n",
    "print(\"ReLU function: max(0, z)\")\n",
    "print(\"For z = [1.6, 0.1]:\")\n",
    "print(\"  max(0, 1.6) = 1.6\")\n",
    "print(\"  max(0, 0.1) = 0.1\")\n",
    "print(\"Expected output: [1.6, 0.1]\")\n",
    "print()\n",
    "\n",
    "# Test different activation functions\n",
    "print(\"Testing all activation functions:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "test_z = [[-2.0, -0.5, 0.0, 0.5, 2.0]]\n",
    "activations = ['relu', 'sigmoid', 'tanh', 'linear']\n",
    "\n",
    "for activation in activations:\n",
    "    test_layer = Dense_Layer(n_inputs=1, n_neurons=5, activation=activation)\n",
    "    test_output = test_layer.apply_activation_function(np.array(test_z))\n",
    "    print(f\"{activation.upper():8} | Input: {test_z[0]} → Output: {test_output.flatten().round(3).tolist()}\")\n",
    "\n",
    "print()\n",
    "print(\"✅ TEST 3 PASSED: apply_activation_function working for all activation types!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1106b790",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TEST 4: Calculate Loss Function ===\n",
      "\n",
      "Configuration:\n",
      "Predicted output: [1.6 0.1]\n",
      "Target output:    [1.5, 0.0]\n",
      "\n",
      "Calculated mse loss: 0.010000\n",
      "MSE Loss: 0.010000\n",
      "\n",
      "Manual MSE Verification:\n",
      "Manual calculation: mean([(1.6 - 1.5)², (0.1 - 0.0)²])\n",
      "                  = mean([(0.1)², (0.1)²])\n",
      "                  = mean([0.01, 0.01])\n",
      "                  = 0.010000\n",
      "\n",
      "Testing all loss functions:\n",
      "--------------------------------------------------\n",
      "Calculated mse loss: 0.040000\n",
      "MSE                  | Loss: 0.040000\n",
      "Calculated mae loss: 0.200000\n",
      "MAE                  | Loss: 0.200000\n",
      "Calculated categorical_crossentropy loss: 0.223144\n",
      "CATEGORICAL_CROSSENTROPY | Loss: 0.223144\n",
      "Calculated binary_crossentropy loss: 0.356675\n",
      "BINARY_CROSSENTROPY  | Loss: 0.356675\n",
      "\n",
      "✅ TEST 4 PASSED: calculate_loss function working for all loss types!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# TEST 4: Function calculate_loss (15 points)\n",
    "print(\"=== TEST 4: Calculate Loss Function ===\")\n",
    "print()\n",
    "\n",
    "# Using output from previous test as predictions\n",
    "predictions = output\n",
    "target = [[1.5, 0.0]]\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"Predicted output: {predictions.flatten()}\")\n",
    "print(f\"Target output:    {target[0]}\")\n",
    "print()\n",
    "\n",
    "# Calculate MSE loss\n",
    "loss = layer1.calculate_loss(predictions, target, 'mse')\n",
    "print(f\"MSE Loss: {loss:.6f}\")\n",
    "print()\n",
    "\n",
    "# Manual verification of MSE\n",
    "print(\"Manual MSE Verification:\")\n",
    "pred_flat = predictions.flatten()\n",
    "target_flat = np.array(target).flatten()\n",
    "manual_mse = np.mean((pred_flat - target_flat) ** 2)\n",
    "print(f\"Manual calculation: mean([({pred_flat[0]:.1f} - {target_flat[0]:.1f})², ({pred_flat[1]:.1f} - {target_flat[1]:.1f})²])\")\n",
    "print(f\"                  = mean([({pred_flat[0] - target_flat[0]:.1f})², ({pred_flat[1] - target_flat[1]:.1f})²])\")\n",
    "print(f\"                  = mean([{(pred_flat[0] - target_flat[0])**2:.2f}, {(pred_flat[1] - target_flat[1])**2:.2f}])\")\n",
    "print(f\"                  = {manual_mse:.6f}\")\n",
    "print()\n",
    "\n",
    "# Test different loss functions\n",
    "print(\"Testing all loss functions:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Test cases for different loss types\n",
    "test_pred = [[0.8, 0.2]]\n",
    "test_target = [[1.0, 0.0]]\n",
    "\n",
    "loss_functions = ['mse', 'mae', 'categorical_crossentropy']\n",
    "for loss_type in loss_functions:\n",
    "    test_loss = layer1.calculate_loss(test_pred, test_target, loss_type)\n",
    "    print(f\"{loss_type.upper():20} | Loss: {test_loss:.6f}\")\n",
    "\n",
    "# Binary classification test\n",
    "binary_pred = [[0.7]]\n",
    "binary_target = [[1.0]]\n",
    "binary_loss = layer1.calculate_loss(binary_pred, binary_target, 'binary_crossentropy')\n",
    "print(f\"{'BINARY_CROSSENTROPY':20} | Loss: {binary_loss:.6f}\")\n",
    "\n",
    "print()\n",
    "print(\"✅ TEST 4 PASSED: calculate_loss function working for all loss types!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "03b41b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPREHENSIVE TEST: Complete Forward Pass ===\n",
      "\n",
      "Configuration:\n",
      "Input shape: (3, 4) (3 samples, 4 features)\n",
      "Target shape: (3, 3) (3 samples, 3 outputs)\n",
      "Layer: 4 inputs → 3 neurons\n",
      "Activation: relu\n",
      "\n",
      "Setup complete:\n",
      "Input shape: (3, 4)\n",
      "Weights shape: (4, 3)\n",
      "Biases shape: (1, 3)\n",
      "\n",
      "Running complete forward pass:\n",
      "------------------------------\n",
      "Weighted sum + bias computed. Output shape: (3, 3)\n",
      "1. Weighted sum + bias (z) shape: (3, 3)\n",
      "   Sample outputs: [-0.199 -1.542  1.927]\n",
      "Applied relu activation function\n",
      "2. After ReLU activation shape: (3, 3)\n",
      "   Sample outputs: [0.    0.    1.927]\n",
      "Calculated mse loss: 0.778804\n",
      "3. MSE Loss: 0.778804\n",
      "\n",
      "FINAL RESULTS SUMMARY:\n",
      "==================================================\n",
      "Sample | Input Features           | Predicted Output      | Target Output        | Individual Error\n",
      "-------|--------------------------|----------------------|----------------------|------------------\n",
      "   1   | [1.0, 2.0, -1.0, 0.5]    | [0.    0.    1.927]  | [1.0, 0.0, 0.5]      | 1.012315\n",
      "   2   | [0.5, -1.0, 2.0, 1.5]    | [1.407 1.647 1.151]  | [0.0, 1.0, 0.3]      | 1.041047\n",
      "   3   | [-0.5, 1.0, 0.0, -1.0]   | [0.    0.    1.591]  | [0.5, 0.5, 1.0]      | 0.283049\n",
      "\n",
      "Overall MSE Loss: 0.778804\n",
      "\n",
      "🎉 COMPREHENSIVE TEST PASSED: All Dense Layer functions working correctly!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# COMPREHENSIVE TEST: Complete Forward Pass Integration\n",
    "print(\"=== COMPREHENSIVE TEST: Complete Forward Pass ===\")\n",
    "print()\n",
    "\n",
    "# Create a new layer for batch processing\n",
    "batch_layer = Dense_Layer(n_inputs=4, n_neurons=3, activation='relu')\n",
    "\n",
    "# Sample batch of data (3 samples, 4 features each)\n",
    "batch_inputs = [[1.0, 2.0, -1.0, 0.5],\n",
    "                [0.5, -1.0, 2.0, 1.5],\n",
    "                [-0.5, 1.0, 0.0, -1.0]]\n",
    "\n",
    "# Target outputs for the batch\n",
    "target_outputs = [[1.0, 0.0, 0.5],\n",
    "                  [0.0, 1.0, 0.3],\n",
    "                  [0.5, 0.5, 1.0]]\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"Input shape: {np.array(batch_inputs).shape} (3 samples, 4 features)\")\n",
    "print(f\"Target shape: {np.array(target_outputs).shape} (3 samples, 3 outputs)\")\n",
    "print(f\"Layer: {batch_layer.n_inputs} inputs → {batch_layer.n_neurons} neurons\")\n",
    "print(f\"Activation: {batch_layer.activation_type}\")\n",
    "print()\n",
    "\n",
    "# Initialize with random weights (Xavier initialization)\n",
    "batch_layer.setup_weights_and_inputs(batch_inputs)\n",
    "print()\n",
    "\n",
    "print(\"Running complete forward pass:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Step-by-step forward pass\n",
    "z = batch_layer.weighted_sum_plus_bias()\n",
    "print(f\"1. Weighted sum + bias (z) shape: {z.shape}\")\n",
    "print(f\"   Sample outputs: {z[0].round(3)}\")\n",
    "\n",
    "output = batch_layer.apply_activation_function(z)\n",
    "print(f\"2. After ReLU activation shape: {output.shape}\")\n",
    "print(f\"   Sample outputs: {output[0].round(3)}\")\n",
    "\n",
    "loss = batch_layer.calculate_loss(output, target_outputs, 'mse')\n",
    "print(f\"3. MSE Loss: {loss:.6f}\")\n",
    "print()\n",
    "\n",
    "print(\"FINAL RESULTS SUMMARY:\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Sample | Input Features           | Predicted Output      | Target Output        | Individual Error\")\n",
    "print(\"-------|--------------------------|----------------------|----------------------|------------------\")\n",
    "\n",
    "for i, (inp, pred, target) in enumerate(zip(batch_inputs, output, target_outputs)):\n",
    "    individual_error = np.mean((np.array(pred) - np.array(target)) ** 2)\n",
    "    print(f\"   {i+1}   | {str(inp):24} | {str(pred.round(3)):20} | {str(target):20} | {individual_error:.6f}\")\n",
    "\n",
    "print()\n",
    "print(f\"Overall MSE Loss: {loss:.6f}\")\n",
    "print()\n",
    "print(\"🎉 COMPREHENSIVE TEST PASSED: All Dense Layer functions working correctly!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fbe99c8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Problem 2: Multi-Layer Neural Network Classification (50 points)\n",
    "\n",
    "**Objective:** Use the Dense_Layer class from Problem 1 to build complete multi-layer neural networks for classification tasks.\n",
    "\n",
    "**Team Assignment:** Each member completed one classification problem:\n",
    "- **Problem 2a:** Iris Dataset Classification - *Kirk Henrich Gamo*\n",
    "- **Problem 2b:** Breast Cancer Dataset Classification - *Dallas Aquino*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2adfb8",
   "metadata": {},
   "source": [
    "### Multi-Layer Network Class\n",
    "\n",
    "The following class uses Dense_Layer instances from Problem 1 to create complete neural networks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f8ad0c78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Multi-layer network class defined using Dense_Layer from Problem 1!\n",
      "This implementation demonstrates how the Dense_Layer class can build complete neural networks.\n"
     ]
    }
   ],
   "source": [
    "# Multi-layer network class using Dense_Layer from Problem 1\n",
    "import numpy as np\n",
    "\n",
    "class MultiLayerNetwork:\n",
    "    \"\"\"\n",
    "    Multi-layer neural network using Dense_Layer class from Problem 1\n",
    "    This demonstrates how the Dense_Layer class can be used to build complete neural networks\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        \n",
    "    def add_dense_layer(self, n_inputs, n_neurons, activation, weights=None, biases=None):\n",
    "        \"\"\"Add a Dense_Layer instance to the network\"\"\"\n",
    "        layer = Dense_Layer(n_inputs, n_neurons, activation)\n",
    "        if weights is not None and biases is not None:\n",
    "            # Initialize with provided weights and biases\n",
    "            dummy_input = [[0] * n_inputs]  # Dummy input for setup\n",
    "            layer.setup_weights_and_inputs(dummy_input, weights, biases)\n",
    "            \n",
    "        # Initialize additional attributes needed for tracking\n",
    "        layer.current_weighted_sums = None\n",
    "        layer.layer_outputs = None\n",
    "        \n",
    "        self.layers.append(layer)\n",
    "        return layer\n",
    "    \n",
    "    def forward_pass(self, X):\n",
    "        \"\"\"Perform forward pass through all Dense_Layer instances\"\"\"\n",
    "        current_input = np.array(X)\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            # Update the layer's inputs\n",
    "            layer.inputs = current_input\n",
    "            \n",
    "            # Compute weighted sum + bias using Dense_Layer method\n",
    "            z = layer.weighted_sum_plus_bias()\n",
    "            layer.current_weighted_sums = z  # Store for later access\n",
    "            \n",
    "            # Apply activation function using Dense_Layer method\n",
    "            output = layer.apply_activation_function(z)\n",
    "            layer.layer_outputs = output  # Store for later access\n",
    "            \n",
    "            # Output becomes input for next layer\n",
    "            current_input = output\n",
    "            \n",
    "        return current_input\n",
    "\n",
    "print(\"✅ Multi-layer network class defined using Dense_Layer from Problem 1!\")\n",
    "print(\"This implementation demonstrates how the Dense_Layer class can build complete neural networks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc92b9b",
   "metadata": {},
   "source": [
    "### Problem 2a: Iris Dataset Classification\n",
    "### *Completed by: Kirk Henrich Gamo*\n",
    "\n",
    "**Objective:** Given inputs from the Iris Dataset using sepal length, sepal width, petal length and petal width, determine what class (Iris-setosa, Iris-versicolor, and Iris-virginica) the input belongs to by calculating the network output through a 3-layer neural network using Dense_Layer class.\n",
    "\n",
    "**Network Architecture:**\n",
    "- **Input Layer:** 4 features (sepal length, sepal width, petal length, petal width)\n",
    "- **Hidden Layer 1:** 5 neurons with ReLU activation  \n",
    "- **Hidden Layer 2:** 4 neurons with Sigmoid activation\n",
    "- **Output Layer:** 3 neurons (one for each Iris species) with Softmax activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0f6e6f",
   "metadata": {},
   "source": [
    "### Problem 2a: Network Configuration and Setup\n",
    "\n",
    "Setting up the Iris classification problem with the given network architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a84152f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Configurations:\n",
      "Layer 1: (4, 5) weights, (1, 5) biases, ReLU activation\n",
      "Layer 2: (5, 4) weights, (1, 4) biases, Sigmoid activation\n",
      "Layer 3: (4, 3) weights, (1, 3) biases, Softmax activation\n",
      "\n",
      "Setup complete:\n",
      "Input shape: (1, 4)\n",
      "Weights shape: (4, 5)\n",
      "Biases shape: (1, 5)\n",
      "Setup complete:\n",
      "Input shape: (1, 5)\n",
      "Weights shape: (5, 4)\n",
      "Biases shape: (1, 4)\n",
      "Setup complete:\n",
      "Input shape: (1, 4)\n",
      "Weights shape: (4, 3)\n",
      "Biases shape: (1, 3)\n"
     ]
    }
   ],
   "source": [
    "# Layer 3: 4 inputs → 3 neurons (Softmax)\n",
    "W3 = [[ 0.8,  0.2, -0.3],\n",
    "      [-0.4,  0.9,  0.1],\n",
    "      [ 0.3, -0.2,  0.7],\n",
    "      [ 0.6,  0.4, -0.5]]\n",
    "B3 = [[0.0, 0.1, -0.1]]\n",
    "\n",
    "print(\"Layer Configurations:\")\n",
    "print(f\"Layer 1: {np.array(W1).shape} weights, {np.array(B1).shape} biases, ReLU activation\")\n",
    "print(f\"Layer 2: {np.array(W2).shape} weights, {np.array(B2).shape} biases, Sigmoid activation\")\n",
    "print(f\"Layer 3: {np.array(W3).shape} weights, {np.array(B3).shape} biases, Softmax activation\")\n",
    "print()\n",
    "\n",
    "# Create network using Dense_Layer class\n",
    "network = MultiLayerNetwork()\n",
    "\n",
    "# Add layers using Dense_Layer instances\n",
    "layer1 = network.add_dense_layer(4, 5, 'relu', W1, B1)\n",
    "layer2 = network.add_dense_layer(5, 4, 'sigmoid', W2, B2)\n",
    "layer3 = network.add_dense_layer(4, 3, 'softmax', W3, B3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8279801",
   "metadata": {},
   "source": [
    "### Problem 2a: Forward Pass Execution\n",
    "\n",
    "Running the complete forward pass through the multi-layer network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4caabc0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PROBLEM 2a FORWARD PASS COMPUTATION ===\n",
      "\n",
      "Input sample: [5.1, 3.5, 1.4, 0.2]\n",
      "\n",
      "Weighted sum + bias computed. Output shape: (1, 5)\n",
      "Applied relu activation function\n",
      "Weighted sum + bias computed. Output shape: (1, 4)\n",
      "Applied sigmoid activation function\n",
      "Weighted sum + bias computed. Output shape: (1, 3)\n",
      "Applied softmax activation function\n",
      "Layer-by-Layer Computation Using Dense_Layer Class:\n",
      "=======================================================\n",
      "LAYER 1 (4 → 5 neurons, ReLU activation):\n",
      "Input: [5.1 3.5 1.4 0.2]\n",
      "Weighted Sum: [ 2.27  0.37  4.3  -0.56  1.33]\n",
      "After ReLU: [2.27 0.37 4.3  0.   1.33]\n",
      "→ Passed to Layer 2\n",
      "\n",
      "LAYER 2 (5 → 4 neurons, Sigmoid activation):\n",
      "Input: [2.27 0.37 4.3  0.   1.33]\n",
      "Weighted Sum: [0.384 1.26  4.255 1.048]\n",
      "After Sigmoid: [0.59483749 0.77902611 0.98600553 0.74039066]\n",
      "→ Passed to Layer 3\n",
      "\n",
      "LAYER 3 (4 → 3 neurons, Softmax activation - FINAL OUTPUT):\n",
      "Input: [0.59483749 0.77902611 0.98600553 0.74039066]\n",
      "Weighted Sum: [0.90429561 1.01904615 0.11945991]\n",
      "After Softmax: [0.38792946 0.4350992  0.17697134]\n",
      "\n",
      "=======================================================\n",
      "FINAL CLASSIFICATION RESULTS:\n",
      "Class Probabilities (from Softmax layer):\n",
      "  Iris-setosa    : 0.3879 (38.8%)\n",
      "  Iris-versicolor: 0.4351 (43.5%) ← PREDICTED\n",
      "  Iris-virginica : 0.1770 (17.7%)\n",
      "\n",
      "Predicted Class: Iris-versicolor\n",
      "Actual Class:    Iris-setosa\n",
      "Classification: ❌ Incorrect\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PROBLEM 2a FORWARD PASS: Complete Network Computation using Dense_Layer Class\n",
    "print(\"=== PROBLEM 2a FORWARD PASS COMPUTATION ===\")\n",
    "print()\n",
    "\n",
    "print(\"Input sample:\", X_input[0])\n",
    "print()\n",
    "\n",
    "# Forward pass through all Dense_Layer instances\n",
    "final_output = network.forward_pass(X_input)\n",
    "\n",
    "print(\"Layer-by-Layer Computation Using Dense_Layer Class:\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Layer 1 computation\n",
    "print(\"LAYER 1 (4 → 5 neurons, ReLU activation):\")\n",
    "layer1_output = network.layers[0].layer_outputs\n",
    "print(f\"Input: {network.layers[0].inputs.flatten()}\")\n",
    "print(f\"Weighted Sum: {network.layers[0].current_weighted_sums.flatten()}\")\n",
    "print(f\"After ReLU: {layer1_output.flatten()}\")\n",
    "print(\"→ Passed to Layer 2\")\n",
    "print()\n",
    "\n",
    "# Layer 2 computation  \n",
    "print(\"LAYER 2 (5 → 4 neurons, Sigmoid activation):\")\n",
    "layer2_output = network.layers[1].layer_outputs\n",
    "print(f\"Input: {network.layers[1].inputs.flatten()}\")\n",
    "print(f\"Weighted Sum: {network.layers[1].current_weighted_sums.flatten()}\")\n",
    "print(f\"After Sigmoid: {layer2_output.flatten()}\")\n",
    "print(\"→ Passed to Layer 3\")\n",
    "print()\n",
    "\n",
    "# Layer 3 computation (Final output)\n",
    "print(\"LAYER 3 (4 → 3 neurons, Softmax activation - FINAL OUTPUT):\")\n",
    "layer3_output = network.layers[2].layer_outputs\n",
    "print(f\"Input: {network.layers[2].inputs.flatten()}\")\n",
    "print(f\"Weighted Sum: {network.layers[2].current_weighted_sums.flatten()}\")\n",
    "print(f\"After Softmax: {layer3_output.flatten()}\")\n",
    "print()\n",
    "\n",
    "# Since layer 3 now uses softmax activation, the outputs are already probabilities\n",
    "final_probs = layer3_output.flatten()\n",
    "predicted_class = np.argmax(final_probs)\n",
    "class_names = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print(\"FINAL CLASSIFICATION RESULTS:\")\n",
    "print(\"Class Probabilities (from Softmax layer):\")\n",
    "for i, (class_name, prob) in enumerate(zip(class_names, final_probs)):\n",
    "    marker = \" ← PREDICTED\" if i == predicted_class else \"\"\n",
    "    print(f\"  {class_name:15}: {prob:.4f} ({prob*100:.1f}%){marker}\")\n",
    "\n",
    "print()\n",
    "print(f\"Predicted Class: {class_names[predicted_class]}\")\n",
    "print(f\"Actual Class:    {class_names[np.argmax(target_output[0])]}\")\n",
    "print(f\"Classification: {'✅ Correct' if predicted_class == np.argmax(target_output[0]) else '❌ Incorrect'}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eede4ed3",
   "metadata": {},
   "source": [
    "### Problem 2a: Verification and Loss Calculation\n",
    "\n",
    "Manual step-by-step verification using individual Dense_Layer instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d1a47c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PROBLEM 2a VERIFICATION: MANUAL CALCULATION ===\n",
      "Verifying Dense_Layer implementation step by step\n",
      "\n",
      "MANUAL VERIFICATION USING DENSE_LAYER CLASS:\n",
      "==================================================\n",
      "LAYER 1 VERIFICATION (4 → 5, ReLU):\n",
      "Setup complete:\n",
      "Input shape: (1, 4)\n",
      "Weights shape: (4, 5)\n",
      "Biases shape: (1, 5)\n",
      "Weighted sum + bias computed. Output shape: (1, 5)\n",
      "Applied relu activation function\n",
      "✓ Input: [5.1 3.5 1.4 0.2]\n",
      "✓ Weighted Sum: [ 2.27  0.37  4.3  -0.56  1.33]\n",
      "✓ ReLU Output: [2.27 0.37 4.3  0.   1.33]\n",
      "\n",
      "LAYER 2 VERIFICATION (5 → 4, Sigmoid):\n",
      "Setup complete:\n",
      "Input shape: (1, 5)\n",
      "Weights shape: (5, 4)\n",
      "Biases shape: (1, 4)\n",
      "Weighted sum + bias computed. Output shape: (1, 4)\n",
      "Applied sigmoid activation function\n",
      "✓ Input: [2.27 0.37 4.3  0.   1.33]\n",
      "✓ Weighted Sum: [0.384 1.26  4.255 1.048]\n",
      "✓ Sigmoid Output: [0.59483749 0.77902611 0.98600553 0.74039066]\n",
      "\n",
      "LAYER 3 VERIFICATION (4 → 3, Softmax - FINAL):\n",
      "Setup complete:\n",
      "Input shape: (1, 4)\n",
      "Weights shape: (4, 3)\n",
      "Biases shape: (1, 3)\n",
      "Weighted sum + bias computed. Output shape: (1, 3)\n",
      "Applied softmax activation function\n",
      "✓ Input: [0.59483749 0.77902611 0.98600553 0.74039066]\n",
      "✓ Weighted Sum: [0.90429561 1.01904615 0.11945991]\n",
      "✓ Final Output (Softmax): [0.38792946 0.4350992  0.17697134]\n",
      "\n",
      "==================================================\n",
      "COMPARISON WITH NETWORK RESULTS:\n",
      "Manual calculation:  [0.38792946 0.4350992  0.17697134]\n",
      "Network calculation: [0.38792946 0.4350992  0.17697134]\n",
      "Difference: [0. 0. 0.]\n",
      "Match: ✅ PERFECT MATCH\n",
      "\n",
      "LOSS CALCULATION:\n",
      "Calculated categorical_crossentropy loss: 0.946932\n",
      "Categorical Cross-Entropy Loss: 0.946932\n",
      "Target: [1, 0, 0], Prediction: [0.38792946 0.4350992  0.17697134]\n",
      "\n",
      "✅ Problem 2a completed successfully using Dense_Layer class from Problem 1!\n"
     ]
    }
   ],
   "source": [
    "# PROBLEM 2a VERIFICATION: Manual Step-by-Step Calculation using Dense_Layer Class\n",
    "print(\"=== PROBLEM 2a VERIFICATION: MANUAL CALCULATION ===\")\n",
    "print(\"Verifying Dense_Layer implementation step by step\")\n",
    "print()\n",
    "\n",
    "# Manual verification using Dense_Layer class functions\n",
    "print(\"MANUAL VERIFICATION USING DENSE_LAYER CLASS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create individual layers for verification\n",
    "layer1_verify = Dense_Layer(4, 5, 'relu')\n",
    "layer2_verify = Dense_Layer(5, 4, 'sigmoid')  \n",
    "layer3_verify = Dense_Layer(4, 3, 'softmax')\n",
    "\n",
    "# Layer 1 verification\n",
    "print(\"LAYER 1 VERIFICATION (4 → 5, ReLU):\")\n",
    "layer1_verify.setup_weights_and_inputs(X_input, W1, B1)\n",
    "z1 = layer1_verify.weighted_sum_plus_bias()\n",
    "a1 = layer1_verify.apply_activation_function(z1)\n",
    "print(f\"✓ Input: {layer1_verify.inputs.flatten()}\")\n",
    "print(f\"✓ Weighted Sum: {z1.flatten()}\")\n",
    "print(f\"✓ ReLU Output: {a1.flatten()}\")\n",
    "print()\n",
    "\n",
    "# Layer 2 verification  \n",
    "print(\"LAYER 2 VERIFICATION (5 → 4, Sigmoid):\")\n",
    "layer2_verify.setup_weights_and_inputs(a1, W2, B2)\n",
    "z2 = layer2_verify.weighted_sum_plus_bias()\n",
    "a2 = layer2_verify.apply_activation_function(z2)\n",
    "print(f\"✓ Input: {layer2_verify.inputs.flatten()}\")\n",
    "print(f\"✓ Weighted Sum: {z2.flatten()}\")\n",
    "print(f\"✓ Sigmoid Output: {a2.flatten()}\")\n",
    "print()\n",
    "\n",
    "# Layer 3 verification (Final)\n",
    "print(\"LAYER 3 VERIFICATION (4 → 3, Softmax - FINAL):\")\n",
    "layer3_verify.setup_weights_and_inputs(a2, W3, B3)\n",
    "z3 = layer3_verify.weighted_sum_plus_bias()\n",
    "a3 = layer3_verify.apply_activation_function(z3)\n",
    "print(f\"✓ Input: {layer3_verify.inputs.flatten()}\")\n",
    "print(f\"✓ Weighted Sum: {z3.flatten()}\")\n",
    "print(f\"✓ Final Output (Softmax): {a3.flatten()}\")\n",
    "print()\n",
    "\n",
    "# Compare with network results\n",
    "print(\"=\" * 50)\n",
    "print(\"COMPARISON WITH NETWORK RESULTS:\")\n",
    "manual_result = a3.flatten()\n",
    "network_result = layer3_output.flatten()\n",
    "\n",
    "print(f\"Manual calculation:  {manual_result}\")\n",
    "print(f\"Network calculation: {network_result}\")\n",
    "print(f\"Difference: {np.abs(manual_result - network_result)}\")\n",
    "print(f\"Match: {'✅ PERFECT MATCH' if np.allclose(manual_result, network_result, atol=1e-6) else '❌ MISMATCH'}\")\n",
    "print()\n",
    "\n",
    "# Loss calculation using Dense_Layer class\n",
    "print(\"LOSS CALCULATION:\")\n",
    "loss = layer3_verify.calculate_loss(a3, target_output, 'categorical_crossentropy')\n",
    "print(f\"Categorical Cross-Entropy Loss: {loss:.6f}\")\n",
    "print(f\"Target: {target_output[0]}, Prediction: {a3.flatten()}\")\n",
    "print()\n",
    "print(\"✅ Problem 2a completed successfully using Dense_Layer class from Problem 1!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee426d9e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Problem 2b: Breast Cancer Dataset Classification\n",
    "### *Completed by: Dallas Aquino*\n",
    "\n",
    "**Objective:** Given inputs from the Breast Cancer Dataset using three features (Mean Radius, Mean Texture, and Mean Smoothness), determine whether the tumor is Benign (0) or Malignant (1) by calculating the network outputs step by step.\n",
    "\n",
    "**Network Architecture:**\n",
    "- **Input Layer:** 3 features (Mean Radius, Mean Texture, Mean Smoothness)\n",
    "- **Hidden Layer 1:** ReLU activation  \n",
    "- **Hidden Layer 2:** Sigmoid activation\n",
    "- **Output Layer:** 1 neuron (Binary classification) with Sigmoid activation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb7932e",
   "metadata": {},
   "source": [
    "### Problem 2b: Network Configuration and Setup\n",
    "\n",
    "Setting up the Breast Cancer classification problem with the given network architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1ceeb981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PROBLEM 2b SETUP: BREAST CANCER CLASSIFICATION ===\n",
      "Implemented by: Dallas Aquino\n",
      "\n",
      "Problem Configuration:\n",
      "Input features (X): [14.1, 20.3, 0.095]\n",
      "  - Mean Radius:     14.1\n",
      "  - Mean Texture:    20.3\n",
      "  - Mean Smoothness: 0.095\n",
      "\n",
      "Target output: 1 (1 = Malignant)\n",
      "\n",
      "Network Architecture:\n",
      "Input Layer (3) → Hidden Layer 1 (2) → Hidden Layer 2 (2) → Output Layer (1)\n",
      "\n",
      "Layer Configurations:\n",
      "Layer 1: (3, 2) weights, (1, 2) biases, ReLU activation\n",
      "Layer 2: (2, 2) weights, (1, 2) biases, Sigmoid activation\n",
      "Layer 3: (2, 1) weights, (1, 1) biases, Sigmoid activation\n",
      "\n",
      "Setup complete:\n",
      "Input shape: (1, 3)\n",
      "Weights shape: (3, 2)\n",
      "Biases shape: (1, 2)\n",
      "Setup complete:\n",
      "Input shape: (1, 2)\n",
      "Weights shape: (2, 2)\n",
      "Biases shape: (1, 2)\n",
      "Setup complete:\n",
      "Input shape: (1, 2)\n",
      "Weights shape: (2, 1)\n",
      "Biases shape: (1, 1)\n",
      "✅ Network created successfully using Dense_Layer class from Problem 1!\n",
      "Number of Dense_Layer instances: 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PROBLEM 2b SETUP: Breast Cancer Classification using Dense_Layer Class (Dallas's Implementation)\n",
    "print(\"=== PROBLEM 2b SETUP: BREAST CANCER CLASSIFICATION ===\")\n",
    "print(\"Implemented by: Dallas Aquino\")\n",
    "print()\n",
    "\n",
    "# Input data from Breast Cancer Dataset (Mean Radius, Mean Texture, Mean Smoothness)\n",
    "X_breast = [[14.1, 20.3, 0.095]]  # Sample breast cancer measurements\n",
    "\n",
    "# Target output (Binary classification: 0 = Benign, 1 = Malignant)\n",
    "target_breast = [[1]]  # This sample should be Malignant (1)\n",
    "\n",
    "print(\"Problem Configuration:\")\n",
    "print(f\"Input features (X): {X_breast[0]}\")\n",
    "print(\"  - Mean Radius:     {:.1f}\".format(X_breast[0][0]))\n",
    "print(\"  - Mean Texture:    {:.1f}\".format(X_breast[0][1]))\n",
    "print(\"  - Mean Smoothness: {:.3f}\".format(X_breast[0][2]))\n",
    "print()\n",
    "print(f\"Target output: {target_breast[0][0]} (1 = Malignant)\")\n",
    "print()\n",
    "\n",
    "# Network Architecture: 3 → 2 → 2 → 1 neurons\n",
    "print(\"Network Architecture:\")\n",
    "print(\"Input Layer (3) → Hidden Layer 1 (2) → Hidden Layer 2 (2) → Output Layer (1)\")\n",
    "print()\n",
    "\n",
    "# Define weights and biases for each layer\n",
    "# Layer 1: 3 inputs → 2 neurons (ReLU)\n",
    "W1_breast = [[ 0.5, -0.3],\n",
    "             [ 0.2,  0.4], \n",
    "             [-0.7,  0.9]]\n",
    "B1_breast = [[0.0, -0.5]]\n",
    "\n",
    "# Layer 2: 2 inputs → 2 neurons (Sigmoid)  \n",
    "W2_breast = [[ 0.6, -0.2],\n",
    "             [-0.3,  0.5]]\n",
    "B2_breast = [[-0.8, 0.1]]\n",
    "\n",
    "# Layer 3: 2 inputs → 1 neuron (Sigmoid)\n",
    "W3_breast = [[ 0.7],\n",
    "             [-0.5]]\n",
    "B3_breast = [[0.2]]\n",
    "\n",
    "print(\"Layer Configurations:\")\n",
    "print(f\"Layer 1: {np.array(W1_breast).shape} weights, {np.array(B1_breast).shape} biases, ReLU activation\")\n",
    "print(f\"Layer 2: {np.array(W2_breast).shape} weights, {np.array(B2_breast).shape} biases, Sigmoid activation\")\n",
    "print(f\"Layer 3: {np.array(W3_breast).shape} weights, {np.array(B3_breast).shape} biases, Sigmoid activation\")\n",
    "print()\n",
    "\n",
    "# Create network using Dense_Layer class\n",
    "network_breast = MultiLayerNetwork()\n",
    "\n",
    "# Add Dense_Layer instances\n",
    "layer1_breast = network_breast.add_dense_layer(3, 2, 'relu', W1_breast, B1_breast)\n",
    "layer2_breast = network_breast.add_dense_layer(2, 2, 'sigmoid', W2_breast, B2_breast)\n",
    "layer3_breast = network_breast.add_dense_layer(2, 1, 'sigmoid', W3_breast, B3_breast)\n",
    "\n",
    "print(\"✅ Network created successfully using Dense_Layer class from Problem 1!\")\n",
    "print(f\"Number of Dense_Layer instances: {len(network_breast.layers)}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4726ea0c",
   "metadata": {},
   "source": [
    "### Problem 2b: Forward Pass Execution\n",
    "\n",
    "Running the complete forward pass through the multi-layer Breast Cancer classification network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a0de0679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PROBLEM 2b FORWARD PASS COMPUTATION ===\n",
      "\n",
      "Input sample: [14.1, 20.3, 0.095]\n",
      "\n",
      "Weighted sum + bias computed. Output shape: (1, 2)\n",
      "Applied relu activation function\n",
      "Weighted sum + bias computed. Output shape: (1, 2)\n",
      "Applied sigmoid activation function\n",
      "Weighted sum + bias computed. Output shape: (1, 1)\n",
      "Applied sigmoid activation function\n",
      "Layer-by-Layer Computation Using Dense_Layer Class:\n",
      "=======================================================\n",
      "LAYER 1 (3 → 2 neurons, ReLU activation):\n",
      "Input: [14.1   20.3    0.095]\n",
      "Weighted Sum: [11.0435  3.4755]\n",
      "After ReLU: [11.0435  3.4755]\n",
      "→ Passed to Layer 2\n",
      "\n",
      "LAYER 2 (2 → 2 neurons, Sigmoid activation):\n",
      "Input: [11.0435  3.4755]\n",
      "Weighted Sum: [ 4.78345 -0.37095]\n",
      "After Sigmoid: [0.99170234 0.40831149]\n",
      "→ Passed to Layer 3\n",
      "\n",
      "LAYER 3 (2 → 1 neuron, Sigmoid activation - FINAL OUTPUT):\n",
      "Input: [0.99170234 0.40831149]\n",
      "Weighted Sum: [0.6900359]\n",
      "After Sigmoid: [0.66597491]\n",
      "\n",
      "=======================================================\n",
      "FINAL RESULTS:\n",
      "Network Prediction: 0.665975\n",
      "Target Value: 1\n",
      "Prediction Interpretation: Malignant\n",
      "Classification: ✅ Correct\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# PROBLEM 2b FORWARD PASS: Complete Network Computation using Dense_Layer Class\n",
    "print(\"=== PROBLEM 2b FORWARD PASS COMPUTATION ===\")\n",
    "print()\n",
    "\n",
    "print(\"Input sample:\", X_breast[0])\n",
    "print()\n",
    "\n",
    "# Forward pass through all Dense_Layer instances\n",
    "final_output_breast = network_breast.forward_pass(X_breast)\n",
    "\n",
    "print(\"Layer-by-Layer Computation Using Dense_Layer Class:\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Layer 1 computation\n",
    "print(\"LAYER 1 (3 → 2 neurons, ReLU activation):\")\n",
    "layer1_output_breast = network_breast.layers[0].layer_outputs\n",
    "print(f\"Input: {network_breast.layers[0].inputs.flatten()}\")\n",
    "print(f\"Weighted Sum: {network_breast.layers[0].current_weighted_sums.flatten()}\")\n",
    "print(f\"After ReLU: {layer1_output_breast.flatten()}\")\n",
    "print(\"→ Passed to Layer 2\")\n",
    "print()\n",
    "\n",
    "# Layer 2 computation  \n",
    "print(\"LAYER 2 (2 → 2 neurons, Sigmoid activation):\")\n",
    "layer2_output_breast = network_breast.layers[1].layer_outputs\n",
    "print(f\"Input: {network_breast.layers[1].inputs.flatten()}\")\n",
    "print(f\"Weighted Sum: {network_breast.layers[1].current_weighted_sums.flatten()}\")\n",
    "print(f\"After Sigmoid: {layer2_output_breast.flatten()}\")\n",
    "print(\"→ Passed to Layer 3\")\n",
    "print()\n",
    "\n",
    "# Layer 3 computation (Final output)\n",
    "print(\"LAYER 3 (2 → 1 neuron, Sigmoid activation - FINAL OUTPUT):\")\n",
    "layer3_output_breast = network_breast.layers[2].layer_outputs\n",
    "print(f\"Input: {network_breast.layers[2].inputs.flatten()}\")\n",
    "print(f\"Weighted Sum: {network_breast.layers[2].current_weighted_sums.flatten()}\")\n",
    "print(f\"After Sigmoid: {layer3_output_breast.flatten()}\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print(\"FINAL RESULTS:\")\n",
    "final_prediction_breast = layer3_output_breast[0][0]\n",
    "print(f\"Network Prediction: {final_prediction_breast:.6f}\")\n",
    "print(f\"Target Value: {target_breast[0][0]}\")\n",
    "print(f\"Prediction Interpretation: {'Malignant' if final_prediction_breast > 0.5 else 'Benign'}\")\n",
    "print(f\"Classification: {'✅ Correct' if (final_prediction_breast > 0.5) == (target_breast[0][0] == 1) else '❌ Incorrect'}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1265f261",
   "metadata": {},
   "source": [
    "### Problem 2b: Verification and Loss Calculation\n",
    "\n",
    "Manual step-by-step verification using individual Dense_Layer instances:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3bf5c2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PROBLEM 2b VERIFICATION: MANUAL CALCULATION ===\n",
      "Verifying Dense_Layer implementation step by step\n",
      "\n",
      "MANUAL VERIFICATION USING DENSE_LAYER CLASS:\n",
      "==================================================\n",
      "LAYER 1 VERIFICATION (3 → 2, ReLU):\n",
      "Setup complete:\n",
      "Input shape: (1, 3)\n",
      "Weights shape: (3, 2)\n",
      "Biases shape: (1, 2)\n",
      "Weighted sum + bias computed. Output shape: (1, 2)\n",
      "Applied relu activation function\n",
      "✓ Input: [14.1   20.3    0.095]\n",
      "✓ Weighted Sum: [11.0435  3.4755]\n",
      "✓ ReLU Output: [11.0435  3.4755]\n",
      "\n",
      "LAYER 2 VERIFICATION (2 → 2, Sigmoid):\n",
      "Setup complete:\n",
      "Input shape: (1, 2)\n",
      "Weights shape: (2, 2)\n",
      "Biases shape: (1, 2)\n",
      "Weighted sum + bias computed. Output shape: (1, 2)\n",
      "Applied sigmoid activation function\n",
      "✓ Input: [11.0435  3.4755]\n",
      "✓ Weighted Sum: [ 4.78345 -0.37095]\n",
      "✓ Sigmoid Output: [0.99170234 0.40831149]\n",
      "\n",
      "LAYER 3 VERIFICATION (2 → 1, Sigmoid - FINAL):\n",
      "Setup complete:\n",
      "Input shape: (1, 2)\n",
      "Weights shape: (2, 1)\n",
      "Biases shape: (1, 1)\n",
      "Weighted sum + bias computed. Output shape: (1, 1)\n",
      "Applied sigmoid activation function\n",
      "✓ Input: [0.99170234 0.40831149]\n",
      "✓ Weighted Sum: [0.6900359]\n",
      "✓ Final Output: [0.66597491]\n",
      "\n",
      "==================================================\n",
      "COMPARISON WITH NETWORK RESULTS:\n",
      "Manual calculation:  0.66597491\n",
      "Network calculation: 0.66597491\n",
      "Difference: 0.0000000000\n",
      "Match: ✅ PERFECT MATCH\n",
      "\n",
      "LOSS CALCULATION:\n",
      "Calculated binary_crossentropy loss: 0.406503\n",
      "Binary Cross-Entropy Loss: 0.406503\n",
      "Target: 1, Prediction: 0.665975\n",
      "\n",
      "✅ Problem 2b completed successfully using Dense_Layer class from Problem 1!\n"
     ]
    }
   ],
   "source": [
    "# PROBLEM 2b VERIFICATION: Manual Step-by-Step Calculation using Dense_Layer Class\n",
    "print(\"=== PROBLEM 2b VERIFICATION: MANUAL CALCULATION ===\")\n",
    "print(\"Verifying Dense_Layer implementation step by step\")\n",
    "print()\n",
    "\n",
    "# Manual verification using Dense_Layer class functions\n",
    "print(\"MANUAL VERIFICATION USING DENSE_LAYER CLASS:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create individual layers for verification\n",
    "layer1_verify = Dense_Layer(3, 2, 'relu')\n",
    "layer2_verify = Dense_Layer(2, 2, 'sigmoid')  \n",
    "layer3_verify = Dense_Layer(2, 1, 'sigmoid')\n",
    "\n",
    "# Layer 1 verification\n",
    "print(\"LAYER 1 VERIFICATION (3 → 2, ReLU):\")\n",
    "layer1_verify.setup_weights_and_inputs(X_breast, W1_breast, B1_breast)\n",
    "z1 = layer1_verify.weighted_sum_plus_bias()\n",
    "a1 = layer1_verify.apply_activation_function(z1)\n",
    "print(f\"✓ Input: {layer1_verify.inputs.flatten()}\")\n",
    "print(f\"✓ Weighted Sum: {z1.flatten()}\")\n",
    "print(f\"✓ ReLU Output: {a1.flatten()}\")\n",
    "print()\n",
    "\n",
    "# Layer 2 verification  \n",
    "print(\"LAYER 2 VERIFICATION (2 → 2, Sigmoid):\")\n",
    "layer2_verify.setup_weights_and_inputs(a1, W2_breast, B2_breast)\n",
    "z2 = layer2_verify.weighted_sum_plus_bias()\n",
    "a2 = layer2_verify.apply_activation_function(z2)\n",
    "print(f\"✓ Input: {layer2_verify.inputs.flatten()}\")\n",
    "print(f\"✓ Weighted Sum: {z2.flatten()}\")\n",
    "print(f\"✓ Sigmoid Output: {a2.flatten()}\")\n",
    "print()\n",
    "\n",
    "# Layer 3 verification (Final)\n",
    "print(\"LAYER 3 VERIFICATION (2 → 1, Sigmoid - FINAL):\")\n",
    "layer3_verify.setup_weights_and_inputs(a2, W3_breast, B3_breast)\n",
    "z3 = layer3_verify.weighted_sum_plus_bias()\n",
    "a3 = layer3_verify.apply_activation_function(z3)\n",
    "print(f\"✓ Input: {layer3_verify.inputs.flatten()}\")\n",
    "print(f\"✓ Weighted Sum: {z3.flatten()}\")\n",
    "print(f\"✓ Final Output: {a3.flatten()}\")\n",
    "print()\n",
    "\n",
    "# Compare with network results\n",
    "print(\"=\" * 50)\n",
    "print(\"COMPARISON WITH NETWORK RESULTS:\")\n",
    "manual_result = a3[0][0]\n",
    "network_result = final_prediction_breast  # From previous cell\n",
    "\n",
    "print(f\"Manual calculation:  {manual_result:.8f}\")\n",
    "print(f\"Network calculation: {network_result:.8f}\")\n",
    "print(f\"Difference: {abs(manual_result - network_result):.10f}\")\n",
    "print(f\"Match: {'✅ PERFECT MATCH' if abs(manual_result - network_result) < 1e-6 else '❌ MISMATCH'}\")\n",
    "print()\n",
    "\n",
    "# Loss calculation using Dense_Layer class\n",
    "print(\"LOSS CALCULATION:\")\n",
    "loss = layer3_verify.calculate_loss(a3, target_breast, 'binary_crossentropy')\n",
    "print(f\"Binary Cross-Entropy Loss: {loss:.6f}\")\n",
    "print(f\"Target: {target_breast[0][0]}, Prediction: {a3[0][0]:.6f}\")\n",
    "print()\n",
    "print(\"✅ Problem 2b completed successfully using Dense_Layer class from Problem 1!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

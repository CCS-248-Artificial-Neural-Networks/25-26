{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1152e9dd",
   "metadata": {},
   "source": [
    "This is the Python implementation of a simple neural network with hidden layers.\n",
    "\n",
    "A primary challenge on handling multiple neurons is that you need to handle multiple paramaters. Lucky for us, we have the right packages to easen the process of managing them, as well as calculating the outputs.\n",
    "\n",
    "Before running this, make sure you have installed the following packages in your system/environment:\n",
    "\n",
    "numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8651d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb698659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the neural network structure\n",
    "# First layer\n",
    "inputs = [\n",
    "    [1, 2, 3, 2.5], \n",
    "    # [2.0, 5.0, -1.0, 2.0],  \n",
    "    # [-1.5, 2.7, 3.3, 0.8], \n",
    "]\n",
    "\n",
    "weights = [\n",
    "    [0.2, 0.8, -0.5, 1.0],  # Weights for Neuron 1\n",
    "    [0.5, -0.91, 0.26, -0.5],  # Weights for Neuron 2\n",
    "    [-0.26, -0.27, 0.17, 0.87]  # Weights for Neuron 3\n",
    "]\n",
    "\n",
    "biases = [2.0, 3.0, 0.5]  # Biases for each neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ff9d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the second layer\n",
    "weights2 = [\n",
    "    [0.1, -0.14, 0.5],\n",
    "    [-0.5, 0.12, -0.33],\n",
    "    [-0.44, 0.73, -0.13],\n",
    "]\n",
    "\n",
    "biases2 = [-1.0, 2.0, -0.5]  # Biases for each neuron in second layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9555d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the forward pass\n",
    "# First layer\n",
    "layer1_outputs = np.dot(inputs, np.array(weights).T) + biases\n",
    "print(\"Layer 1 Raw Outputs:\", layer1_outputs)\n",
    "for item in layer1_outputs:\n",
    "    for i in range(len(item)):\n",
    "        item[i] = max(0, item[i])  # ReLU activation function\n",
    "\n",
    "print(\"Layer 1 Outputs:\", layer1_outputs)\n",
    "\n",
    "print(\"Layer 1 Outputs:\", layer1_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0565f5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second layer\n",
    "# The output of the first layer is used as input for the second layer\n",
    "layer2_outputs = np.dot(layer1_outputs, np.array(weights2).T) + biases2\n",
    "print(\"Raw Layer 2 Outputs:\", layer2_outputs)\n",
    "for item in layer2_outputs:\n",
    "    for i in range(len(item)):\n",
    "        item[i] = max(0, item[i])  # ReLU activation function\n",
    "\n",
    "print(\"Layer 2 Outputs:\", layer2_outputs)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
